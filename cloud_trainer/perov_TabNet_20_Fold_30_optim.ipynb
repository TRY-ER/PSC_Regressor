{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d331def",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-02-17T14:44:55.333886Z",
     "iopub.status.busy": "2023-02-17T14:44:55.333344Z",
     "iopub.status.idle": "2023-02-17T14:45:07.162355Z",
     "shell.execute_reply": "2023-02-17T14:45:07.161191Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 11.83727,
     "end_time": "2023-02-17T14:45:07.165032",
     "exception": false,
     "start_time": "2023-02-17T14:44:55.327762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 17 14:44:56 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (2.10.1)\r\n",
      "Requirement already satisfied: xgboost==1.6.1 in /opt/conda/lib/python3.7/site-packages (1.6.1)\r\n",
      "Collecting pytorch-tabnet\r\n",
      "  Downloading pytorch_tabnet-4.0-py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m533.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost==1.6.1) (1.7.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost==1.6.1) (1.21.6)\r\n",
      "Requirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna) (1.8.1)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.39)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna) (6.6.0)\r\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna) (0.8.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (21.3)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna) (6.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.64.0)\r\n",
      "Requirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna) (3.10.1)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.11.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.0.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (3.0.9)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.1.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (4.12.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (4.1.1)\r\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (5.8.0)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.2.1)\r\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (2.4.2)\r\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.3.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.9.0)\r\n",
      "Requirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (0.5.1)\r\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.5.0)\r\n",
      "Requirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\r\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna) (2.0.1)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "! pip install optuna xgboost==1.6.1 pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dcd8d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:07.174564Z",
     "iopub.status.busy": "2023-02-17T14:45:07.174091Z",
     "iopub.status.idle": "2023-02-17T14:45:21.502616Z",
     "shell.execute_reply": "2023-02-17T14:45:21.501405Z"
    },
    "papermill": {
     "duration": 14.336971,
     "end_time": "2023-02-17T14:45:21.506173",
     "exception": false,
     "start_time": "2023-02-17T14:45:07.169202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaleido\r\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: kaleido\r\n",
      "Successfully installed kaleido-0.2.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006ce8ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:21.527490Z",
     "iopub.status.busy": "2023-02-17T14:45:21.526963Z",
     "iopub.status.idle": "2023-02-17T14:45:24.617629Z",
     "shell.execute_reply": "2023-02-17T14:45:24.616677Z"
    },
    "papermill": {
     "duration": 3.103638,
     "end_time": "2023-02-17T14:45:24.620139",
     "exception": false,
     "start_time": "2023-02-17T14:45:21.516501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from xgboost import XGBRegressor\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,mean_absolute_error,r2_score\n",
    "import optuna as opt\n",
    "import joblib\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10494e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:24.634461Z",
     "iopub.status.busy": "2023-02-17T14:45:24.634035Z",
     "iopub.status.idle": "2023-02-17T14:45:24.642296Z",
     "shell.execute_reply": "2023-02-17T14:45:24.641401Z"
    },
    "papermill": {
     "duration": 0.017647,
     "end_time": "2023-02-17T14:45:24.644172",
     "exception": false,
     "start_time": "2023-02-17T14:45:24.626525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_save_cv_model(i,model_name,model,optim,mse_loss,trial_data,output_path=\"./\"):\n",
    "\n",
    "    ''' This function saves cross validation model in the corresponding directory ( if the path does not exist it creates the path for it'''\n",
    "\n",
    "\n",
    "    if os.path.exists(os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}\")):\n",
    "        joblib.dump(model, os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/{i}_model.z\"))\n",
    "        with open(os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/mse_loss.txt\"),\"w+\") as file:file.write(f\"{mse_loss}\")\n",
    "        joblib.dump(trial_data, os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/{i}_trial_data.z\"))\n",
    "    else:\n",
    "        os.mkdir(os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}\"))\n",
    "        joblib.dump(model, os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/{i}_model.z\"))\n",
    "        with open(os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/mse_loss.txt\"),\"w+\") as file:file.write(f\"{mse_loss}\")\n",
    "        joblib.dump(trial_data, os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/{i}_trial_data.z\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d080beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:24.657280Z",
     "iopub.status.busy": "2023-02-17T14:45:24.657009Z",
     "iopub.status.idle": "2023-02-17T14:45:24.663725Z",
     "shell.execute_reply": "2023-02-17T14:45:24.662788Z"
    },
    "papermill": {
     "duration": 0.0155,
     "end_time": "2023-02-17T14:45:24.665668",
     "exception": false,
     "start_time": "2023-02-17T14:45:24.650168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_cv_model(i,model_name,model,optim,mse_loss,output_path=\"./\"):\n",
    "\n",
    "    ''' This function saves cross validation model in the corresponding directory ( if the path does not exist it creates the path for it'''\n",
    "\n",
    "\n",
    "    if os.path.exists(os.path.join(output_path,f\"{i}_{model_name}_{optim}\")):\n",
    "        joblib.dump(model, os.path.join(output_path,f\"{i}_{model_name}_{optim}/{i}_model.z\"))\n",
    "        with open(os.path.join(output_path,f\"{i}_{model_name}_{optim}/losses_{fold}.txt\"),\"w+\") as file:file.write(f\"{mse_loss}\")\n",
    "    else:\n",
    "        os.mkdir(os.path.join(output_path,f\"{i}_{model_name}_{optim}\"))\n",
    "        joblib.dump(model, os.path.join(output_path,f\"{i}_{model_name}_{optim}/{i}_model.z\"))\n",
    "        with open(os.path.join(output_path,f\"{i}_{model_name}_{optim}/losses_{fold}.txt\"),\"w+\") as file:file.write(f\"{mse_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "640f440d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:24.679218Z",
     "iopub.status.busy": "2023-02-17T14:45:24.678358Z",
     "iopub.status.idle": "2023-02-17T14:45:24.684038Z",
     "shell.execute_reply": "2023-02-17T14:45:24.683217Z"
    },
    "papermill": {
     "duration": 0.014329,
     "end_time": "2023-02-17T14:45:24.685924",
     "exception": false,
     "start_time": "2023-02-17T14:45:24.671595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trial_data(trial) -> list:\n",
    "  ''' This function takes the trial objects and returns the dictionary containing the trial details for plotting and comparing purposes '''\n",
    "  trial_data = trial.get_trials()\n",
    "  value_dict = {}\n",
    "  for i in trial_data:\n",
    "    print(i.params)\n",
    "    value_dict[i.number] = {\"params\": i.params , \"rmse\": i.values}\n",
    "    print(f\"{i.number} : {i.values}\")\n",
    "  return value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c9f6b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:24.700108Z",
     "iopub.status.busy": "2023-02-17T14:45:24.698698Z",
     "iopub.status.idle": "2023-02-17T14:45:24.705753Z",
     "shell.execute_reply": "2023-02-17T14:45:24.704870Z"
    },
    "papermill": {
     "duration": 0.015785,
     "end_time": "2023-02-17T14:45:24.707698",
     "exception": false,
     "start_time": "2023-02-17T14:45:24.691913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_optuna_plots(study,dirname):\n",
    "    optim_hist = opt.visualization.plot_optimization_history(study)\n",
    "    intermediate = opt.visualization.plot_intermediate_values(study)\n",
    "    parallel = opt.visualization.plot_parallel_coordinate(study)\n",
    "    plot_slice = opt.visualization.plot_slice(study)\n",
    "    name_lists = [\"optim_hist\",\"intermediate\",\"parallel\", \"plot_slice\"]\n",
    "    plot_lists = [optim_hist,intermediate,parallel, plot_slice]\n",
    "    for name,plot in zip(name_lists,plot_lists):\n",
    "        if os.path.exists(f\"./{dirname}\"):\n",
    "            print(\"getting into if block\")\n",
    "            plot.write_image(f\"./{dirname}/{name}.jpg\",width=2, height=2)\n",
    "        else:\n",
    "            print(\"getting into else block\")\n",
    "            os.mkdir(f\"./{dirname}\")\n",
    "            plot.write_image(f\"./{dirname}/{name}.jpg\",width=2, height=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa76050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:24.721168Z",
     "iopub.status.busy": "2023-02-17T14:45:24.720882Z",
     "iopub.status.idle": "2023-02-17T14:45:24.737506Z",
     "shell.execute_reply": "2023-02-17T14:45:24.736690Z"
    },
    "papermill": {
     "duration": 0.025689,
     "end_time": "2023-02-17T14:45:24.739435",
     "exception": false,
     "start_time": "2023-02-17T14:45:24.713746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_trial(fold_dict,fold,model_name,sc_df,tar_col,optim,optim_trial,k_folds,tar_cols=\"\",verbose=1):\n",
    "\n",
    "    ''' this function is used to train the model with parameters optimization using optuna and cross validation using stratified k_folds'''\n",
    "\n",
    "    y = sc_df[tar_col]\n",
    "    print(y.shape)\n",
    "    x = sc_df.drop([tar_col],axis=1)\n",
    "    print(x.shape)\n",
    "    model_name = model_name \n",
    "    def objective(trial):\n",
    "      train_index = fold_dict[fold][\"train\"]\n",
    "      test_index = fold_dict[fold][\"test\"]\n",
    "      clf = TabNetRegressor(n_d=trial.suggest_int(\"n_d\", 8, 64),\n",
    "                                    n_a =trial.suggest_int(\"n_a\", 8, 64),\n",
    "                                    n_steps = trial.suggest_int(\"n_steps\",3,10),\n",
    "                                    gamma =trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "                                    n_independent = trial.suggest_int(\"n_independent\",1,5),\n",
    "                                    n_shared = trial.suggest_int(\"n_shared\",1,5),\n",
    "                                    momentum = trial.suggest_float(\"momentum\", 0.01, 0.4),\n",
    "                                    optimizer_fn = torch.optim.Adam,\n",
    "                                    # scheduler_fn = torch.optim.lr_scheduler,\n",
    "                                    # scheduler_params = {\"gamma\" :trial.suggest_float(\"sch-gamma\", 0.5, 0.95), \"step_size\": trial.suggest_int(\"sch_step_size\", 10, 20, 2)},\n",
    "                                    verbose = verbose,\n",
    "                                    device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                                    )\n",
    "      # print(f\" train_index :: {train_index}\")\n",
    "      # print(f\" test_index :: {test_index}\")\n",
    "      X_train,X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "      # print(X_train.shape, X_test.shape)\n",
    "      X_train, X_test = X_train.to_numpy(dtype=np.float64), X_test.to_numpy(dtype=np.float64)\n",
    "      Y_train, Y_test = y.iloc[train_index].to_numpy(dtype=np.float64), y.iloc[test_index].to_numpy(np.float64)\n",
    "      # Y_train, Y_test = Y_train.to_numpy(dtype=np.float64), Y_test.to_numpy(dtype=np.float64)\n",
    "      print(X_train.shape)\n",
    "      print(Y_train.shape)\n",
    "      print(X_test.shape)\n",
    "      print(Y_test.shape)\n",
    "      Y_train = Y_train.reshape(-1,1)\n",
    "      clf.fit(X_train, Y_train)\n",
    "      Y_pred = clf.predict(X_test)\n",
    "      mse_error = mean_squared_error(Y_pred, Y_test, squared=False) \n",
    "      return mse_error\n",
    "\n",
    "    print(f\"Starting optimization for fold : [{fold}/{k_folds}]\")\n",
    "    study = opt.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=optim_trial)\n",
    "    save_optuna_plots(study,\"optuna_plots\")\n",
    "    best_params = study.best_params\n",
    "    trial_data = get_trial_data(study)\n",
    "    print(f\" Best params for fold : [{fold}/{k_folds}]\")\n",
    "    print(best_params)\n",
    "    train_index = fold_dict[fold][\"train\"]\n",
    "    test_index = fold_dict[fold][\"test\"]\n",
    "    X_train,X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "    # print(X_train.shape, X_test.shape)\n",
    "    X_train, X_test = X_train.to_numpy(dtype=np.float64), X_test.to_numpy(dtype=np.float64)\n",
    "    Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    Y_train, Y_test = Y_train.to_numpy(dtype=np.float64), Y_test.to_numpy(dtype=np.float64)\n",
    "    clf_model = TabNetRegressor(**study.best_params,device_name= \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    clf_model.fit(X_train,Y_train)\n",
    "    Y_pred = clf_model.predict(X_test)\n",
    "    error = {\"mse_error\" : mean_squared_error(Y_pred, Y_test, squared=False),\n",
    "    \"mae_error\" : mean_absolute_error(Y_pred,Y_test),\n",
    "    \"rmse_error\" : mean_squared_error(Y_pred, Y_test),\n",
    "    \"r2_score\" : r2_score(Y_pred,Y_test) }\n",
    "    # try:\n",
    "    print(\"[++] Saving the model and parameters in corresponding directories\")\n",
    "    make_save_cv_model(fold,model_name,clf_model,optim,mse_loss=error,trial_data=trial_data)\n",
    "    return trial_data,best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75bd81f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:24.753266Z",
     "iopub.status.busy": "2023-02-17T14:45:24.752420Z",
     "iopub.status.idle": "2023-02-17T14:45:24.761527Z",
     "shell.execute_reply": "2023-02-17T14:45:24.760701Z"
    },
    "papermill": {
     "duration": 0.01795,
     "end_time": "2023-02-17T14:45:24.763449",
     "exception": false,
     "start_time": "2023-02-17T14:45:24.745499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(fold_dict,fold,model_name,sc_df,tar_col,optim,k_folds,best_params,tar_cols=\"\",verbose=1):\n",
    "\n",
    "    ''' this function is used to train the model with parameters optimization using optuna and cross validation using stratified k_folds'''\n",
    "\n",
    "    y = sc_df[tar_col]\n",
    "    print(y.shape)\n",
    "    x = sc_df.drop([tar_col],axis=1)\n",
    "    print(x.shape)\n",
    "    model_name = model_name \n",
    "    train_index = fold_dict[fold][\"train\"]\n",
    "    test_index = fold_dict[fold][\"test\"]\n",
    "    X_train,X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "    # print(X_train.shape, X_test.shape)\n",
    "    X_train, X_test = X_train.to_numpy(dtype=np.float64), X_test.to_numpy(dtype=np.float64)\n",
    "    Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    Y_train, Y_test = Y_train.to_numpy(dtype=np.float64), Y_test.to_numpy(dtype=np.float64)\n",
    "    clf_model = TabNetRegressor(**best_params,device_name= \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    clf_model.fit(X_train,Y_train)\n",
    "    Y_pred = clf_model.predict(X_test)\n",
    "    error = {\n",
    "    \"mse_error\" : mean_squared_error(Y_pred, Y_test, squared=False),\n",
    "    \"mae_error\" : mean_absolute_error(Y_pred,Y_test),\n",
    "    \"rmse_error\" : mean_squared_error(Y_pred, Y_test),\n",
    "    \"r2_score\" : r2_score(Y_pred,Y_test) }\n",
    "    # try:\n",
    "    print(\"[++] Saving the model and parameters in corresponding directories\")\n",
    "    save_cv_model(fold,model_name,clf_model,optim,mse_loss=error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf16237a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T14:45:24.777126Z",
     "iopub.status.busy": "2023-02-17T14:45:24.776269Z",
     "iopub.status.idle": "2023-02-17T20:00:10.079413Z",
     "shell.execute_reply": "2023-02-17T20:00:10.078004Z"
    },
    "papermill": {
     "duration": 18885.3124,
     "end_time": "2023-02-17T20:00:10.081794",
     "exception": false,
     "start_time": "2023-02-17T14:45:24.769394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:45:27,105]\u001b[0m A new study created in memory with name: no-name-750da9dc-61e8-431f-ab01-6dc907d6a244\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46820,)\n",
      "(46820, 103)\n",
      "Starting optimization for fold : [18/20]\n",
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 135.87072|  0:00:05s\n",
      "epoch 1  | loss: 74.57016|  0:00:10s\n",
      "epoch 2  | loss: 58.03098|  0:00:15s\n",
      "epoch 3  | loss: 31.13192|  0:00:20s\n",
      "epoch 4  | loss: 24.87486|  0:00:26s\n",
      "epoch 5  | loss: 20.03892|  0:00:31s\n",
      "epoch 6  | loss: 17.89666|  0:00:36s\n",
      "epoch 7  | loss: 16.29748|  0:00:40s\n",
      "epoch 8  | loss: 14.77788|  0:00:45s\n",
      "epoch 9  | loss: 14.31684|  0:00:50s\n",
      "epoch 10 | loss: 13.65709|  0:00:56s\n",
      "epoch 11 | loss: 12.87542|  0:01:00s\n",
      "epoch 12 | loss: 12.6173 |  0:01:05s\n",
      "epoch 13 | loss: 12.11638|  0:01:10s\n",
      "epoch 14 | loss: 11.87871|  0:01:15s\n",
      "epoch 15 | loss: 11.67392|  0:01:20s\n",
      "epoch 16 | loss: 11.17395|  0:01:25s\n",
      "epoch 17 | loss: 10.95202|  0:01:30s\n",
      "epoch 18 | loss: 10.62529|  0:01:35s\n",
      "epoch 19 | loss: 10.41418|  0:01:40s\n",
      "epoch 20 | loss: 10.18635|  0:01:44s\n",
      "epoch 21 | loss: 10.22864|  0:01:49s\n",
      "epoch 22 | loss: 9.87715 |  0:01:54s\n",
      "epoch 23 | loss: 9.87845 |  0:01:59s\n",
      "epoch 24 | loss: 9.43164 |  0:02:04s\n",
      "epoch 25 | loss: 9.6484  |  0:02:09s\n",
      "epoch 26 | loss: 9.32056 |  0:02:14s\n",
      "epoch 27 | loss: 9.20234 |  0:02:18s\n",
      "epoch 28 | loss: 9.00374 |  0:02:23s\n",
      "epoch 29 | loss: 8.85027 |  0:02:29s\n",
      "epoch 30 | loss: 8.58632 |  0:02:33s\n",
      "epoch 31 | loss: 8.58423 |  0:02:38s\n",
      "epoch 32 | loss: 8.50512 |  0:02:43s\n",
      "epoch 33 | loss: 8.57682 |  0:02:48s\n",
      "epoch 34 | loss: 8.21416 |  0:02:53s\n",
      "epoch 35 | loss: 8.10813 |  0:02:58s\n",
      "epoch 36 | loss: 8.25265 |  0:03:03s\n",
      "epoch 37 | loss: 7.83114 |  0:03:08s\n",
      "epoch 38 | loss: 7.75082 |  0:03:13s\n",
      "epoch 39 | loss: 7.73841 |  0:03:17s\n",
      "epoch 40 | loss: 7.95033 |  0:03:22s\n",
      "epoch 41 | loss: 7.51938 |  0:03:27s\n",
      "epoch 42 | loss: 7.56724 |  0:03:32s\n",
      "epoch 43 | loss: 7.42597 |  0:03:37s\n",
      "epoch 44 | loss: 7.41461 |  0:03:42s\n",
      "epoch 45 | loss: 7.30931 |  0:03:47s\n",
      "epoch 46 | loss: 7.36675 |  0:03:52s\n",
      "epoch 47 | loss: 7.46697 |  0:03:56s\n",
      "epoch 48 | loss: 7.57926 |  0:04:01s\n",
      "epoch 49 | loss: 7.18235 |  0:04:07s\n",
      "epoch 50 | loss: 7.09263 |  0:04:11s\n",
      "epoch 51 | loss: 7.02499 |  0:04:16s\n",
      "epoch 52 | loss: 6.78969 |  0:04:21s\n",
      "epoch 53 | loss: 6.96061 |  0:04:26s\n",
      "epoch 54 | loss: 6.82261 |  0:04:31s\n",
      "epoch 55 | loss: 6.83254 |  0:04:36s\n",
      "epoch 56 | loss: 6.84937 |  0:04:41s\n",
      "epoch 57 | loss: 6.66039 |  0:04:46s\n",
      "epoch 58 | loss: 6.49494 |  0:04:51s\n",
      "epoch 59 | loss: 6.50721 |  0:04:55s\n",
      "epoch 60 | loss: 6.88896 |  0:05:00s\n",
      "epoch 61 | loss: 6.61131 |  0:05:05s\n",
      "epoch 62 | loss: 6.29444 |  0:05:10s\n",
      "epoch 63 | loss: 6.23172 |  0:05:15s\n",
      "epoch 64 | loss: 6.4042  |  0:05:20s\n",
      "epoch 65 | loss: 6.23816 |  0:05:25s\n",
      "epoch 66 | loss: 6.07514 |  0:05:29s\n",
      "epoch 67 | loss: 5.9843  |  0:05:34s\n",
      "epoch 68 | loss: 6.07941 |  0:05:40s\n",
      "epoch 69 | loss: 5.85014 |  0:05:45s\n",
      "epoch 70 | loss: 5.868   |  0:05:49s\n",
      "epoch 71 | loss: 5.95551 |  0:05:55s\n",
      "epoch 72 | loss: 5.96525 |  0:05:59s\n",
      "epoch 73 | loss: 5.73015 |  0:06:04s\n",
      "epoch 74 | loss: 5.78397 |  0:06:09s\n",
      "epoch 75 | loss: 5.70441 |  0:06:14s\n",
      "epoch 76 | loss: 5.55161 |  0:06:19s\n",
      "epoch 77 | loss: 5.62775 |  0:06:24s\n",
      "epoch 78 | loss: 5.5117  |  0:06:29s\n",
      "epoch 79 | loss: 5.45928 |  0:06:34s\n",
      "epoch 80 | loss: 5.51948 |  0:06:39s\n",
      "epoch 81 | loss: 5.54334 |  0:06:44s\n",
      "epoch 82 | loss: 5.44936 |  0:06:49s\n",
      "epoch 83 | loss: 5.53756 |  0:06:53s\n",
      "epoch 84 | loss: 5.5005  |  0:06:59s\n",
      "epoch 85 | loss: 5.70003 |  0:07:03s\n",
      "epoch 86 | loss: 5.63708 |  0:07:08s\n",
      "epoch 87 | loss: 5.25346 |  0:07:13s\n",
      "epoch 88 | loss: 5.51545 |  0:07:18s\n",
      "epoch 89 | loss: 6.0281  |  0:07:23s\n",
      "epoch 90 | loss: 5.37099 |  0:07:28s\n",
      "epoch 91 | loss: 5.41532 |  0:07:33s\n",
      "epoch 92 | loss: 5.37946 |  0:07:38s\n",
      "epoch 93 | loss: 5.49565 |  0:07:43s\n",
      "epoch 94 | loss: 5.33928 |  0:07:48s\n",
      "epoch 95 | loss: 5.50424 |  0:07:53s\n",
      "epoch 96 | loss: 5.24073 |  0:07:58s\n",
      "epoch 97 | loss: 5.27615 |  0:08:03s\n",
      "epoch 98 | loss: 5.08304 |  0:08:08s\n",
      "epoch 99 | loss: 5.39561 |  0:08:13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 14:53:48,833]\u001b[0m Trial 0 finished with value: 2.7376944614398324 and parameters: {'n_d': 46, 'n_a': 59, 'n_steps': 10, 'gamma': 1.6768572281509007, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.31161473810203194}. Best is trial 0 with value: 2.7376944614398324.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 131.47558|  0:00:04s\n",
      "epoch 1  | loss: 46.20409|  0:00:10s\n",
      "epoch 2  | loss: 44.90423|  0:00:15s\n",
      "epoch 3  | loss: 42.99662|  0:00:20s\n",
      "epoch 4  | loss: 48.42155|  0:00:25s\n",
      "epoch 5  | loss: 43.63635|  0:00:31s\n",
      "epoch 6  | loss: 39.76204|  0:00:36s\n",
      "epoch 7  | loss: 38.63186|  0:00:41s\n",
      "epoch 8  | loss: 32.72553|  0:00:46s\n",
      "epoch 9  | loss: 35.68605|  0:00:51s\n",
      "epoch 10 | loss: 40.97946|  0:00:56s\n",
      "epoch 11 | loss: 37.18357|  0:01:01s\n",
      "epoch 12 | loss: 27.45212|  0:01:06s\n",
      "epoch 13 | loss: 25.7515 |  0:01:11s\n",
      "epoch 14 | loss: 20.21089|  0:01:17s\n",
      "epoch 15 | loss: 18.16614|  0:01:22s\n",
      "epoch 16 | loss: 17.67696|  0:01:27s\n",
      "epoch 17 | loss: 16.7119 |  0:01:33s\n",
      "epoch 18 | loss: 15.76728|  0:01:38s\n",
      "epoch 19 | loss: 15.04975|  0:01:43s\n",
      "epoch 20 | loss: 14.44825|  0:01:48s\n",
      "epoch 21 | loss: 13.86296|  0:01:53s\n",
      "epoch 22 | loss: 13.58109|  0:01:58s\n",
      "epoch 23 | loss: 13.23146|  0:02:03s\n",
      "epoch 24 | loss: 12.96301|  0:02:08s\n",
      "epoch 25 | loss: 12.6524 |  0:02:13s\n",
      "epoch 26 | loss: 12.47949|  0:02:18s\n",
      "epoch 27 | loss: 12.53805|  0:02:23s\n",
      "epoch 28 | loss: 12.32999|  0:02:28s\n",
      "epoch 29 | loss: 12.51541|  0:02:34s\n",
      "epoch 30 | loss: 12.48985|  0:02:39s\n",
      "epoch 31 | loss: 12.28031|  0:02:44s\n",
      "epoch 32 | loss: 11.88356|  0:02:49s\n",
      "epoch 33 | loss: 11.66002|  0:02:54s\n",
      "epoch 34 | loss: 11.37499|  0:02:59s\n",
      "epoch 35 | loss: 11.59131|  0:03:05s\n",
      "epoch 36 | loss: 10.70198|  0:03:10s\n",
      "epoch 37 | loss: 10.80186|  0:03:15s\n",
      "epoch 38 | loss: 10.54056|  0:03:20s\n",
      "epoch 39 | loss: 10.41469|  0:03:25s\n",
      "epoch 40 | loss: 10.2717 |  0:03:30s\n",
      "epoch 41 | loss: 10.64218|  0:03:35s\n",
      "epoch 42 | loss: 10.20245|  0:03:40s\n",
      "epoch 43 | loss: 9.8961  |  0:03:45s\n",
      "epoch 44 | loss: 10.07121|  0:03:51s\n",
      "epoch 45 | loss: 9.73888 |  0:03:55s\n",
      "epoch 46 | loss: 9.65212 |  0:04:01s\n",
      "epoch 47 | loss: 9.46264 |  0:04:06s\n",
      "epoch 48 | loss: 9.46416 |  0:04:11s\n",
      "epoch 49 | loss: 9.46254 |  0:04:16s\n",
      "epoch 50 | loss: 9.38438 |  0:04:21s\n",
      "epoch 51 | loss: 9.37931 |  0:04:26s\n",
      "epoch 52 | loss: 9.27532 |  0:04:31s\n",
      "epoch 53 | loss: 9.20938 |  0:04:37s\n",
      "epoch 54 | loss: 9.07004 |  0:04:42s\n",
      "epoch 55 | loss: 9.17366 |  0:04:47s\n",
      "epoch 56 | loss: 8.89252 |  0:04:52s\n",
      "epoch 57 | loss: 8.7293  |  0:04:57s\n",
      "epoch 58 | loss: 9.19282 |  0:05:02s\n",
      "epoch 59 | loss: 8.89854 |  0:05:07s\n",
      "epoch 60 | loss: 8.66461 |  0:05:12s\n",
      "epoch 61 | loss: 8.55851 |  0:05:17s\n",
      "epoch 62 | loss: 8.44164 |  0:05:22s\n",
      "epoch 63 | loss: 8.86029 |  0:05:28s\n",
      "epoch 64 | loss: 8.48683 |  0:05:32s\n",
      "epoch 65 | loss: 8.63584 |  0:05:38s\n",
      "epoch 66 | loss: 8.9828  |  0:05:43s\n",
      "epoch 67 | loss: 8.67113 |  0:05:48s\n",
      "epoch 68 | loss: 8.52112 |  0:05:53s\n",
      "epoch 69 | loss: 8.18232 |  0:05:58s\n",
      "epoch 70 | loss: 8.12004 |  0:06:03s\n",
      "epoch 71 | loss: 8.05939 |  0:06:08s\n",
      "epoch 72 | loss: 8.10477 |  0:06:14s\n",
      "epoch 73 | loss: 8.07169 |  0:06:19s\n",
      "epoch 74 | loss: 8.20813 |  0:06:24s\n",
      "epoch 75 | loss: 7.98053 |  0:06:28s\n",
      "epoch 76 | loss: 7.90717 |  0:06:34s\n",
      "epoch 77 | loss: 7.69606 |  0:06:39s\n",
      "epoch 78 | loss: 7.8055  |  0:06:44s\n",
      "epoch 79 | loss: 7.70063 |  0:06:49s\n",
      "epoch 80 | loss: 7.78883 |  0:06:54s\n",
      "epoch 81 | loss: 7.67998 |  0:06:59s\n",
      "epoch 82 | loss: 7.57    |  0:07:04s\n",
      "epoch 83 | loss: 7.36312 |  0:07:10s\n",
      "epoch 84 | loss: 7.35352 |  0:07:15s\n",
      "epoch 85 | loss: 7.77737 |  0:07:20s\n",
      "epoch 86 | loss: 7.39777 |  0:07:25s\n",
      "epoch 87 | loss: 7.27369 |  0:07:30s\n",
      "epoch 88 | loss: 7.10528 |  0:07:35s\n",
      "epoch 89 | loss: 7.10356 |  0:07:40s\n",
      "epoch 90 | loss: 7.25577 |  0:07:46s\n",
      "epoch 91 | loss: 7.03895 |  0:07:51s\n",
      "epoch 92 | loss: 7.09431 |  0:07:56s\n",
      "epoch 93 | loss: 6.95852 |  0:08:01s\n",
      "epoch 94 | loss: 6.75819 |  0:08:06s\n",
      "epoch 95 | loss: 6.88439 |  0:08:11s\n",
      "epoch 96 | loss: 7.04734 |  0:08:17s\n",
      "epoch 97 | loss: 6.82284 |  0:08:22s\n",
      "epoch 98 | loss: 6.71276 |  0:08:27s\n",
      "epoch 99 | loss: 7.04996 |  0:08:32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:02:25,445]\u001b[0m Trial 1 finished with value: 2.8661875269913644 and parameters: {'n_d': 29, 'n_a': 46, 'n_steps': 8, 'gamma': 1.194379512158311, 'n_independent': 2, 'n_shared': 5, 'momentum': 0.2713129813806114}. Best is trial 0 with value: 2.7376944614398324.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 185.82007|  0:00:06s\n",
      "epoch 1  | loss: 49.41424|  0:00:12s\n",
      "epoch 2  | loss: 40.12385|  0:00:19s\n",
      "epoch 3  | loss: 54.64799|  0:00:25s\n",
      "epoch 4  | loss: 56.58602|  0:00:31s\n",
      "epoch 5  | loss: 35.74104|  0:00:37s\n",
      "epoch 6  | loss: 38.07512|  0:00:44s\n",
      "epoch 7  | loss: 32.06285|  0:00:51s\n",
      "epoch 8  | loss: 30.16225|  0:00:57s\n",
      "epoch 9  | loss: 23.70241|  0:01:03s\n",
      "epoch 10 | loss: 35.80789|  0:01:09s\n",
      "epoch 11 | loss: 29.7848 |  0:01:16s\n",
      "epoch 12 | loss: 24.79921|  0:01:23s\n",
      "epoch 13 | loss: 18.88185|  0:01:29s\n",
      "epoch 14 | loss: 18.49406|  0:01:35s\n",
      "epoch 15 | loss: 17.72426|  0:01:41s\n",
      "epoch 16 | loss: 24.3387 |  0:01:48s\n",
      "epoch 17 | loss: 17.44108|  0:01:54s\n",
      "epoch 18 | loss: 16.74611|  0:02:01s\n",
      "epoch 19 | loss: 22.73212|  0:02:07s\n",
      "epoch 20 | loss: 16.5119 |  0:02:13s\n",
      "epoch 21 | loss: 14.09982|  0:02:20s\n",
      "epoch 22 | loss: 13.7051 |  0:02:26s\n",
      "epoch 23 | loss: 12.58406|  0:02:33s\n",
      "epoch 24 | loss: 12.08306|  0:02:39s\n",
      "epoch 25 | loss: 11.98569|  0:02:45s\n",
      "epoch 26 | loss: 11.62391|  0:02:51s\n",
      "epoch 27 | loss: 11.29998|  0:02:58s\n",
      "epoch 28 | loss: 11.34438|  0:03:04s\n",
      "epoch 29 | loss: 10.8457 |  0:03:10s\n",
      "epoch 30 | loss: 10.63702|  0:03:17s\n",
      "epoch 31 | loss: 10.83779|  0:03:23s\n",
      "epoch 32 | loss: 10.6011 |  0:03:30s\n",
      "epoch 33 | loss: 10.512  |  0:03:36s\n",
      "epoch 34 | loss: 10.5148 |  0:03:42s\n",
      "epoch 35 | loss: 10.21099|  0:03:48s\n",
      "epoch 36 | loss: 9.93313 |  0:03:55s\n",
      "epoch 37 | loss: 9.84065 |  0:04:01s\n",
      "epoch 38 | loss: 9.79837 |  0:04:07s\n",
      "epoch 39 | loss: 10.16984|  0:04:14s\n",
      "epoch 40 | loss: 9.82632 |  0:04:20s\n",
      "epoch 41 | loss: 10.06334|  0:04:27s\n",
      "epoch 42 | loss: 10.29457|  0:04:33s\n",
      "epoch 43 | loss: 10.2867 |  0:04:39s\n",
      "epoch 44 | loss: 9.68776 |  0:04:46s\n",
      "epoch 45 | loss: 9.41991 |  0:04:52s\n",
      "epoch 46 | loss: 9.08379 |  0:04:59s\n",
      "epoch 47 | loss: 8.8647  |  0:05:05s\n",
      "epoch 48 | loss: 8.88791 |  0:05:11s\n",
      "epoch 49 | loss: 8.72454 |  0:05:18s\n",
      "epoch 50 | loss: 8.52179 |  0:05:24s\n",
      "epoch 51 | loss: 8.35589 |  0:05:30s\n",
      "epoch 52 | loss: 8.46605 |  0:05:37s\n",
      "epoch 53 | loss: 8.38599 |  0:05:43s\n",
      "epoch 54 | loss: 8.31287 |  0:05:49s\n",
      "epoch 55 | loss: 8.38307 |  0:05:56s\n",
      "epoch 56 | loss: 8.32299 |  0:06:02s\n",
      "epoch 57 | loss: 8.02614 |  0:06:08s\n",
      "epoch 58 | loss: 7.98021 |  0:06:15s\n",
      "epoch 59 | loss: 7.91761 |  0:06:21s\n",
      "epoch 60 | loss: 7.9385  |  0:06:28s\n",
      "epoch 61 | loss: 7.81951 |  0:06:34s\n",
      "epoch 62 | loss: 7.5296  |  0:06:40s\n",
      "epoch 63 | loss: 7.60861 |  0:06:47s\n",
      "epoch 64 | loss: 7.53789 |  0:06:53s\n",
      "epoch 65 | loss: 7.53974 |  0:07:00s\n",
      "epoch 66 | loss: 7.61123 |  0:07:06s\n",
      "epoch 67 | loss: 7.4825  |  0:07:12s\n",
      "epoch 68 | loss: 7.29589 |  0:07:19s\n",
      "epoch 69 | loss: 7.31825 |  0:07:25s\n",
      "epoch 70 | loss: 7.17746 |  0:07:32s\n",
      "epoch 71 | loss: 7.21645 |  0:07:38s\n",
      "epoch 72 | loss: 7.09622 |  0:07:44s\n",
      "epoch 73 | loss: 7.04703 |  0:07:50s\n",
      "epoch 74 | loss: 7.05586 |  0:07:57s\n",
      "epoch 75 | loss: 7.38029 |  0:08:04s\n",
      "epoch 76 | loss: 6.88155 |  0:08:10s\n",
      "epoch 77 | loss: 7.27057 |  0:08:16s\n",
      "epoch 78 | loss: 6.86825 |  0:08:22s\n",
      "epoch 79 | loss: 6.78432 |  0:08:28s\n",
      "epoch 80 | loss: 6.73301 |  0:08:35s\n",
      "epoch 81 | loss: 6.58814 |  0:08:41s\n",
      "epoch 82 | loss: 6.51368 |  0:08:48s\n",
      "epoch 83 | loss: 6.69603 |  0:08:54s\n",
      "epoch 84 | loss: 6.63251 |  0:09:00s\n",
      "epoch 85 | loss: 6.48533 |  0:09:07s\n",
      "epoch 86 | loss: 6.50724 |  0:09:13s\n",
      "epoch 87 | loss: 6.57659 |  0:09:20s\n",
      "epoch 88 | loss: 6.36939 |  0:09:26s\n",
      "epoch 89 | loss: 6.60473 |  0:09:32s\n",
      "epoch 90 | loss: 6.29854 |  0:09:39s\n",
      "epoch 91 | loss: 6.2177  |  0:09:45s\n",
      "epoch 92 | loss: 6.35689 |  0:09:52s\n",
      "epoch 93 | loss: 6.314   |  0:09:58s\n",
      "epoch 94 | loss: 6.40606 |  0:10:04s\n",
      "epoch 95 | loss: 6.08694 |  0:10:11s\n",
      "epoch 96 | loss: 6.05572 |  0:10:17s\n",
      "epoch 97 | loss: 6.32541 |  0:10:23s\n",
      "epoch 98 | loss: 6.23279 |  0:10:30s\n",
      "epoch 99 | loss: 6.01838 |  0:10:36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:13:07,489]\u001b[0m Trial 2 finished with value: 2.693232858724422 and parameters: {'n_d': 55, 'n_a': 12, 'n_steps': 9, 'gamma': 1.1555724854026241, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.22773175035640097}. Best is trial 2 with value: 2.693232858724422.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 130.78336|  0:00:05s\n",
      "epoch 1  | loss: 62.17407|  0:00:10s\n",
      "epoch 2  | loss: 45.74977|  0:00:15s\n",
      "epoch 3  | loss: 39.65126|  0:00:20s\n",
      "epoch 4  | loss: 33.24732|  0:00:25s\n",
      "epoch 5  | loss: 36.00907|  0:00:31s\n",
      "epoch 6  | loss: 32.74789|  0:00:35s\n",
      "epoch 7  | loss: 29.00429|  0:00:41s\n",
      "epoch 8  | loss: 25.27809|  0:00:45s\n",
      "epoch 9  | loss: 22.02987|  0:00:51s\n",
      "epoch 10 | loss: 18.3945 |  0:00:56s\n",
      "epoch 11 | loss: 18.81481|  0:01:01s\n",
      "epoch 12 | loss: 27.27503|  0:01:06s\n",
      "epoch 13 | loss: 19.4143 |  0:01:12s\n",
      "epoch 14 | loss: 17.21715|  0:01:17s\n",
      "epoch 15 | loss: 16.07539|  0:01:22s\n",
      "epoch 16 | loss: 15.04928|  0:01:27s\n",
      "epoch 17 | loss: 16.03775|  0:01:33s\n",
      "epoch 18 | loss: 15.80899|  0:01:38s\n",
      "epoch 19 | loss: 14.39819|  0:01:43s\n",
      "epoch 20 | loss: 13.85783|  0:01:48s\n",
      "epoch 21 | loss: 13.3864 |  0:01:53s\n",
      "epoch 22 | loss: 12.67127|  0:01:58s\n",
      "epoch 23 | loss: 11.9877 |  0:02:04s\n",
      "epoch 24 | loss: 11.65168|  0:02:09s\n",
      "epoch 25 | loss: 11.36526|  0:02:14s\n",
      "epoch 26 | loss: 11.0764 |  0:02:19s\n",
      "epoch 27 | loss: 10.97125|  0:02:24s\n",
      "epoch 28 | loss: 11.18181|  0:02:29s\n",
      "epoch 29 | loss: 10.49689|  0:02:34s\n",
      "epoch 30 | loss: 10.42055|  0:02:40s\n",
      "epoch 31 | loss: 10.44014|  0:02:45s\n",
      "epoch 32 | loss: 10.28724|  0:02:50s\n",
      "epoch 33 | loss: 10.07647|  0:02:55s\n",
      "epoch 34 | loss: 10.07711|  0:03:00s\n",
      "epoch 35 | loss: 10.33006|  0:03:05s\n",
      "epoch 36 | loss: 9.90292 |  0:03:10s\n",
      "epoch 37 | loss: 9.86266 |  0:03:15s\n",
      "epoch 38 | loss: 9.84953 |  0:03:21s\n",
      "epoch 39 | loss: 9.60139 |  0:03:26s\n",
      "epoch 40 | loss: 9.53845 |  0:03:31s\n",
      "epoch 41 | loss: 9.38914 |  0:03:36s\n",
      "epoch 42 | loss: 9.18697 |  0:03:41s\n",
      "epoch 43 | loss: 9.19432 |  0:03:47s\n",
      "epoch 44 | loss: 9.05394 |  0:03:52s\n",
      "epoch 45 | loss: 8.71671 |  0:03:57s\n",
      "epoch 46 | loss: 8.82726 |  0:04:02s\n",
      "epoch 47 | loss: 8.78958 |  0:04:08s\n",
      "epoch 48 | loss: 8.74624 |  0:04:12s\n",
      "epoch 49 | loss: 8.50002 |  0:04:18s\n",
      "epoch 50 | loss: 8.88812 |  0:04:23s\n",
      "epoch 51 | loss: 8.62421 |  0:04:28s\n",
      "epoch 52 | loss: 8.37679 |  0:04:33s\n",
      "epoch 53 | loss: 8.28876 |  0:04:39s\n",
      "epoch 54 | loss: 8.22282 |  0:04:44s\n",
      "epoch 55 | loss: 8.1555  |  0:04:49s\n",
      "epoch 56 | loss: 8.17258 |  0:04:54s\n",
      "epoch 57 | loss: 8.16449 |  0:05:00s\n",
      "epoch 58 | loss: 8.04989 |  0:05:05s\n",
      "epoch 59 | loss: 8.08033 |  0:05:11s\n",
      "epoch 60 | loss: 7.85758 |  0:05:16s\n",
      "epoch 61 | loss: 8.11304 |  0:05:21s\n",
      "epoch 62 | loss: 7.84865 |  0:05:26s\n",
      "epoch 63 | loss: 7.75749 |  0:05:31s\n",
      "epoch 64 | loss: 7.60826 |  0:05:36s\n",
      "epoch 65 | loss: 7.51352 |  0:05:41s\n",
      "epoch 66 | loss: 7.5302  |  0:05:47s\n",
      "epoch 67 | loss: 7.40252 |  0:05:51s\n",
      "epoch 68 | loss: 7.48544 |  0:05:56s\n",
      "epoch 69 | loss: 7.57995 |  0:06:01s\n",
      "epoch 70 | loss: 7.39145 |  0:06:07s\n",
      "epoch 71 | loss: 7.41746 |  0:06:11s\n",
      "epoch 72 | loss: 7.36193 |  0:06:17s\n",
      "epoch 73 | loss: 7.15782 |  0:06:22s\n",
      "epoch 74 | loss: 7.18679 |  0:06:27s\n",
      "epoch 75 | loss: 7.09401 |  0:06:32s\n",
      "epoch 76 | loss: 7.18526 |  0:06:37s\n",
      "epoch 77 | loss: 7.07676 |  0:06:42s\n",
      "epoch 78 | loss: 6.99667 |  0:06:47s\n",
      "epoch 79 | loss: 7.10686 |  0:06:52s\n",
      "epoch 80 | loss: 6.99131 |  0:06:57s\n",
      "epoch 81 | loss: 7.09795 |  0:07:02s\n",
      "epoch 82 | loss: 7.06118 |  0:07:07s\n",
      "epoch 83 | loss: 7.19689 |  0:07:12s\n",
      "epoch 84 | loss: 6.89891 |  0:07:18s\n",
      "epoch 85 | loss: 7.50602 |  0:07:23s\n",
      "epoch 86 | loss: 7.01847 |  0:07:28s\n",
      "epoch 87 | loss: 6.87492 |  0:07:33s\n",
      "epoch 88 | loss: 7.07784 |  0:07:38s\n",
      "epoch 89 | loss: 7.06705 |  0:07:43s\n",
      "epoch 90 | loss: 7.57925 |  0:07:48s\n",
      "epoch 91 | loss: 7.18708 |  0:07:53s\n",
      "epoch 92 | loss: 7.03362 |  0:07:58s\n",
      "epoch 93 | loss: 7.17841 |  0:08:03s\n",
      "epoch 94 | loss: 6.83379 |  0:08:08s\n",
      "epoch 95 | loss: 6.82808 |  0:08:13s\n",
      "epoch 96 | loss: 6.90477 |  0:08:18s\n",
      "epoch 97 | loss: 6.63221 |  0:08:23s\n",
      "epoch 98 | loss: 6.74832 |  0:08:28s\n",
      "epoch 99 | loss: 6.93413 |  0:08:33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:21:45,365]\u001b[0m Trial 3 finished with value: 7.4211230836075215 and parameters: {'n_d': 23, 'n_a': 18, 'n_steps': 7, 'gamma': 1.4272670215385386, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.07796151816267341}. Best is trial 2 with value: 2.693232858724422.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 169.46079|  0:00:05s\n",
      "epoch 1  | loss: 50.39759|  0:00:11s\n",
      "epoch 2  | loss: 42.77008|  0:00:16s\n",
      "epoch 3  | loss: 44.79586|  0:00:22s\n",
      "epoch 4  | loss: 35.93852|  0:00:28s\n",
      "epoch 5  | loss: 31.94879|  0:00:33s\n",
      "epoch 6  | loss: 23.02367|  0:00:39s\n",
      "epoch 7  | loss: 17.23494|  0:00:45s\n",
      "epoch 8  | loss: 15.34003|  0:00:50s\n",
      "epoch 9  | loss: 14.3053 |  0:00:56s\n",
      "epoch 10 | loss: 13.49971|  0:01:01s\n",
      "epoch 11 | loss: 13.18831|  0:01:07s\n",
      "epoch 12 | loss: 12.68855|  0:01:12s\n",
      "epoch 13 | loss: 12.0029 |  0:01:18s\n",
      "epoch 14 | loss: 11.93382|  0:01:24s\n",
      "epoch 15 | loss: 11.61834|  0:01:29s\n",
      "epoch 16 | loss: 11.19638|  0:01:35s\n",
      "epoch 17 | loss: 10.74082|  0:01:40s\n",
      "epoch 18 | loss: 10.58771|  0:01:46s\n",
      "epoch 19 | loss: 10.55961|  0:01:52s\n",
      "epoch 20 | loss: 9.99132 |  0:01:57s\n",
      "epoch 21 | loss: 10.11291|  0:02:03s\n",
      "epoch 22 | loss: 9.52019 |  0:02:09s\n",
      "epoch 23 | loss: 9.74017 |  0:02:14s\n",
      "epoch 24 | loss: 9.32249 |  0:02:20s\n",
      "epoch 25 | loss: 9.42131 |  0:02:26s\n",
      "epoch 26 | loss: 9.01513 |  0:02:31s\n",
      "epoch 27 | loss: 8.85753 |  0:02:37s\n",
      "epoch 28 | loss: 8.65184 |  0:02:42s\n",
      "epoch 29 | loss: 9.00128 |  0:02:48s\n",
      "epoch 30 | loss: 8.83468 |  0:02:54s\n",
      "epoch 31 | loss: 8.41751 |  0:02:59s\n",
      "epoch 32 | loss: 8.43407 |  0:03:05s\n",
      "epoch 33 | loss: 8.13818 |  0:03:10s\n",
      "epoch 34 | loss: 8.24834 |  0:03:16s\n",
      "epoch 35 | loss: 8.09218 |  0:03:22s\n",
      "epoch 36 | loss: 7.88474 |  0:03:27s\n",
      "epoch 37 | loss: 7.96133 |  0:03:33s\n",
      "epoch 38 | loss: 7.77964 |  0:03:38s\n",
      "epoch 39 | loss: 7.48032 |  0:03:44s\n",
      "epoch 40 | loss: 7.6408  |  0:03:49s\n",
      "epoch 41 | loss: 7.39093 |  0:03:56s\n",
      "epoch 42 | loss: 7.20291 |  0:04:01s\n",
      "epoch 43 | loss: 7.20206 |  0:04:07s\n",
      "epoch 44 | loss: 7.13291 |  0:04:12s\n",
      "epoch 45 | loss: 7.10148 |  0:04:18s\n",
      "epoch 46 | loss: 7.14966 |  0:04:24s\n",
      "epoch 47 | loss: 7.04736 |  0:04:29s\n",
      "epoch 48 | loss: 7.18774 |  0:04:35s\n",
      "epoch 49 | loss: 6.79889 |  0:04:40s\n",
      "epoch 50 | loss: 6.86649 |  0:04:46s\n",
      "epoch 51 | loss: 6.85977 |  0:04:51s\n",
      "epoch 52 | loss: 6.83507 |  0:04:57s\n",
      "epoch 53 | loss: 6.7159  |  0:05:03s\n",
      "epoch 54 | loss: 6.70157 |  0:05:08s\n",
      "epoch 55 | loss: 6.90431 |  0:05:14s\n",
      "epoch 56 | loss: 6.65714 |  0:05:19s\n",
      "epoch 57 | loss: 6.57399 |  0:05:25s\n",
      "epoch 58 | loss: 6.58191 |  0:05:31s\n",
      "epoch 59 | loss: 6.65331 |  0:05:36s\n",
      "epoch 60 | loss: 6.46673 |  0:05:42s\n",
      "epoch 61 | loss: 6.06244 |  0:05:47s\n",
      "epoch 62 | loss: 6.32495 |  0:05:53s\n",
      "epoch 63 | loss: 6.0581  |  0:05:59s\n",
      "epoch 64 | loss: 6.26608 |  0:06:05s\n",
      "epoch 65 | loss: 6.30569 |  0:06:10s\n",
      "epoch 66 | loss: 6.55382 |  0:06:16s\n",
      "epoch 67 | loss: 6.36556 |  0:06:21s\n",
      "epoch 68 | loss: 6.34531 |  0:06:27s\n",
      "epoch 69 | loss: 6.3247  |  0:06:33s\n",
      "epoch 70 | loss: 6.06287 |  0:06:38s\n",
      "epoch 71 | loss: 6.36147 |  0:06:44s\n",
      "epoch 72 | loss: 5.83511 |  0:06:49s\n",
      "epoch 73 | loss: 5.89819 |  0:06:55s\n",
      "epoch 74 | loss: 5.78801 |  0:07:01s\n",
      "epoch 75 | loss: 5.78298 |  0:07:06s\n",
      "epoch 76 | loss: 5.60243 |  0:07:12s\n",
      "epoch 77 | loss: 5.92493 |  0:07:17s\n",
      "epoch 78 | loss: 5.59742 |  0:07:23s\n",
      "epoch 79 | loss: 5.57056 |  0:07:28s\n",
      "epoch 80 | loss: 5.47683 |  0:07:34s\n",
      "epoch 81 | loss: 5.42235 |  0:07:40s\n",
      "epoch 82 | loss: 5.55826 |  0:07:46s\n",
      "epoch 83 | loss: 5.41991 |  0:07:51s\n",
      "epoch 84 | loss: 5.54019 |  0:07:57s\n",
      "epoch 85 | loss: 5.38933 |  0:08:02s\n",
      "epoch 86 | loss: 5.37729 |  0:08:08s\n",
      "epoch 87 | loss: 5.3287  |  0:08:13s\n",
      "epoch 88 | loss: 5.38052 |  0:08:19s\n",
      "epoch 89 | loss: 5.38441 |  0:08:24s\n",
      "epoch 90 | loss: 5.36133 |  0:08:30s\n",
      "epoch 91 | loss: 5.128   |  0:08:36s\n",
      "epoch 92 | loss: 5.4364  |  0:08:42s\n",
      "epoch 93 | loss: 5.3609  |  0:08:47s\n",
      "epoch 94 | loss: 5.29697 |  0:08:53s\n",
      "epoch 95 | loss: 5.15928 |  0:08:58s\n",
      "epoch 96 | loss: 5.25913 |  0:09:04s\n",
      "epoch 97 | loss: 5.34035 |  0:09:10s\n",
      "epoch 98 | loss: 5.20202 |  0:09:15s\n",
      "epoch 99 | loss: 5.1051  |  0:09:21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:31:11,085]\u001b[0m Trial 4 finished with value: 2.4813181501130956 and parameters: {'n_d': 34, 'n_a': 59, 'n_steps': 7, 'gamma': 1.4401134890241982, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3256366679620535}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 137.94993|  0:00:02s\n",
      "epoch 1  | loss: 28.47361|  0:00:05s\n",
      "epoch 2  | loss: 27.32688|  0:00:08s\n",
      "epoch 3  | loss: 21.2129 |  0:00:12s\n",
      "epoch 4  | loss: 19.50534|  0:00:15s\n",
      "epoch 5  | loss: 19.9018 |  0:00:18s\n",
      "epoch 6  | loss: 15.01262|  0:00:21s\n",
      "epoch 7  | loss: 13.92252|  0:00:24s\n",
      "epoch 8  | loss: 13.0013 |  0:00:27s\n",
      "epoch 9  | loss: 12.30425|  0:00:30s\n",
      "epoch 10 | loss: 11.94195|  0:00:33s\n",
      "epoch 11 | loss: 13.03424|  0:00:36s\n",
      "epoch 12 | loss: 12.09897|  0:00:39s\n",
      "epoch 13 | loss: 11.32129|  0:00:42s\n",
      "epoch 14 | loss: 11.18639|  0:00:45s\n",
      "epoch 15 | loss: 10.92217|  0:00:48s\n",
      "epoch 16 | loss: 10.43623|  0:00:51s\n",
      "epoch 17 | loss: 10.46553|  0:00:54s\n",
      "epoch 18 | loss: 10.26705|  0:00:57s\n",
      "epoch 19 | loss: 10.30512|  0:01:00s\n",
      "epoch 20 | loss: 10.32593|  0:01:03s\n",
      "epoch 21 | loss: 10.06909|  0:01:06s\n",
      "epoch 22 | loss: 9.7401  |  0:01:09s\n",
      "epoch 23 | loss: 9.47081 |  0:01:12s\n",
      "epoch 24 | loss: 10.36673|  0:01:15s\n",
      "epoch 25 | loss: 9.60568 |  0:01:18s\n",
      "epoch 26 | loss: 9.33935 |  0:01:21s\n",
      "epoch 27 | loss: 9.30317 |  0:01:24s\n",
      "epoch 28 | loss: 9.06674 |  0:01:27s\n",
      "epoch 29 | loss: 9.24118 |  0:01:30s\n",
      "epoch 30 | loss: 8.7592  |  0:01:33s\n",
      "epoch 31 | loss: 8.79399 |  0:01:36s\n",
      "epoch 32 | loss: 8.76999 |  0:01:39s\n",
      "epoch 33 | loss: 8.75283 |  0:01:42s\n",
      "epoch 34 | loss: 8.66357 |  0:01:46s\n",
      "epoch 35 | loss: 8.55335 |  0:01:49s\n",
      "epoch 36 | loss: 8.51412 |  0:01:52s\n",
      "epoch 37 | loss: 8.33489 |  0:01:55s\n",
      "epoch 38 | loss: 8.19775 |  0:01:58s\n",
      "epoch 39 | loss: 8.18816 |  0:02:01s\n",
      "epoch 40 | loss: 7.93385 |  0:02:04s\n",
      "epoch 41 | loss: 7.95294 |  0:02:07s\n",
      "epoch 42 | loss: 7.93558 |  0:02:10s\n",
      "epoch 43 | loss: 7.8768  |  0:02:13s\n",
      "epoch 44 | loss: 7.57312 |  0:02:16s\n",
      "epoch 45 | loss: 7.66901 |  0:02:19s\n",
      "epoch 46 | loss: 7.77217 |  0:02:22s\n",
      "epoch 47 | loss: 7.5383  |  0:02:25s\n",
      "epoch 48 | loss: 7.59015 |  0:02:28s\n",
      "epoch 49 | loss: 7.42137 |  0:02:31s\n",
      "epoch 50 | loss: 7.20564 |  0:02:34s\n",
      "epoch 51 | loss: 7.27013 |  0:02:37s\n",
      "epoch 52 | loss: 7.05575 |  0:02:40s\n",
      "epoch 53 | loss: 7.17745 |  0:02:43s\n",
      "epoch 54 | loss: 6.91518 |  0:02:46s\n",
      "epoch 55 | loss: 6.91823 |  0:02:49s\n",
      "epoch 56 | loss: 6.78994 |  0:02:52s\n",
      "epoch 57 | loss: 6.95484 |  0:02:55s\n",
      "epoch 58 | loss: 6.90806 |  0:02:58s\n",
      "epoch 59 | loss: 6.88118 |  0:03:01s\n",
      "epoch 60 | loss: 6.68864 |  0:03:04s\n",
      "epoch 61 | loss: 6.70186 |  0:03:07s\n",
      "epoch 62 | loss: 6.64484 |  0:03:10s\n",
      "epoch 63 | loss: 6.61515 |  0:03:13s\n",
      "epoch 64 | loss: 6.50726 |  0:03:16s\n",
      "epoch 65 | loss: 6.51616 |  0:03:20s\n",
      "epoch 66 | loss: 6.4193  |  0:03:23s\n",
      "epoch 67 | loss: 6.36963 |  0:03:26s\n",
      "epoch 68 | loss: 6.49717 |  0:03:29s\n",
      "epoch 69 | loss: 6.31028 |  0:03:32s\n",
      "epoch 70 | loss: 6.12571 |  0:03:35s\n",
      "epoch 71 | loss: 6.17841 |  0:03:38s\n",
      "epoch 72 | loss: 6.09081 |  0:03:41s\n",
      "epoch 73 | loss: 6.29806 |  0:03:44s\n",
      "epoch 74 | loss: 6.59474 |  0:03:47s\n",
      "epoch 75 | loss: 6.19538 |  0:03:50s\n",
      "epoch 76 | loss: 6.07685 |  0:03:53s\n",
      "epoch 77 | loss: 5.88638 |  0:03:56s\n",
      "epoch 78 | loss: 5.92002 |  0:03:59s\n",
      "epoch 79 | loss: 6.15447 |  0:04:02s\n",
      "epoch 80 | loss: 5.85181 |  0:04:05s\n",
      "epoch 81 | loss: 5.92453 |  0:04:08s\n",
      "epoch 82 | loss: 5.86499 |  0:04:11s\n",
      "epoch 83 | loss: 5.62241 |  0:04:14s\n",
      "epoch 84 | loss: 5.7133  |  0:04:17s\n",
      "epoch 85 | loss: 5.73211 |  0:04:20s\n",
      "epoch 86 | loss: 5.79406 |  0:04:23s\n",
      "epoch 87 | loss: 5.67258 |  0:04:26s\n",
      "epoch 88 | loss: 5.91295 |  0:04:29s\n",
      "epoch 89 | loss: 5.64269 |  0:04:32s\n",
      "epoch 90 | loss: 5.96525 |  0:04:35s\n",
      "epoch 91 | loss: 6.09383 |  0:04:38s\n",
      "epoch 92 | loss: 5.70994 |  0:04:41s\n",
      "epoch 93 | loss: 5.80434 |  0:04:44s\n",
      "epoch 94 | loss: 5.60288 |  0:04:47s\n",
      "epoch 95 | loss: 5.61856 |  0:04:49s\n",
      "epoch 96 | loss: 5.64832 |  0:04:53s\n",
      "epoch 97 | loss: 5.41232 |  0:04:56s\n",
      "epoch 98 | loss: 5.53367 |  0:04:59s\n",
      "epoch 99 | loss: 5.51862 |  0:05:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:36:15,505]\u001b[0m Trial 5 finished with value: 2.604970366636673 and parameters: {'n_d': 16, 'n_a': 62, 'n_steps': 4, 'gamma': 1.4058017182400961, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.18685323700714593}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 103.5438|  0:00:02s\n",
      "epoch 1  | loss: 25.7121 |  0:00:04s\n",
      "epoch 2  | loss: 20.86667|  0:00:06s\n",
      "epoch 3  | loss: 18.59358|  0:00:08s\n",
      "epoch 4  | loss: 18.20337|  0:00:10s\n",
      "epoch 5  | loss: 17.20666|  0:00:12s\n",
      "epoch 6  | loss: 15.70001|  0:00:14s\n",
      "epoch 7  | loss: 16.00275|  0:00:16s\n",
      "epoch 8  | loss: 13.6258 |  0:00:18s\n",
      "epoch 9  | loss: 13.43431|  0:00:20s\n",
      "epoch 10 | loss: 13.10573|  0:00:22s\n",
      "epoch 11 | loss: 12.09944|  0:00:24s\n",
      "epoch 12 | loss: 12.08585|  0:00:27s\n",
      "epoch 13 | loss: 11.92381|  0:00:29s\n",
      "epoch 14 | loss: 11.59421|  0:00:31s\n",
      "epoch 15 | loss: 10.73031|  0:00:33s\n",
      "epoch 16 | loss: 11.41164|  0:00:35s\n",
      "epoch 17 | loss: 10.6822 |  0:00:37s\n",
      "epoch 18 | loss: 10.1717 |  0:00:39s\n",
      "epoch 19 | loss: 9.8116  |  0:00:41s\n",
      "epoch 20 | loss: 9.70935 |  0:00:43s\n",
      "epoch 21 | loss: 9.36537 |  0:00:45s\n",
      "epoch 22 | loss: 9.1484  |  0:00:47s\n",
      "epoch 23 | loss: 8.87391 |  0:00:49s\n",
      "epoch 24 | loss: 8.93114 |  0:00:52s\n",
      "epoch 25 | loss: 8.75296 |  0:00:54s\n",
      "epoch 26 | loss: 8.93548 |  0:00:56s\n",
      "epoch 27 | loss: 8.89887 |  0:00:58s\n",
      "epoch 28 | loss: 8.79346 |  0:01:00s\n",
      "epoch 29 | loss: 9.2237  |  0:01:02s\n",
      "epoch 30 | loss: 8.84561 |  0:01:04s\n",
      "epoch 31 | loss: 8.60941 |  0:01:06s\n",
      "epoch 32 | loss: 8.45136 |  0:01:08s\n",
      "epoch 33 | loss: 8.04777 |  0:01:10s\n",
      "epoch 34 | loss: 8.47686 |  0:01:12s\n",
      "epoch 35 | loss: 8.12931 |  0:01:15s\n",
      "epoch 36 | loss: 7.80717 |  0:01:17s\n",
      "epoch 37 | loss: 7.56806 |  0:01:18s\n",
      "epoch 38 | loss: 7.46191 |  0:01:20s\n",
      "epoch 39 | loss: 7.39968 |  0:01:23s\n",
      "epoch 40 | loss: 7.52241 |  0:01:25s\n",
      "epoch 41 | loss: 7.28773 |  0:01:27s\n",
      "epoch 42 | loss: 7.13813 |  0:01:29s\n",
      "epoch 43 | loss: 7.10385 |  0:01:31s\n",
      "epoch 44 | loss: 6.71665 |  0:01:33s\n",
      "epoch 45 | loss: 6.72242 |  0:01:35s\n",
      "epoch 46 | loss: 6.75493 |  0:01:38s\n",
      "epoch 47 | loss: 6.76044 |  0:01:40s\n",
      "epoch 48 | loss: 6.64876 |  0:01:42s\n",
      "epoch 49 | loss: 6.47756 |  0:01:44s\n",
      "epoch 50 | loss: 6.51763 |  0:01:46s\n",
      "epoch 51 | loss: 6.55262 |  0:01:48s\n",
      "epoch 52 | loss: 6.45839 |  0:01:50s\n",
      "epoch 53 | loss: 6.41058 |  0:01:52s\n",
      "epoch 54 | loss: 6.24869 |  0:01:55s\n",
      "epoch 55 | loss: 6.30688 |  0:01:57s\n",
      "epoch 56 | loss: 6.43484 |  0:01:59s\n",
      "epoch 57 | loss: 6.62456 |  0:02:01s\n",
      "epoch 58 | loss: 6.46435 |  0:02:03s\n",
      "epoch 59 | loss: 6.47998 |  0:02:05s\n",
      "epoch 60 | loss: 6.49346 |  0:02:07s\n",
      "epoch 61 | loss: 6.12384 |  0:02:09s\n",
      "epoch 62 | loss: 6.44648 |  0:02:11s\n",
      "epoch 63 | loss: 6.10905 |  0:02:13s\n",
      "epoch 64 | loss: 5.8768  |  0:02:16s\n",
      "epoch 65 | loss: 5.89054 |  0:02:18s\n",
      "epoch 66 | loss: 5.87791 |  0:02:20s\n",
      "epoch 67 | loss: 5.92109 |  0:02:22s\n",
      "epoch 68 | loss: 5.80575 |  0:02:24s\n",
      "epoch 69 | loss: 5.56662 |  0:02:26s\n",
      "epoch 70 | loss: 5.55154 |  0:02:28s\n",
      "epoch 71 | loss: 5.54446 |  0:02:31s\n",
      "epoch 72 | loss: 5.53977 |  0:02:33s\n",
      "epoch 73 | loss: 5.47009 |  0:02:35s\n",
      "epoch 74 | loss: 5.56302 |  0:02:37s\n",
      "epoch 75 | loss: 5.35585 |  0:02:39s\n",
      "epoch 76 | loss: 5.38158 |  0:02:41s\n",
      "epoch 77 | loss: 5.38722 |  0:02:43s\n",
      "epoch 78 | loss: 5.28104 |  0:02:45s\n",
      "epoch 79 | loss: 5.34506 |  0:02:47s\n",
      "epoch 80 | loss: 5.29976 |  0:02:49s\n",
      "epoch 81 | loss: 5.40605 |  0:02:51s\n",
      "epoch 82 | loss: 5.28824 |  0:02:53s\n",
      "epoch 83 | loss: 5.09198 |  0:02:55s\n",
      "epoch 84 | loss: 5.04461 |  0:02:58s\n",
      "epoch 85 | loss: 5.02583 |  0:03:00s\n",
      "epoch 86 | loss: 5.07824 |  0:03:02s\n",
      "epoch 87 | loss: 5.18921 |  0:03:04s\n",
      "epoch 88 | loss: 5.0528  |  0:03:06s\n",
      "epoch 89 | loss: 4.91327 |  0:03:08s\n",
      "epoch 90 | loss: 5.0329  |  0:03:10s\n",
      "epoch 91 | loss: 4.98224 |  0:03:12s\n",
      "epoch 92 | loss: 4.92124 |  0:03:14s\n",
      "epoch 93 | loss: 4.93596 |  0:03:16s\n",
      "epoch 94 | loss: 4.88594 |  0:03:18s\n",
      "epoch 95 | loss: 4.78639 |  0:03:20s\n",
      "epoch 96 | loss: 4.81117 |  0:03:22s\n",
      "epoch 97 | loss: 4.94491 |  0:03:25s\n",
      "epoch 98 | loss: 4.7951  |  0:03:27s\n",
      "epoch 99 | loss: 4.86475 |  0:03:29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:39:47,264]\u001b[0m Trial 6 finished with value: 2.6291827114250705 and parameters: {'n_d': 51, 'n_a': 42, 'n_steps': 4, 'gamma': 1.2890335708792262, 'n_independent': 3, 'n_shared': 1, 'momentum': 0.2031508725076623}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 102.11896|  0:00:03s\n",
      "epoch 1  | loss: 30.28227|  0:00:06s\n",
      "epoch 2  | loss: 25.55772|  0:00:09s\n",
      "epoch 3  | loss: 21.9701 |  0:00:12s\n",
      "epoch 4  | loss: 20.0968 |  0:00:15s\n",
      "epoch 5  | loss: 18.72867|  0:00:19s\n",
      "epoch 6  | loss: 17.45827|  0:00:22s\n",
      "epoch 7  | loss: 16.23214|  0:00:25s\n",
      "epoch 8  | loss: 32.38314|  0:00:29s\n",
      "epoch 9  | loss: 16.52136|  0:00:32s\n",
      "epoch 10 | loss: 14.8219 |  0:00:35s\n",
      "epoch 11 | loss: 14.47447|  0:00:38s\n",
      "epoch 12 | loss: 13.51331|  0:00:41s\n",
      "epoch 13 | loss: 12.82579|  0:00:44s\n",
      "epoch 14 | loss: 12.36967|  0:00:47s\n",
      "epoch 15 | loss: 11.83713|  0:00:51s\n",
      "epoch 16 | loss: 11.42009|  0:00:54s\n",
      "epoch 17 | loss: 10.99562|  0:00:57s\n",
      "epoch 18 | loss: 10.80021|  0:01:00s\n",
      "epoch 19 | loss: 10.52671|  0:01:03s\n",
      "epoch 20 | loss: 10.07658|  0:01:07s\n",
      "epoch 21 | loss: 10.16115|  0:01:10s\n",
      "epoch 22 | loss: 9.83332 |  0:01:13s\n",
      "epoch 23 | loss: 9.69449 |  0:01:16s\n",
      "epoch 24 | loss: 9.25829 |  0:01:19s\n",
      "epoch 25 | loss: 9.10464 |  0:01:23s\n",
      "epoch 26 | loss: 9.08609 |  0:01:26s\n",
      "epoch 27 | loss: 8.75161 |  0:01:29s\n",
      "epoch 28 | loss: 8.67699 |  0:01:33s\n",
      "epoch 29 | loss: 8.51734 |  0:01:36s\n",
      "epoch 30 | loss: 8.40469 |  0:01:39s\n",
      "epoch 31 | loss: 8.24442 |  0:01:42s\n",
      "epoch 32 | loss: 8.17197 |  0:01:45s\n",
      "epoch 33 | loss: 8.01072 |  0:01:49s\n",
      "epoch 34 | loss: 7.91927 |  0:01:52s\n",
      "epoch 35 | loss: 7.83628 |  0:01:55s\n",
      "epoch 36 | loss: 7.81936 |  0:01:58s\n",
      "epoch 37 | loss: 7.5392  |  0:02:01s\n",
      "epoch 38 | loss: 7.45893 |  0:02:04s\n",
      "epoch 39 | loss: 7.48183 |  0:02:08s\n",
      "epoch 40 | loss: 7.38778 |  0:02:11s\n",
      "epoch 41 | loss: 7.18602 |  0:02:14s\n",
      "epoch 42 | loss: 7.2193  |  0:02:17s\n",
      "epoch 43 | loss: 6.96638 |  0:02:20s\n",
      "epoch 44 | loss: 7.08553 |  0:02:23s\n",
      "epoch 45 | loss: 6.83778 |  0:02:26s\n",
      "epoch 46 | loss: 6.60854 |  0:02:30s\n",
      "epoch 47 | loss: 6.73881 |  0:02:33s\n",
      "epoch 48 | loss: 6.66233 |  0:02:36s\n",
      "epoch 49 | loss: 6.7537  |  0:02:40s\n",
      "epoch 50 | loss: 6.52814 |  0:02:43s\n",
      "epoch 51 | loss: 6.56765 |  0:02:46s\n",
      "epoch 52 | loss: 6.40428 |  0:02:49s\n",
      "epoch 53 | loss: 6.46211 |  0:02:52s\n",
      "epoch 54 | loss: 6.27407 |  0:02:55s\n",
      "epoch 55 | loss: 6.17321 |  0:02:58s\n",
      "epoch 56 | loss: 6.09809 |  0:03:02s\n",
      "epoch 57 | loss: 6.04717 |  0:03:05s\n",
      "epoch 58 | loss: 6.10551 |  0:03:08s\n",
      "epoch 59 | loss: 6.05714 |  0:03:12s\n",
      "epoch 60 | loss: 6.0823  |  0:03:15s\n",
      "epoch 61 | loss: 6.04534 |  0:03:18s\n",
      "epoch 62 | loss: 5.83039 |  0:03:21s\n",
      "epoch 63 | loss: 5.95311 |  0:03:24s\n",
      "epoch 64 | loss: 5.9104  |  0:03:27s\n",
      "epoch 65 | loss: 5.9307  |  0:03:30s\n",
      "epoch 66 | loss: 5.82917 |  0:03:34s\n",
      "epoch 67 | loss: 5.77282 |  0:03:37s\n",
      "epoch 68 | loss: 5.74747 |  0:03:40s\n",
      "epoch 69 | loss: 5.69448 |  0:03:43s\n",
      "epoch 70 | loss: 5.56112 |  0:03:47s\n",
      "epoch 71 | loss: 5.69519 |  0:03:50s\n",
      "epoch 72 | loss: 5.58434 |  0:03:53s\n",
      "epoch 73 | loss: 5.82352 |  0:03:56s\n",
      "epoch 74 | loss: 5.44058 |  0:03:59s\n",
      "epoch 75 | loss: 5.50298 |  0:04:03s\n",
      "epoch 76 | loss: 5.45725 |  0:04:06s\n",
      "epoch 77 | loss: 5.38546 |  0:04:09s\n",
      "epoch 78 | loss: 5.35655 |  0:04:13s\n",
      "epoch 79 | loss: 5.29142 |  0:04:16s\n",
      "epoch 80 | loss: 5.15553 |  0:04:19s\n",
      "epoch 81 | loss: 5.25679 |  0:04:22s\n",
      "epoch 82 | loss: 5.16562 |  0:04:25s\n",
      "epoch 83 | loss: 5.20575 |  0:04:29s\n",
      "epoch 84 | loss: 5.16822 |  0:04:32s\n",
      "epoch 85 | loss: 5.10999 |  0:04:35s\n",
      "epoch 86 | loss: 5.14155 |  0:04:38s\n",
      "epoch 87 | loss: 5.0157  |  0:04:42s\n",
      "epoch 88 | loss: 5.0696  |  0:04:45s\n",
      "epoch 89 | loss: 4.96685 |  0:04:48s\n",
      "epoch 90 | loss: 5.12483 |  0:04:51s\n",
      "epoch 91 | loss: 5.04936 |  0:04:54s\n",
      "epoch 92 | loss: 4.93538 |  0:04:57s\n",
      "epoch 93 | loss: 4.97328 |  0:05:00s\n",
      "epoch 94 | loss: 4.88671 |  0:05:04s\n",
      "epoch 95 | loss: 4.86267 |  0:05:07s\n",
      "epoch 96 | loss: 4.71704 |  0:05:10s\n",
      "epoch 97 | loss: 4.6919  |  0:05:14s\n",
      "epoch 98 | loss: 4.85099 |  0:05:17s\n",
      "epoch 99 | loss: 4.96284 |  0:05:20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:45:10,298]\u001b[0m Trial 7 finished with value: 2.613729115405267 and parameters: {'n_d': 28, 'n_a': 59, 'n_steps': 5, 'gamma': 1.2986491244003742, 'n_independent': 5, 'n_shared': 1, 'momentum': 0.021597847324575627}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 257.18877|  0:00:06s\n",
      "epoch 1  | loss: 82.23752|  0:00:12s\n",
      "epoch 2  | loss: 84.0789 |  0:00:19s\n",
      "epoch 3  | loss: 56.25086|  0:00:25s\n",
      "epoch 4  | loss: 27.32829|  0:00:31s\n",
      "epoch 5  | loss: 22.82038|  0:00:37s\n",
      "epoch 6  | loss: 18.7876 |  0:00:43s\n",
      "epoch 7  | loss: 17.50008|  0:00:50s\n",
      "epoch 8  | loss: 15.31969|  0:00:56s\n",
      "epoch 9  | loss: 14.52195|  0:01:02s\n",
      "epoch 10 | loss: 13.24866|  0:01:08s\n",
      "epoch 11 | loss: 12.79136|  0:01:14s\n",
      "epoch 12 | loss: 11.97047|  0:01:21s\n",
      "epoch 13 | loss: 11.85538|  0:01:27s\n",
      "epoch 14 | loss: 11.58993|  0:01:34s\n",
      "epoch 15 | loss: 11.44359|  0:01:40s\n",
      "epoch 16 | loss: 11.27348|  0:01:46s\n",
      "epoch 17 | loss: 10.40463|  0:01:52s\n",
      "epoch 18 | loss: 10.4906 |  0:01:58s\n",
      "epoch 19 | loss: 9.85584 |  0:02:05s\n",
      "epoch 20 | loss: 9.6832  |  0:02:11s\n",
      "epoch 21 | loss: 9.87336 |  0:02:17s\n",
      "epoch 22 | loss: 9.74355 |  0:02:24s\n",
      "epoch 23 | loss: 9.31272 |  0:02:30s\n",
      "epoch 24 | loss: 9.53025 |  0:02:36s\n",
      "epoch 25 | loss: 9.3342  |  0:02:42s\n",
      "epoch 26 | loss: 9.07174 |  0:02:49s\n",
      "epoch 27 | loss: 9.11439 |  0:02:55s\n",
      "epoch 28 | loss: 8.94859 |  0:03:01s\n",
      "epoch 29 | loss: 8.68169 |  0:03:08s\n",
      "epoch 30 | loss: 8.8189  |  0:03:14s\n",
      "epoch 31 | loss: 8.65264 |  0:03:20s\n",
      "epoch 32 | loss: 8.41746 |  0:03:27s\n",
      "epoch 33 | loss: 8.27567 |  0:03:33s\n",
      "epoch 34 | loss: 8.01435 |  0:03:39s\n",
      "epoch 35 | loss: 8.02851 |  0:03:45s\n",
      "epoch 36 | loss: 7.85872 |  0:03:51s\n",
      "epoch 37 | loss: 8.09762 |  0:03:58s\n",
      "epoch 38 | loss: 8.20599 |  0:04:04s\n",
      "epoch 39 | loss: 8.13655 |  0:04:10s\n",
      "epoch 40 | loss: 7.94887 |  0:04:17s\n",
      "epoch 41 | loss: 7.60232 |  0:04:23s\n",
      "epoch 42 | loss: 7.49993 |  0:04:29s\n",
      "epoch 43 | loss: 7.45449 |  0:04:36s\n",
      "epoch 44 | loss: 7.21137 |  0:04:42s\n",
      "epoch 45 | loss: 7.10467 |  0:04:48s\n",
      "epoch 46 | loss: 7.10363 |  0:04:54s\n",
      "epoch 47 | loss: 7.05478 |  0:05:01s\n",
      "epoch 48 | loss: 7.04037 |  0:05:07s\n",
      "epoch 49 | loss: 6.88318 |  0:05:13s\n",
      "epoch 50 | loss: 6.92145 |  0:05:19s\n",
      "epoch 51 | loss: 7.25494 |  0:05:25s\n",
      "epoch 52 | loss: 6.60894 |  0:05:32s\n",
      "epoch 53 | loss: 6.60898 |  0:05:38s\n",
      "epoch 54 | loss: 6.66996 |  0:05:44s\n",
      "epoch 55 | loss: 6.6272  |  0:05:50s\n",
      "epoch 56 | loss: 6.34316 |  0:05:56s\n",
      "epoch 57 | loss: 6.48221 |  0:06:03s\n",
      "epoch 58 | loss: 6.75379 |  0:06:09s\n",
      "epoch 59 | loss: 6.4029  |  0:06:15s\n",
      "epoch 60 | loss: 6.50492 |  0:06:22s\n",
      "epoch 61 | loss: 6.24423 |  0:06:28s\n",
      "epoch 62 | loss: 6.42529 |  0:06:34s\n",
      "epoch 63 | loss: 6.50214 |  0:06:40s\n",
      "epoch 64 | loss: 6.02192 |  0:06:47s\n",
      "epoch 65 | loss: 6.35841 |  0:06:53s\n",
      "epoch 66 | loss: 6.06574 |  0:06:59s\n",
      "epoch 67 | loss: 6.0807  |  0:07:06s\n",
      "epoch 68 | loss: 5.93438 |  0:07:12s\n",
      "epoch 69 | loss: 6.08055 |  0:07:18s\n",
      "epoch 70 | loss: 5.80824 |  0:07:24s\n",
      "epoch 71 | loss: 5.85622 |  0:07:31s\n",
      "epoch 72 | loss: 5.89651 |  0:07:37s\n",
      "epoch 73 | loss: 6.02268 |  0:07:43s\n",
      "epoch 74 | loss: 5.8266  |  0:07:50s\n",
      "epoch 75 | loss: 5.81677 |  0:07:56s\n",
      "epoch 76 | loss: 5.78285 |  0:08:02s\n",
      "epoch 77 | loss: 5.6394  |  0:08:09s\n",
      "epoch 78 | loss: 5.6938  |  0:08:15s\n",
      "epoch 79 | loss: 5.72906 |  0:08:21s\n",
      "epoch 80 | loss: 5.66498 |  0:08:27s\n",
      "epoch 81 | loss: 5.57141 |  0:08:34s\n",
      "epoch 82 | loss: 5.55103 |  0:08:40s\n",
      "epoch 83 | loss: 5.43275 |  0:08:47s\n",
      "epoch 84 | loss: 5.62324 |  0:08:53s\n",
      "epoch 85 | loss: 5.70547 |  0:08:59s\n",
      "epoch 86 | loss: 5.33478 |  0:09:05s\n",
      "epoch 87 | loss: 5.48918 |  0:09:12s\n",
      "epoch 88 | loss: 5.41585 |  0:09:18s\n",
      "epoch 89 | loss: 5.30128 |  0:09:24s\n",
      "epoch 90 | loss: 5.62069 |  0:09:31s\n",
      "epoch 91 | loss: 5.35267 |  0:09:37s\n",
      "epoch 92 | loss: 5.50571 |  0:09:43s\n",
      "epoch 93 | loss: 5.20068 |  0:09:50s\n",
      "epoch 94 | loss: 5.19308 |  0:09:56s\n",
      "epoch 95 | loss: 5.16296 |  0:10:02s\n",
      "epoch 96 | loss: 5.07301 |  0:10:08s\n",
      "epoch 97 | loss: 5.31094 |  0:10:15s\n",
      "epoch 98 | loss: 5.2767  |  0:10:21s\n",
      "epoch 99 | loss: 5.36974 |  0:10:27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 15:55:43,380]\u001b[0m Trial 8 finished with value: 2.608274487338463 and parameters: {'n_d': 12, 'n_a': 56, 'n_steps': 10, 'gamma': 1.6677232393531742, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.03850865498287028}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 93.08618|  0:00:03s\n",
      "epoch 1  | loss: 23.00979|  0:00:06s\n",
      "epoch 2  | loss: 21.01984|  0:00:09s\n",
      "epoch 3  | loss: 18.78667|  0:00:13s\n",
      "epoch 4  | loss: 15.98549|  0:00:16s\n",
      "epoch 5  | loss: 13.88076|  0:00:19s\n",
      "epoch 6  | loss: 12.69075|  0:00:22s\n",
      "epoch 7  | loss: 12.23648|  0:00:25s\n",
      "epoch 8  | loss: 12.38597|  0:00:29s\n",
      "epoch 9  | loss: 11.97337|  0:00:32s\n",
      "epoch 10 | loss: 12.08478|  0:00:35s\n",
      "epoch 11 | loss: 10.9834 |  0:00:38s\n",
      "epoch 12 | loss: 10.64307|  0:00:41s\n",
      "epoch 13 | loss: 10.65991|  0:00:45s\n",
      "epoch 14 | loss: 10.43133|  0:00:48s\n",
      "epoch 15 | loss: 10.25032|  0:00:51s\n",
      "epoch 16 | loss: 10.20517|  0:00:54s\n",
      "epoch 17 | loss: 10.14871|  0:00:57s\n",
      "epoch 18 | loss: 9.92414 |  0:01:00s\n",
      "epoch 19 | loss: 9.6002  |  0:01:03s\n",
      "epoch 20 | loss: 9.42466 |  0:01:07s\n",
      "epoch 21 | loss: 9.14402 |  0:01:10s\n",
      "epoch 22 | loss: 8.95286 |  0:01:13s\n",
      "epoch 23 | loss: 9.05882 |  0:01:17s\n",
      "epoch 24 | loss: 8.9075  |  0:01:20s\n",
      "epoch 25 | loss: 8.84208 |  0:01:23s\n",
      "epoch 26 | loss: 8.52827 |  0:01:26s\n",
      "epoch 27 | loss: 8.65564 |  0:01:29s\n",
      "epoch 28 | loss: 8.71047 |  0:01:33s\n",
      "epoch 29 | loss: 8.40959 |  0:01:36s\n",
      "epoch 30 | loss: 8.15661 |  0:01:39s\n",
      "epoch 31 | loss: 7.90704 |  0:01:42s\n",
      "epoch 32 | loss: 7.87525 |  0:01:46s\n",
      "epoch 33 | loss: 7.71547 |  0:01:49s\n",
      "epoch 34 | loss: 7.69914 |  0:01:52s\n",
      "epoch 35 | loss: 7.70364 |  0:01:55s\n",
      "epoch 36 | loss: 7.52779 |  0:01:58s\n",
      "epoch 37 | loss: 7.41985 |  0:02:01s\n",
      "epoch 38 | loss: 7.16597 |  0:02:05s\n",
      "epoch 39 | loss: 7.30341 |  0:02:08s\n",
      "epoch 40 | loss: 7.05147 |  0:02:11s\n",
      "epoch 41 | loss: 7.06317 |  0:02:14s\n",
      "epoch 42 | loss: 6.84474 |  0:02:17s\n",
      "epoch 43 | loss: 6.89681 |  0:02:21s\n",
      "epoch 44 | loss: 6.89305 |  0:02:24s\n",
      "epoch 45 | loss: 7.08182 |  0:02:27s\n",
      "epoch 46 | loss: 6.82293 |  0:02:30s\n",
      "epoch 47 | loss: 6.67956 |  0:02:33s\n",
      "epoch 48 | loss: 6.69373 |  0:02:37s\n",
      "epoch 49 | loss: 6.59365 |  0:02:40s\n",
      "epoch 50 | loss: 6.46687 |  0:02:43s\n",
      "epoch 51 | loss: 6.43159 |  0:02:46s\n",
      "epoch 52 | loss: 6.40641 |  0:02:50s\n",
      "epoch 53 | loss: 6.37714 |  0:02:53s\n",
      "epoch 54 | loss: 6.40538 |  0:02:56s\n",
      "epoch 55 | loss: 6.2048  |  0:02:59s\n",
      "epoch 56 | loss: 6.17552 |  0:03:02s\n",
      "epoch 57 | loss: 6.29204 |  0:03:06s\n",
      "epoch 58 | loss: 6.04755 |  0:03:09s\n",
      "epoch 59 | loss: 6.23501 |  0:03:12s\n",
      "epoch 60 | loss: 5.96865 |  0:03:15s\n",
      "epoch 61 | loss: 6.07234 |  0:03:18s\n",
      "epoch 62 | loss: 5.95511 |  0:03:22s\n",
      "epoch 63 | loss: 5.93205 |  0:03:25s\n",
      "epoch 64 | loss: 5.98613 |  0:03:28s\n",
      "epoch 65 | loss: 5.94024 |  0:03:32s\n",
      "epoch 66 | loss: 7.17202 |  0:03:35s\n",
      "epoch 67 | loss: 5.79723 |  0:03:38s\n",
      "epoch 68 | loss: 5.74248 |  0:03:41s\n",
      "epoch 69 | loss: 5.764   |  0:03:44s\n",
      "epoch 70 | loss: 5.68202 |  0:03:47s\n",
      "epoch 71 | loss: 5.50808 |  0:03:50s\n",
      "epoch 72 | loss: 5.61426 |  0:03:54s\n",
      "epoch 73 | loss: 5.78836 |  0:03:57s\n",
      "epoch 74 | loss: 5.5449  |  0:04:00s\n",
      "epoch 75 | loss: 5.87556 |  0:04:03s\n",
      "epoch 76 | loss: 5.78965 |  0:04:06s\n",
      "epoch 77 | loss: 5.45759 |  0:04:09s\n",
      "epoch 78 | loss: 5.592   |  0:04:13s\n",
      "epoch 79 | loss: 5.70139 |  0:04:16s\n",
      "epoch 80 | loss: 5.33854 |  0:04:19s\n",
      "epoch 81 | loss: 5.73245 |  0:04:22s\n",
      "epoch 82 | loss: 5.67044 |  0:04:26s\n",
      "epoch 83 | loss: 5.36863 |  0:04:29s\n",
      "epoch 84 | loss: 5.38179 |  0:04:32s\n",
      "epoch 85 | loss: 5.27717 |  0:04:35s\n",
      "epoch 86 | loss: 5.42362 |  0:04:38s\n",
      "epoch 87 | loss: 5.34033 |  0:04:41s\n",
      "epoch 88 | loss: 5.33178 |  0:04:45s\n",
      "epoch 89 | loss: 5.21806 |  0:04:48s\n",
      "epoch 90 | loss: 5.2043  |  0:04:51s\n",
      "epoch 91 | loss: 5.13312 |  0:04:54s\n",
      "epoch 92 | loss: 5.27753 |  0:04:58s\n",
      "epoch 93 | loss: 5.24692 |  0:05:01s\n",
      "epoch 94 | loss: 5.14224 |  0:05:04s\n",
      "epoch 95 | loss: 5.08093 |  0:05:07s\n",
      "epoch 96 | loss: 5.11605 |  0:05:10s\n",
      "epoch 97 | loss: 5.15647 |  0:05:13s\n",
      "epoch 98 | loss: 5.03791 |  0:05:16s\n",
      "epoch 99 | loss: 5.13515 |  0:05:20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:01:05,936]\u001b[0m Trial 9 finished with value: 4.269978995763713 and parameters: {'n_d': 60, 'n_a': 17, 'n_steps': 3, 'gamma': 1.1824426828823316, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.15638740143042137}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 169.11921|  0:00:03s\n",
      "epoch 1  | loss: 56.42601|  0:00:06s\n",
      "epoch 2  | loss: 44.1026 |  0:00:10s\n",
      "epoch 3  | loss: 31.22793|  0:00:13s\n",
      "epoch 4  | loss: 24.68651|  0:00:16s\n",
      "epoch 5  | loss: 20.51083|  0:00:19s\n",
      "epoch 6  | loss: 16.88065|  0:00:22s\n",
      "epoch 7  | loss: 16.9087 |  0:00:25s\n",
      "epoch 8  | loss: 15.59902|  0:00:28s\n",
      "epoch 9  | loss: 16.02069|  0:00:31s\n",
      "epoch 10 | loss: 14.26112|  0:00:35s\n",
      "epoch 11 | loss: 13.73758|  0:00:38s\n",
      "epoch 12 | loss: 13.53323|  0:00:41s\n",
      "epoch 13 | loss: 13.05809|  0:00:44s\n",
      "epoch 14 | loss: 12.68992|  0:00:47s\n",
      "epoch 15 | loss: 12.41199|  0:00:50s\n",
      "epoch 16 | loss: 12.57255|  0:00:54s\n",
      "epoch 17 | loss: 12.14908|  0:00:57s\n",
      "epoch 18 | loss: 11.69187|  0:01:00s\n",
      "epoch 19 | loss: 11.40555|  0:01:03s\n",
      "epoch 20 | loss: 11.16414|  0:01:07s\n",
      "epoch 21 | loss: 11.24445|  0:01:10s\n",
      "epoch 22 | loss: 11.0124 |  0:01:13s\n",
      "epoch 23 | loss: 10.68903|  0:01:16s\n",
      "epoch 24 | loss: 10.67839|  0:01:19s\n",
      "epoch 25 | loss: 10.4244 |  0:01:22s\n",
      "epoch 26 | loss: 10.18595|  0:01:26s\n",
      "epoch 27 | loss: 10.24043|  0:01:29s\n",
      "epoch 28 | loss: 9.91176 |  0:01:32s\n",
      "epoch 29 | loss: 9.91059 |  0:01:35s\n",
      "epoch 30 | loss: 9.38556 |  0:01:39s\n",
      "epoch 31 | loss: 9.518   |  0:01:42s\n",
      "epoch 32 | loss: 9.75994 |  0:01:45s\n",
      "epoch 33 | loss: 9.46949 |  0:01:48s\n",
      "epoch 34 | loss: 9.48967 |  0:01:51s\n",
      "epoch 35 | loss: 9.66404 |  0:01:54s\n",
      "epoch 36 | loss: 9.2518  |  0:01:57s\n",
      "epoch 37 | loss: 9.09339 |  0:02:01s\n",
      "epoch 38 | loss: 9.1462  |  0:02:04s\n",
      "epoch 39 | loss: 8.97816 |  0:02:07s\n",
      "epoch 40 | loss: 8.78469 |  0:02:11s\n",
      "epoch 41 | loss: 8.88069 |  0:02:14s\n",
      "epoch 42 | loss: 8.68366 |  0:02:17s\n",
      "epoch 43 | loss: 8.61893 |  0:02:20s\n",
      "epoch 44 | loss: 8.87893 |  0:02:23s\n",
      "epoch 45 | loss: 8.81939 |  0:02:26s\n",
      "epoch 46 | loss: 8.46162 |  0:02:29s\n",
      "epoch 47 | loss: 8.60493 |  0:02:32s\n",
      "epoch 48 | loss: 8.74474 |  0:02:35s\n",
      "epoch 49 | loss: 8.3781  |  0:02:39s\n",
      "epoch 50 | loss: 8.10026 |  0:02:42s\n",
      "epoch 51 | loss: 8.4647  |  0:02:45s\n",
      "epoch 52 | loss: 8.50408 |  0:02:48s\n",
      "epoch 53 | loss: 8.42937 |  0:02:52s\n",
      "epoch 54 | loss: 8.18907 |  0:02:55s\n",
      "epoch 55 | loss: 8.2473  |  0:02:58s\n",
      "epoch 56 | loss: 8.14686 |  0:03:01s\n",
      "epoch 57 | loss: 8.0937  |  0:03:04s\n",
      "epoch 58 | loss: 7.78128 |  0:03:07s\n",
      "epoch 59 | loss: 7.8541  |  0:03:11s\n",
      "epoch 60 | loss: 7.75624 |  0:03:14s\n",
      "epoch 61 | loss: 7.67631 |  0:03:17s\n",
      "epoch 62 | loss: 7.77481 |  0:03:20s\n",
      "epoch 63 | loss: 7.54311 |  0:03:24s\n",
      "epoch 64 | loss: 7.77381 |  0:03:27s\n",
      "epoch 65 | loss: 7.44933 |  0:03:30s\n",
      "epoch 66 | loss: 7.41249 |  0:03:33s\n",
      "epoch 67 | loss: 7.32166 |  0:03:36s\n",
      "epoch 68 | loss: 7.03606 |  0:03:39s\n",
      "epoch 69 | loss: 7.03131 |  0:03:43s\n",
      "epoch 70 | loss: 6.84101 |  0:03:46s\n",
      "epoch 71 | loss: 6.822   |  0:03:49s\n",
      "epoch 72 | loss: 6.8331  |  0:03:52s\n",
      "epoch 73 | loss: 6.90497 |  0:03:55s\n",
      "epoch 74 | loss: 6.67456 |  0:03:58s\n",
      "epoch 75 | loss: 6.62801 |  0:04:02s\n",
      "epoch 76 | loss: 6.69572 |  0:04:05s\n",
      "epoch 77 | loss: 6.6105  |  0:04:08s\n",
      "epoch 78 | loss: 6.53472 |  0:04:11s\n",
      "epoch 79 | loss: 6.42871 |  0:04:15s\n",
      "epoch 80 | loss: 6.39184 |  0:04:18s\n",
      "epoch 81 | loss: 6.23319 |  0:04:21s\n",
      "epoch 82 | loss: 6.19682 |  0:04:24s\n",
      "epoch 83 | loss: 6.40285 |  0:04:27s\n",
      "epoch 84 | loss: 6.41668 |  0:04:30s\n",
      "epoch 85 | loss: 6.35175 |  0:04:34s\n",
      "epoch 86 | loss: 6.09925 |  0:04:37s\n",
      "epoch 87 | loss: 6.17342 |  0:04:40s\n",
      "epoch 88 | loss: 6.17772 |  0:04:43s\n",
      "epoch 89 | loss: 6.05727 |  0:04:47s\n",
      "epoch 90 | loss: 5.97363 |  0:04:50s\n",
      "epoch 91 | loss: 5.87766 |  0:04:53s\n",
      "epoch 92 | loss: 6.02009 |  0:04:56s\n",
      "epoch 93 | loss: 6.2081  |  0:04:59s\n",
      "epoch 94 | loss: 5.88805 |  0:05:02s\n",
      "epoch 95 | loss: 6.08875 |  0:05:05s\n",
      "epoch 96 | loss: 6.05734 |  0:05:08s\n",
      "epoch 97 | loss: 5.96586 |  0:05:11s\n",
      "epoch 98 | loss: 5.79307 |  0:05:14s\n",
      "epoch 99 | loss: 5.85802 |  0:05:18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:06:27,601]\u001b[0m Trial 10 finished with value: 2.6347771832508045 and parameters: {'n_d': 42, 'n_a': 30, 'n_steps': 6, 'gamma': 1.9961814800271847, 'n_independent': 1, 'n_shared': 4, 'momentum': 0.38522159270892453}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 225.76264|  0:00:03s\n",
      "epoch 1  | loss: 53.61247|  0:00:07s\n",
      "epoch 2  | loss: 42.01029|  0:00:10s\n",
      "epoch 3  | loss: 38.54359|  0:00:14s\n",
      "epoch 4  | loss: 42.45166|  0:00:18s\n",
      "epoch 5  | loss: 40.47711|  0:00:21s\n",
      "epoch 6  | loss: 28.78037|  0:00:25s\n",
      "epoch 7  | loss: 20.41198|  0:00:29s\n",
      "epoch 8  | loss: 17.44719|  0:00:33s\n",
      "epoch 9  | loss: 16.28185|  0:00:36s\n",
      "epoch 10 | loss: 14.62295|  0:00:40s\n",
      "epoch 11 | loss: 14.12038|  0:00:43s\n",
      "epoch 12 | loss: 13.32674|  0:00:47s\n",
      "epoch 13 | loss: 12.89437|  0:00:51s\n",
      "epoch 14 | loss: 12.18862|  0:00:54s\n",
      "epoch 15 | loss: 12.06285|  0:00:58s\n",
      "epoch 16 | loss: 11.70042|  0:01:02s\n",
      "epoch 17 | loss: 10.87266|  0:01:06s\n",
      "epoch 18 | loss: 10.50841|  0:01:09s\n",
      "epoch 19 | loss: 10.23743|  0:01:13s\n",
      "epoch 20 | loss: 10.13761|  0:01:16s\n",
      "epoch 21 | loss: 9.79597 |  0:01:20s\n",
      "epoch 22 | loss: 9.67041 |  0:01:23s\n",
      "epoch 23 | loss: 9.38157 |  0:01:27s\n",
      "epoch 24 | loss: 9.2216  |  0:01:31s\n",
      "epoch 25 | loss: 9.08941 |  0:01:35s\n",
      "epoch 26 | loss: 8.9799  |  0:01:38s\n",
      "epoch 27 | loss: 8.61972 |  0:01:42s\n",
      "epoch 28 | loss: 8.45843 |  0:01:46s\n",
      "epoch 29 | loss: 8.43042 |  0:01:49s\n",
      "epoch 30 | loss: 8.27006 |  0:01:53s\n",
      "epoch 31 | loss: 8.11817 |  0:01:57s\n",
      "epoch 32 | loss: 7.94479 |  0:02:01s\n",
      "epoch 33 | loss: 7.86926 |  0:02:04s\n",
      "epoch 34 | loss: 7.88049 |  0:02:08s\n",
      "epoch 35 | loss: 7.82819 |  0:02:12s\n",
      "epoch 36 | loss: 7.63357 |  0:02:15s\n",
      "epoch 37 | loss: 7.5332  |  0:02:19s\n",
      "epoch 38 | loss: 7.46947 |  0:02:22s\n",
      "epoch 39 | loss: 7.40895 |  0:02:26s\n",
      "epoch 40 | loss: 7.47743 |  0:02:30s\n",
      "epoch 41 | loss: 7.55326 |  0:02:34s\n",
      "epoch 42 | loss: 7.10195 |  0:02:37s\n",
      "epoch 43 | loss: 7.20771 |  0:02:41s\n",
      "epoch 44 | loss: 7.06484 |  0:02:45s\n",
      "epoch 45 | loss: 7.27532 |  0:02:48s\n",
      "epoch 46 | loss: 7.03723 |  0:02:52s\n",
      "epoch 47 | loss: 7.00285 |  0:02:55s\n",
      "epoch 48 | loss: 6.8908  |  0:02:59s\n",
      "epoch 49 | loss: 6.97773 |  0:03:03s\n",
      "epoch 50 | loss: 6.84022 |  0:03:07s\n",
      "epoch 51 | loss: 6.63916 |  0:03:10s\n",
      "epoch 52 | loss: 6.72931 |  0:03:14s\n",
      "epoch 53 | loss: 6.88957 |  0:03:18s\n",
      "epoch 54 | loss: 6.6455  |  0:03:21s\n",
      "epoch 55 | loss: 6.6424  |  0:03:25s\n",
      "epoch 56 | loss: 6.50002 |  0:03:29s\n",
      "epoch 57 | loss: 6.68441 |  0:03:32s\n",
      "epoch 58 | loss: 6.53138 |  0:03:37s\n",
      "epoch 59 | loss: 6.46471 |  0:03:40s\n",
      "epoch 60 | loss: 6.40742 |  0:03:44s\n",
      "epoch 61 | loss: 6.43312 |  0:03:47s\n",
      "epoch 62 | loss: 6.34354 |  0:03:51s\n",
      "epoch 63 | loss: 6.16355 |  0:03:54s\n",
      "epoch 64 | loss: 6.31723 |  0:03:58s\n",
      "epoch 65 | loss: 6.4846  |  0:04:02s\n",
      "epoch 66 | loss: 6.30005 |  0:04:06s\n",
      "epoch 67 | loss: 6.16054 |  0:04:09s\n",
      "epoch 68 | loss: 6.21702 |  0:04:13s\n",
      "epoch 69 | loss: 6.14866 |  0:04:17s\n",
      "epoch 70 | loss: 6.11639 |  0:04:20s\n",
      "epoch 71 | loss: 6.07912 |  0:04:24s\n",
      "epoch 72 | loss: 5.90738 |  0:04:27s\n",
      "epoch 73 | loss: 6.00144 |  0:04:31s\n",
      "epoch 74 | loss: 5.82767 |  0:04:35s\n",
      "epoch 75 | loss: 5.83867 |  0:04:39s\n",
      "epoch 76 | loss: 5.87167 |  0:04:43s\n",
      "epoch 77 | loss: 5.94636 |  0:04:46s\n",
      "epoch 78 | loss: 5.81974 |  0:04:50s\n",
      "epoch 79 | loss: 5.8104  |  0:04:53s\n",
      "epoch 80 | loss: 5.67265 |  0:04:57s\n",
      "epoch 81 | loss: 5.69017 |  0:05:00s\n",
      "epoch 82 | loss: 5.61055 |  0:05:04s\n",
      "epoch 83 | loss: 5.62721 |  0:05:08s\n",
      "epoch 84 | loss: 5.57869 |  0:05:12s\n",
      "epoch 85 | loss: 5.775   |  0:05:16s\n",
      "epoch 86 | loss: 5.80592 |  0:05:19s\n",
      "epoch 87 | loss: 5.74205 |  0:05:23s\n",
      "epoch 88 | loss: 5.53009 |  0:05:26s\n",
      "epoch 89 | loss: 5.4714  |  0:05:30s\n",
      "epoch 90 | loss: 5.42802 |  0:05:33s\n",
      "epoch 91 | loss: 5.5495  |  0:05:37s\n",
      "epoch 92 | loss: 5.42964 |  0:05:41s\n",
      "epoch 93 | loss: 5.45306 |  0:05:45s\n",
      "epoch 94 | loss: 5.37351 |  0:05:49s\n",
      "epoch 95 | loss: 5.33996 |  0:05:52s\n",
      "epoch 96 | loss: 5.41196 |  0:05:56s\n",
      "epoch 97 | loss: 5.25695 |  0:06:00s\n",
      "epoch 98 | loss: 5.70583 |  0:06:03s\n",
      "epoch 99 | loss: 5.43613 |  0:06:07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:12:38,255]\u001b[0m Trial 11 finished with value: 2.6530211675944075 and parameters: {'n_d': 8, 'n_a': 51, 'n_steps': 6, 'gamma': 1.5365613985396152, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.38917916302588396}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 110.45234|  0:00:02s\n",
      "epoch 1  | loss: 21.07024|  0:00:04s\n",
      "epoch 2  | loss: 16.52903|  0:00:06s\n",
      "epoch 3  | loss: 15.43348|  0:00:08s\n",
      "epoch 4  | loss: 13.94306|  0:00:11s\n",
      "epoch 5  | loss: 12.71463|  0:00:13s\n",
      "epoch 6  | loss: 11.85821|  0:00:15s\n",
      "epoch 7  | loss: 11.4622 |  0:00:17s\n",
      "epoch 8  | loss: 10.85801|  0:00:20s\n",
      "epoch 9  | loss: 10.68265|  0:00:22s\n",
      "epoch 10 | loss: 10.47966|  0:00:24s\n",
      "epoch 11 | loss: 10.42688|  0:00:26s\n",
      "epoch 12 | loss: 9.67548 |  0:00:29s\n",
      "epoch 13 | loss: 9.78983 |  0:00:31s\n",
      "epoch 14 | loss: 9.36977 |  0:00:34s\n",
      "epoch 15 | loss: 9.23942 |  0:00:36s\n",
      "epoch 16 | loss: 9.07832 |  0:00:38s\n",
      "epoch 17 | loss: 9.02626 |  0:00:40s\n",
      "epoch 18 | loss: 8.92391 |  0:00:43s\n",
      "epoch 19 | loss: 8.7648  |  0:00:45s\n",
      "epoch 20 | loss: 8.62545 |  0:00:47s\n",
      "epoch 21 | loss: 8.48083 |  0:00:49s\n",
      "epoch 22 | loss: 8.2859  |  0:00:51s\n",
      "epoch 23 | loss: 8.21606 |  0:00:54s\n",
      "epoch 24 | loss: 8.10682 |  0:00:56s\n",
      "epoch 25 | loss: 7.96993 |  0:00:58s\n",
      "epoch 26 | loss: 7.93372 |  0:01:00s\n",
      "epoch 27 | loss: 7.70885 |  0:01:02s\n",
      "epoch 28 | loss: 7.73414 |  0:01:05s\n",
      "epoch 29 | loss: 7.56537 |  0:01:07s\n",
      "epoch 30 | loss: 7.5091  |  0:01:09s\n",
      "epoch 31 | loss: 7.25277 |  0:01:11s\n",
      "epoch 32 | loss: 7.44118 |  0:01:13s\n",
      "epoch 33 | loss: 7.12708 |  0:01:16s\n",
      "epoch 34 | loss: 7.23807 |  0:01:18s\n",
      "epoch 35 | loss: 7.20453 |  0:01:20s\n",
      "epoch 36 | loss: 7.21619 |  0:01:22s\n",
      "epoch 37 | loss: 7.04776 |  0:01:24s\n",
      "epoch 38 | loss: 6.99696 |  0:01:27s\n",
      "epoch 39 | loss: 6.97949 |  0:01:29s\n",
      "epoch 40 | loss: 6.79643 |  0:01:31s\n",
      "epoch 41 | loss: 6.85832 |  0:01:33s\n",
      "epoch 42 | loss: 6.69766 |  0:01:35s\n",
      "epoch 43 | loss: 6.83637 |  0:01:38s\n",
      "epoch 44 | loss: 6.68541 |  0:01:40s\n",
      "epoch 45 | loss: 6.59576 |  0:01:42s\n",
      "epoch 46 | loss: 6.70067 |  0:01:44s\n",
      "epoch 47 | loss: 6.69029 |  0:01:46s\n",
      "epoch 48 | loss: 6.49069 |  0:01:49s\n",
      "epoch 49 | loss: 6.39429 |  0:01:51s\n",
      "epoch 50 | loss: 6.43831 |  0:01:53s\n",
      "epoch 51 | loss: 6.46219 |  0:01:55s\n",
      "epoch 52 | loss: 6.38035 |  0:01:57s\n",
      "epoch 53 | loss: 6.22125 |  0:02:00s\n",
      "epoch 54 | loss: 6.20614 |  0:02:02s\n",
      "epoch 55 | loss: 6.07781 |  0:02:04s\n",
      "epoch 56 | loss: 6.20327 |  0:02:07s\n",
      "epoch 57 | loss: 6.02759 |  0:02:09s\n",
      "epoch 58 | loss: 6.00547 |  0:02:11s\n",
      "epoch 59 | loss: 6.09752 |  0:02:13s\n",
      "epoch 60 | loss: 6.06374 |  0:02:15s\n",
      "epoch 61 | loss: 5.95584 |  0:02:17s\n",
      "epoch 62 | loss: 6.14834 |  0:02:20s\n",
      "epoch 63 | loss: 5.82531 |  0:02:22s\n",
      "epoch 64 | loss: 5.90793 |  0:02:24s\n",
      "epoch 65 | loss: 6.04729 |  0:02:26s\n",
      "epoch 66 | loss: 5.92015 |  0:02:28s\n",
      "epoch 67 | loss: 5.89843 |  0:02:31s\n",
      "epoch 68 | loss: 5.68113 |  0:02:33s\n",
      "epoch 69 | loss: 5.89328 |  0:02:35s\n",
      "epoch 70 | loss: 5.72232 |  0:02:38s\n",
      "epoch 71 | loss: 5.78482 |  0:02:40s\n",
      "epoch 72 | loss: 5.80156 |  0:02:42s\n",
      "epoch 73 | loss: 5.59063 |  0:02:44s\n",
      "epoch 74 | loss: 5.60183 |  0:02:46s\n",
      "epoch 75 | loss: 5.59704 |  0:02:48s\n",
      "epoch 76 | loss: 5.52262 |  0:02:51s\n",
      "epoch 77 | loss: 5.63509 |  0:02:53s\n",
      "epoch 78 | loss: 5.49027 |  0:02:55s\n",
      "epoch 79 | loss: 5.45977 |  0:02:57s\n",
      "epoch 80 | loss: 5.5264  |  0:02:59s\n",
      "epoch 81 | loss: 5.52686 |  0:03:01s\n",
      "epoch 82 | loss: 5.61062 |  0:03:04s\n",
      "epoch 83 | loss: 5.54343 |  0:03:06s\n",
      "epoch 84 | loss: 5.43085 |  0:03:09s\n",
      "epoch 85 | loss: 5.4351  |  0:03:11s\n",
      "epoch 86 | loss: 5.39749 |  0:03:13s\n",
      "epoch 87 | loss: 5.44733 |  0:03:15s\n",
      "epoch 88 | loss: 5.35068 |  0:03:17s\n",
      "epoch 89 | loss: 5.36215 |  0:03:20s\n",
      "epoch 90 | loss: 5.28415 |  0:03:22s\n",
      "epoch 91 | loss: 5.28792 |  0:03:24s\n",
      "epoch 92 | loss: 5.36247 |  0:03:26s\n",
      "epoch 93 | loss: 5.38155 |  0:03:29s\n",
      "epoch 94 | loss: 5.31384 |  0:03:31s\n",
      "epoch 95 | loss: 5.27127 |  0:03:33s\n",
      "epoch 96 | loss: 5.35997 |  0:03:35s\n",
      "epoch 97 | loss: 5.25695 |  0:03:38s\n",
      "epoch 98 | loss: 5.28744 |  0:03:40s\n",
      "epoch 99 | loss: 5.43499 |  0:03:43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:16:23,141]\u001b[0m Trial 12 finished with value: 2.5659268564450155 and parameters: {'n_d': 19, 'n_a': 30, 'n_steps': 3, 'gamma': 1.0162974147472938, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.13970709081059146}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 176.87334|  0:00:05s\n",
      "epoch 1  | loss: 32.95511|  0:00:10s\n",
      "epoch 2  | loss: 25.96187|  0:00:15s\n",
      "epoch 3  | loss: 26.16905|  0:00:20s\n",
      "epoch 4  | loss: 23.11367|  0:00:25s\n",
      "epoch 5  | loss: 20.59438|  0:00:31s\n",
      "epoch 6  | loss: 18.5167 |  0:00:36s\n",
      "epoch 7  | loss: 17.01885|  0:00:41s\n",
      "epoch 8  | loss: 16.52   |  0:00:46s\n",
      "epoch 9  | loss: 14.70164|  0:00:52s\n",
      "epoch 10 | loss: 15.00615|  0:00:57s\n",
      "epoch 11 | loss: 13.82792|  0:01:03s\n",
      "epoch 12 | loss: 13.51174|  0:01:08s\n",
      "epoch 13 | loss: 13.28883|  0:01:13s\n",
      "epoch 14 | loss: 13.39132|  0:01:18s\n",
      "epoch 15 | loss: 12.67255|  0:01:24s\n",
      "epoch 16 | loss: 12.9323 |  0:01:29s\n",
      "epoch 17 | loss: 15.02576|  0:01:34s\n",
      "epoch 18 | loss: 15.05287|  0:01:39s\n",
      "epoch 19 | loss: 12.49324|  0:01:45s\n",
      "epoch 20 | loss: 11.99052|  0:01:49s\n",
      "epoch 21 | loss: 11.10713|  0:01:55s\n",
      "epoch 22 | loss: 10.84952|  0:02:00s\n",
      "epoch 23 | loss: 10.69974|  0:02:05s\n",
      "epoch 24 | loss: 10.29217|  0:02:10s\n",
      "epoch 25 | loss: 10.14866|  0:02:15s\n",
      "epoch 26 | loss: 9.80522 |  0:02:20s\n",
      "epoch 27 | loss: 9.9395  |  0:02:25s\n",
      "epoch 28 | loss: 9.40519 |  0:02:30s\n",
      "epoch 29 | loss: 9.66744 |  0:02:36s\n",
      "epoch 30 | loss: 9.22943 |  0:02:41s\n",
      "epoch 31 | loss: 9.07282 |  0:02:46s\n",
      "epoch 32 | loss: 8.92312 |  0:02:51s\n",
      "epoch 33 | loss: 8.80853 |  0:02:56s\n",
      "epoch 34 | loss: 8.52297 |  0:03:01s\n",
      "epoch 35 | loss: 8.35949 |  0:03:07s\n",
      "epoch 36 | loss: 8.20688 |  0:03:12s\n",
      "epoch 37 | loss: 8.42169 |  0:03:17s\n",
      "epoch 38 | loss: 8.27534 |  0:03:22s\n",
      "epoch 39 | loss: 8.12908 |  0:03:27s\n",
      "epoch 40 | loss: 7.96432 |  0:03:32s\n",
      "epoch 41 | loss: 8.30312 |  0:03:38s\n",
      "epoch 42 | loss: 7.98378 |  0:03:43s\n",
      "epoch 43 | loss: 7.83148 |  0:03:48s\n",
      "epoch 44 | loss: 7.84579 |  0:03:53s\n",
      "epoch 45 | loss: 7.63652 |  0:03:58s\n",
      "epoch 46 | loss: 7.56239 |  0:04:03s\n",
      "epoch 47 | loss: 7.59543 |  0:04:09s\n",
      "epoch 48 | loss: 7.67035 |  0:04:14s\n",
      "epoch 49 | loss: 7.43877 |  0:04:19s\n",
      "epoch 50 | loss: 7.35884 |  0:04:24s\n",
      "epoch 51 | loss: 7.44654 |  0:04:29s\n",
      "epoch 52 | loss: 7.30581 |  0:04:34s\n",
      "epoch 53 | loss: 7.16282 |  0:04:40s\n",
      "epoch 54 | loss: 7.06475 |  0:04:44s\n",
      "epoch 55 | loss: 7.01577 |  0:04:49s\n",
      "epoch 56 | loss: 7.52471 |  0:04:55s\n",
      "epoch 57 | loss: 6.84718 |  0:05:00s\n",
      "epoch 58 | loss: 6.64119 |  0:05:05s\n",
      "epoch 59 | loss: 6.74837 |  0:05:10s\n",
      "epoch 60 | loss: 6.8444  |  0:05:15s\n",
      "epoch 61 | loss: 6.78343 |  0:05:20s\n",
      "epoch 62 | loss: 6.65401 |  0:05:26s\n",
      "epoch 63 | loss: 6.58029 |  0:05:30s\n",
      "epoch 64 | loss: 6.44861 |  0:05:36s\n",
      "epoch 65 | loss: 6.22536 |  0:05:41s\n",
      "epoch 66 | loss: 6.45101 |  0:05:46s\n",
      "epoch 67 | loss: 6.7244  |  0:05:51s\n",
      "epoch 68 | loss: 6.41569 |  0:05:56s\n",
      "epoch 69 | loss: 6.42951 |  0:06:01s\n",
      "epoch 70 | loss: 6.3562  |  0:06:06s\n",
      "epoch 71 | loss: 6.41964 |  0:06:12s\n",
      "epoch 72 | loss: 6.39489 |  0:06:17s\n",
      "epoch 73 | loss: 6.27483 |  0:06:22s\n",
      "epoch 74 | loss: 6.25392 |  0:06:27s\n",
      "epoch 75 | loss: 6.20407 |  0:06:32s\n",
      "epoch 76 | loss: 6.33767 |  0:06:37s\n",
      "epoch 77 | loss: 6.10628 |  0:06:42s\n",
      "epoch 78 | loss: 5.94821 |  0:06:48s\n",
      "epoch 79 | loss: 5.80088 |  0:06:53s\n",
      "epoch 80 | loss: 6.0312  |  0:06:58s\n",
      "epoch 81 | loss: 6.09892 |  0:07:03s\n",
      "epoch 82 | loss: 6.08132 |  0:07:08s\n",
      "epoch 83 | loss: 5.84621 |  0:07:13s\n",
      "epoch 84 | loss: 5.90621 |  0:07:18s\n",
      "epoch 85 | loss: 5.85487 |  0:07:24s\n",
      "epoch 86 | loss: 5.81641 |  0:07:29s\n",
      "epoch 87 | loss: 5.7976  |  0:07:34s\n",
      "epoch 88 | loss: 5.76442 |  0:07:39s\n",
      "epoch 89 | loss: 5.66389 |  0:07:44s\n",
      "epoch 90 | loss: 5.54821 |  0:07:50s\n",
      "epoch 91 | loss: 5.71534 |  0:07:55s\n",
      "epoch 92 | loss: 5.83602 |  0:08:00s\n",
      "epoch 93 | loss: 5.69213 |  0:08:05s\n",
      "epoch 94 | loss: 5.61775 |  0:08:10s\n",
      "epoch 95 | loss: 6.01764 |  0:08:15s\n",
      "epoch 96 | loss: 5.71896 |  0:08:21s\n",
      "epoch 97 | loss: 5.77137 |  0:08:26s\n",
      "epoch 98 | loss: 5.49556 |  0:08:31s\n",
      "epoch 99 | loss: 5.5705  |  0:08:36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:25:03,309]\u001b[0m Trial 13 finished with value: 2.6199099088269837 and parameters: {'n_d': 37, 'n_a': 31, 'n_steps': 7, 'gamma': 1.0346633031487653, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.14183602698316475}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 139.91884|  0:00:03s\n",
      "epoch 1  | loss: 32.66965|  0:00:07s\n",
      "epoch 2  | loss: 29.22672|  0:00:10s\n",
      "epoch 3  | loss: 25.43507|  0:00:14s\n",
      "epoch 4  | loss: 27.61015|  0:00:17s\n",
      "epoch 5  | loss: 24.8668 |  0:00:21s\n",
      "epoch 6  | loss: 22.21416|  0:00:24s\n",
      "epoch 7  | loss: 21.67455|  0:00:28s\n",
      "epoch 8  | loss: 18.28135|  0:00:31s\n",
      "epoch 9  | loss: 18.20667|  0:00:35s\n",
      "epoch 10 | loss: 15.99277|  0:00:38s\n",
      "epoch 11 | loss: 16.35783|  0:00:42s\n",
      "epoch 12 | loss: 16.27309|  0:00:45s\n",
      "epoch 13 | loss: 16.13751|  0:00:49s\n",
      "epoch 14 | loss: 14.93423|  0:00:52s\n",
      "epoch 15 | loss: 15.06863|  0:00:55s\n",
      "epoch 16 | loss: 14.97722|  0:00:59s\n",
      "epoch 17 | loss: 14.65275|  0:01:02s\n",
      "epoch 18 | loss: 14.09222|  0:01:06s\n",
      "epoch 19 | loss: 14.27012|  0:01:09s\n",
      "epoch 20 | loss: 15.18316|  0:01:13s\n",
      "epoch 21 | loss: 14.25885|  0:01:17s\n",
      "epoch 22 | loss: 14.53819|  0:01:20s\n",
      "epoch 23 | loss: 15.36982|  0:01:23s\n",
      "epoch 24 | loss: 14.6872 |  0:01:27s\n",
      "epoch 25 | loss: 13.92714|  0:01:30s\n",
      "epoch 26 | loss: 13.66105|  0:01:34s\n",
      "epoch 27 | loss: 14.6689 |  0:01:37s\n",
      "epoch 28 | loss: 13.24401|  0:01:41s\n",
      "epoch 29 | loss: 13.27697|  0:01:44s\n",
      "epoch 30 | loss: 13.08764|  0:01:48s\n",
      "epoch 31 | loss: 12.61186|  0:01:51s\n",
      "epoch 32 | loss: 12.8592 |  0:01:55s\n",
      "epoch 33 | loss: 12.65522|  0:01:58s\n",
      "epoch 34 | loss: 12.47904|  0:02:01s\n",
      "epoch 35 | loss: 12.09292|  0:02:05s\n",
      "epoch 36 | loss: 14.89288|  0:02:08s\n",
      "epoch 37 | loss: 12.07851|  0:02:12s\n",
      "epoch 38 | loss: 11.71001|  0:02:16s\n",
      "epoch 39 | loss: 11.43566|  0:02:19s\n",
      "epoch 40 | loss: 11.35379|  0:02:23s\n",
      "epoch 41 | loss: 10.89506|  0:02:26s\n",
      "epoch 42 | loss: 10.69171|  0:02:29s\n",
      "epoch 43 | loss: 10.6502 |  0:02:33s\n",
      "epoch 44 | loss: 10.81215|  0:02:36s\n",
      "epoch 45 | loss: 10.55192|  0:02:40s\n",
      "epoch 46 | loss: 10.11357|  0:02:43s\n",
      "epoch 47 | loss: 10.07397|  0:02:47s\n",
      "epoch 48 | loss: 10.50609|  0:02:50s\n",
      "epoch 49 | loss: 9.96701 |  0:02:54s\n",
      "epoch 50 | loss: 9.57528 |  0:02:57s\n",
      "epoch 51 | loss: 9.32752 |  0:03:01s\n",
      "epoch 52 | loss: 9.47154 |  0:03:04s\n",
      "epoch 53 | loss: 9.38085 |  0:03:08s\n",
      "epoch 54 | loss: 9.52182 |  0:03:11s\n",
      "epoch 55 | loss: 9.59689 |  0:03:14s\n",
      "epoch 56 | loss: 9.26769 |  0:03:18s\n",
      "epoch 57 | loss: 9.20001 |  0:03:22s\n",
      "epoch 58 | loss: 8.9727  |  0:03:25s\n",
      "epoch 59 | loss: 8.71778 |  0:03:29s\n",
      "epoch 60 | loss: 8.71272 |  0:03:32s\n",
      "epoch 61 | loss: 8.74405 |  0:03:36s\n",
      "epoch 62 | loss: 8.51662 |  0:03:39s\n",
      "epoch 63 | loss: 8.47013 |  0:03:42s\n",
      "epoch 64 | loss: 8.5049  |  0:03:46s\n",
      "epoch 65 | loss: 8.36258 |  0:03:50s\n",
      "epoch 66 | loss: 8.37987 |  0:03:53s\n",
      "epoch 67 | loss: 8.08863 |  0:03:56s\n",
      "epoch 68 | loss: 8.08306 |  0:04:00s\n",
      "epoch 69 | loss: 8.69725 |  0:04:04s\n",
      "epoch 70 | loss: 8.82023 |  0:04:07s\n",
      "epoch 71 | loss: 8.33285 |  0:04:10s\n",
      "epoch 72 | loss: 8.16431 |  0:04:14s\n",
      "epoch 73 | loss: 8.17185 |  0:04:17s\n",
      "epoch 74 | loss: 8.05099 |  0:04:21s\n",
      "epoch 75 | loss: 7.89219 |  0:04:24s\n",
      "epoch 76 | loss: 7.90783 |  0:04:28s\n",
      "epoch 77 | loss: 7.86975 |  0:04:31s\n",
      "epoch 78 | loss: 7.733   |  0:04:35s\n",
      "epoch 79 | loss: 7.92755 |  0:04:38s\n",
      "epoch 80 | loss: 7.70547 |  0:04:42s\n",
      "epoch 81 | loss: 7.70442 |  0:04:45s\n",
      "epoch 82 | loss: 7.67988 |  0:04:49s\n",
      "epoch 83 | loss: 7.40138 |  0:04:52s\n",
      "epoch 84 | loss: 7.36676 |  0:04:56s\n",
      "epoch 85 | loss: 7.24582 |  0:04:59s\n",
      "epoch 86 | loss: 7.39415 |  0:05:03s\n",
      "epoch 87 | loss: 7.35089 |  0:05:06s\n",
      "epoch 88 | loss: 7.46561 |  0:05:10s\n",
      "epoch 89 | loss: 7.37932 |  0:05:13s\n",
      "epoch 90 | loss: 7.27156 |  0:05:17s\n",
      "epoch 91 | loss: 7.44065 |  0:05:20s\n",
      "epoch 92 | loss: 7.57587 |  0:05:24s\n",
      "epoch 93 | loss: 7.3276  |  0:05:27s\n",
      "epoch 94 | loss: 7.30066 |  0:05:31s\n",
      "epoch 95 | loss: 7.7742  |  0:05:34s\n",
      "epoch 96 | loss: 7.78744 |  0:05:38s\n",
      "epoch 97 | loss: 7.55748 |  0:05:41s\n",
      "epoch 98 | loss: 7.37976 |  0:05:44s\n",
      "epoch 99 | loss: 7.36565 |  0:05:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:30:55,160]\u001b[0m Trial 14 finished with value: 3.018925174801892 and parameters: {'n_d': 20, 'n_a': 34, 'n_steps': 8, 'gamma': 1.0054603431201632, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.3127840105261076}. Best is trial 4 with value: 2.4813181501130956.\u001b[0m\n",
      "\u001b[33m[W 2023-02-17 16:30:55,235]\u001b[0m You need to set up the pruning feature to utilize `plot_intermediate_values()`\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting into else block\n",
      "getting into if block\n",
      "getting into if block\n",
      "getting into if block\n",
      "{'n_d': 46, 'n_a': 59, 'n_steps': 10, 'gamma': 1.6768572281509007, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.31161473810203194}\n",
      "0 : [2.7376944614398324]\n",
      "{'n_d': 29, 'n_a': 46, 'n_steps': 8, 'gamma': 1.194379512158311, 'n_independent': 2, 'n_shared': 5, 'momentum': 0.2713129813806114}\n",
      "1 : [2.8661875269913644]\n",
      "{'n_d': 55, 'n_a': 12, 'n_steps': 9, 'gamma': 1.1555724854026241, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.22773175035640097}\n",
      "2 : [2.693232858724422]\n",
      "{'n_d': 23, 'n_a': 18, 'n_steps': 7, 'gamma': 1.4272670215385386, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.07796151816267341}\n",
      "3 : [7.4211230836075215]\n",
      "{'n_d': 34, 'n_a': 59, 'n_steps': 7, 'gamma': 1.4401134890241982, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3256366679620535}\n",
      "4 : [2.4813181501130956]\n",
      "{'n_d': 16, 'n_a': 62, 'n_steps': 4, 'gamma': 1.4058017182400961, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.18685323700714593}\n",
      "5 : [2.604970366636673]\n",
      "{'n_d': 51, 'n_a': 42, 'n_steps': 4, 'gamma': 1.2890335708792262, 'n_independent': 3, 'n_shared': 1, 'momentum': 0.2031508725076623}\n",
      "6 : [2.6291827114250705]\n",
      "{'n_d': 28, 'n_a': 59, 'n_steps': 5, 'gamma': 1.2986491244003742, 'n_independent': 5, 'n_shared': 1, 'momentum': 0.021597847324575627}\n",
      "7 : [2.613729115405267]\n",
      "{'n_d': 12, 'n_a': 56, 'n_steps': 10, 'gamma': 1.6677232393531742, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.03850865498287028}\n",
      "8 : [2.608274487338463]\n",
      "{'n_d': 60, 'n_a': 17, 'n_steps': 3, 'gamma': 1.1824426828823316, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.15638740143042137}\n",
      "9 : [4.269978995763713]\n",
      "{'n_d': 42, 'n_a': 30, 'n_steps': 6, 'gamma': 1.9961814800271847, 'n_independent': 1, 'n_shared': 4, 'momentum': 0.38522159270892453}\n",
      "10 : [2.6347771832508045]\n",
      "{'n_d': 8, 'n_a': 51, 'n_steps': 6, 'gamma': 1.5365613985396152, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.38917916302588396}\n",
      "11 : [2.6530211675944075]\n",
      "{'n_d': 19, 'n_a': 30, 'n_steps': 3, 'gamma': 1.0162974147472938, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.13970709081059146}\n",
      "12 : [2.5659268564450155]\n",
      "{'n_d': 37, 'n_a': 31, 'n_steps': 7, 'gamma': 1.0346633031487653, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.14183602698316475}\n",
      "13 : [2.6199099088269837]\n",
      "{'n_d': 20, 'n_a': 34, 'n_steps': 8, 'gamma': 1.0054603431201632, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.3127840105261076}\n",
      "14 : [3.018925174801892]\n",
      " Best params for fold : [18/20]\n",
      "{'n_d': 34, 'n_a': 59, 'n_steps': 7, 'gamma': 1.4401134890241982, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3256366679620535}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 169.46079|  0:00:05s\n",
      "epoch 1  | loss: 50.39759|  0:00:11s\n",
      "epoch 2  | loss: 42.77008|  0:00:17s\n",
      "epoch 3  | loss: 44.79586|  0:00:23s\n",
      "epoch 4  | loss: 35.93852|  0:00:29s\n",
      "epoch 5  | loss: 31.94879|  0:00:35s\n",
      "epoch 6  | loss: 23.02367|  0:00:40s\n",
      "epoch 7  | loss: 17.23494|  0:00:46s\n",
      "epoch 8  | loss: 15.34003|  0:00:52s\n",
      "epoch 9  | loss: 14.3053 |  0:00:58s\n",
      "epoch 10 | loss: 13.49971|  0:01:04s\n",
      "epoch 11 | loss: 13.18831|  0:01:10s\n",
      "epoch 12 | loss: 12.68855|  0:01:16s\n",
      "epoch 13 | loss: 12.0029 |  0:01:21s\n",
      "epoch 14 | loss: 11.93382|  0:01:27s\n",
      "epoch 15 | loss: 11.61834|  0:01:33s\n",
      "epoch 16 | loss: 11.19638|  0:01:39s\n",
      "epoch 17 | loss: 10.74082|  0:01:45s\n",
      "epoch 18 | loss: 10.58771|  0:01:51s\n",
      "epoch 19 | loss: 10.55961|  0:01:56s\n",
      "epoch 20 | loss: 9.99132 |  0:02:03s\n",
      "epoch 21 | loss: 10.11291|  0:02:08s\n",
      "epoch 22 | loss: 9.52019 |  0:02:14s\n",
      "epoch 23 | loss: 9.74017 |  0:02:20s\n",
      "epoch 24 | loss: 9.32249 |  0:02:26s\n",
      "epoch 25 | loss: 9.42131 |  0:02:31s\n",
      "epoch 26 | loss: 9.01513 |  0:02:38s\n",
      "epoch 27 | loss: 8.85753 |  0:02:43s\n",
      "epoch 28 | loss: 8.65184 |  0:02:49s\n",
      "epoch 29 | loss: 9.00128 |  0:02:55s\n",
      "epoch 30 | loss: 8.83468 |  0:03:00s\n",
      "epoch 31 | loss: 8.41751 |  0:03:07s\n",
      "epoch 32 | loss: 8.43407 |  0:03:12s\n",
      "epoch 33 | loss: 8.13818 |  0:03:18s\n",
      "epoch 34 | loss: 8.24834 |  0:03:24s\n",
      "epoch 35 | loss: 8.09218 |  0:03:30s\n",
      "epoch 36 | loss: 7.88474 |  0:03:35s\n",
      "epoch 37 | loss: 7.96133 |  0:03:41s\n",
      "epoch 38 | loss: 7.77964 |  0:03:47s\n",
      "epoch 39 | loss: 7.48032 |  0:03:53s\n",
      "epoch 40 | loss: 7.6408  |  0:03:59s\n",
      "epoch 41 | loss: 7.39093 |  0:04:05s\n",
      "epoch 42 | loss: 7.20291 |  0:04:11s\n",
      "epoch 43 | loss: 7.20206 |  0:04:16s\n",
      "epoch 44 | loss: 7.13291 |  0:04:22s\n",
      "epoch 45 | loss: 7.10148 |  0:04:28s\n",
      "epoch 46 | loss: 7.14966 |  0:04:34s\n",
      "epoch 47 | loss: 7.04736 |  0:04:40s\n",
      "epoch 48 | loss: 7.18774 |  0:04:46s\n",
      "epoch 49 | loss: 6.79889 |  0:04:51s\n",
      "epoch 50 | loss: 6.86649 |  0:04:57s\n",
      "epoch 51 | loss: 6.85977 |  0:05:03s\n",
      "epoch 52 | loss: 6.83507 |  0:05:08s\n",
      "epoch 53 | loss: 6.7159  |  0:05:14s\n",
      "epoch 54 | loss: 6.70157 |  0:05:20s\n",
      "epoch 55 | loss: 6.90431 |  0:05:26s\n",
      "epoch 56 | loss: 6.65714 |  0:05:32s\n",
      "epoch 57 | loss: 6.57399 |  0:05:37s\n",
      "epoch 58 | loss: 6.58191 |  0:05:44s\n",
      "epoch 59 | loss: 6.65331 |  0:05:50s\n",
      "epoch 60 | loss: 6.46673 |  0:05:55s\n",
      "epoch 61 | loss: 6.06244 |  0:06:01s\n",
      "epoch 62 | loss: 6.32495 |  0:06:07s\n",
      "epoch 63 | loss: 6.0581  |  0:06:12s\n",
      "epoch 64 | loss: 6.26608 |  0:06:18s\n",
      "epoch 65 | loss: 6.30569 |  0:06:24s\n",
      "epoch 66 | loss: 6.55382 |  0:06:30s\n",
      "epoch 67 | loss: 6.36556 |  0:06:36s\n",
      "epoch 68 | loss: 6.34531 |  0:06:41s\n",
      "epoch 69 | loss: 6.3247  |  0:06:48s\n",
      "epoch 70 | loss: 6.06287 |  0:06:53s\n",
      "epoch 71 | loss: 6.36147 |  0:06:59s\n",
      "epoch 72 | loss: 5.83511 |  0:07:05s\n",
      "epoch 73 | loss: 5.89819 |  0:07:10s\n",
      "epoch 74 | loss: 5.78801 |  0:07:16s\n",
      "epoch 75 | loss: 5.78298 |  0:07:22s\n",
      "epoch 76 | loss: 5.60243 |  0:07:28s\n",
      "epoch 77 | loss: 5.92493 |  0:07:34s\n",
      "epoch 78 | loss: 5.59742 |  0:07:39s\n",
      "epoch 79 | loss: 5.57056 |  0:07:45s\n",
      "epoch 80 | loss: 5.47683 |  0:07:51s\n",
      "epoch 81 | loss: 5.42235 |  0:07:57s\n",
      "epoch 82 | loss: 5.55826 |  0:08:03s\n",
      "epoch 83 | loss: 5.41991 |  0:08:08s\n",
      "epoch 84 | loss: 5.54019 |  0:08:14s\n",
      "epoch 85 | loss: 5.38933 |  0:08:20s\n",
      "epoch 86 | loss: 5.37729 |  0:08:26s\n",
      "epoch 87 | loss: 5.3287  |  0:08:32s\n",
      "epoch 88 | loss: 5.38052 |  0:08:38s\n",
      "epoch 89 | loss: 5.38441 |  0:08:43s\n",
      "epoch 90 | loss: 5.36133 |  0:08:49s\n",
      "epoch 91 | loss: 5.128   |  0:08:55s\n",
      "epoch 92 | loss: 5.4364  |  0:09:01s\n",
      "epoch 93 | loss: 5.3609  |  0:09:07s\n",
      "epoch 94 | loss: 5.29697 |  0:09:12s\n",
      "epoch 95 | loss: 5.15928 |  0:09:18s\n",
      "epoch 96 | loss: 5.25913 |  0:09:24s\n",
      "epoch 97 | loss: 5.34035 |  0:09:29s\n",
      "epoch 98 | loss: 5.20202 |  0:09:35s\n",
      "epoch 99 | loss: 5.1051  |  0:09:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "0: [2.7376944614398324]\n",
      "1: [2.8661875269913644]\n",
      "2: [2.693232858724422]\n",
      "3: [7.4211230836075215]\n",
      "4: [2.4813181501130956]\n",
      "5: [2.604970366636673]\n",
      "6: [2.6291827114250705]\n",
      "7: [2.613729115405267]\n",
      "8: [2.608274487338463]\n",
      "9: [4.269978995763713]\n",
      "10: [2.6347771832508045]\n",
      "11: [2.6530211675944075]\n",
      "12: [2.5659268564450155]\n",
      "13: [2.6199099088269837]\n",
      "14: [3.018925174801892]\n",
      "[++] Ended the training process for fold 18\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 170.59907|  0:00:05s\n",
      "epoch 1  | loss: 63.32704|  0:00:11s\n",
      "epoch 2  | loss: 52.76789|  0:00:17s\n",
      "epoch 3  | loss: 29.48653|  0:00:23s\n",
      "epoch 4  | loss: 23.21464|  0:00:29s\n",
      "epoch 5  | loss: 20.07298|  0:00:34s\n",
      "epoch 6  | loss: 16.49928|  0:00:40s\n",
      "epoch 7  | loss: 14.67933|  0:00:46s\n",
      "epoch 8  | loss: 13.50074|  0:00:52s\n",
      "epoch 9  | loss: 12.80922|  0:00:58s\n",
      "epoch 10 | loss: 12.41715|  0:01:04s\n",
      "epoch 11 | loss: 12.09501|  0:01:10s\n",
      "epoch 12 | loss: 11.35775|  0:01:16s\n",
      "epoch 13 | loss: 10.98102|  0:01:22s\n",
      "epoch 14 | loss: 10.68377|  0:01:27s\n",
      "epoch 15 | loss: 10.61557|  0:01:33s\n",
      "epoch 16 | loss: 10.31439|  0:01:39s\n",
      "epoch 17 | loss: 9.73275 |  0:01:46s\n",
      "epoch 18 | loss: 9.83337 |  0:01:52s\n",
      "epoch 19 | loss: 9.67221 |  0:01:57s\n",
      "epoch 20 | loss: 9.4542  |  0:02:03s\n",
      "epoch 21 | loss: 9.69667 |  0:02:09s\n",
      "epoch 22 | loss: 9.18304 |  0:02:15s\n",
      "epoch 23 | loss: 9.03677 |  0:02:21s\n",
      "epoch 24 | loss: 8.82634 |  0:02:27s\n",
      "epoch 25 | loss: 8.89061 |  0:02:33s\n",
      "epoch 26 | loss: 8.59363 |  0:02:38s\n",
      "epoch 27 | loss: 8.60992 |  0:02:45s\n",
      "epoch 28 | loss: 8.57676 |  0:02:50s\n",
      "epoch 29 | loss: 8.38114 |  0:02:56s\n",
      "epoch 30 | loss: 8.22089 |  0:03:02s\n",
      "epoch 31 | loss: 8.04317 |  0:03:08s\n",
      "epoch 32 | loss: 7.88195 |  0:03:13s\n",
      "epoch 33 | loss: 7.8379  |  0:03:20s\n",
      "epoch 34 | loss: 7.7769  |  0:03:25s\n",
      "epoch 35 | loss: 7.54003 |  0:03:31s\n",
      "epoch 36 | loss: 7.64914 |  0:03:37s\n",
      "epoch 37 | loss: 7.66675 |  0:03:43s\n",
      "epoch 38 | loss: 7.46053 |  0:03:49s\n",
      "epoch 39 | loss: 7.42684 |  0:03:54s\n",
      "epoch 40 | loss: 7.30331 |  0:04:00s\n",
      "epoch 41 | loss: 7.12206 |  0:04:06s\n",
      "epoch 42 | loss: 7.09993 |  0:04:12s\n",
      "epoch 43 | loss: 6.93362 |  0:04:17s\n",
      "epoch 44 | loss: 7.02774 |  0:04:23s\n",
      "epoch 45 | loss: 7.05132 |  0:04:29s\n",
      "epoch 46 | loss: 6.79218 |  0:04:35s\n",
      "epoch 47 | loss: 6.8916  |  0:04:41s\n",
      "epoch 48 | loss: 6.79124 |  0:04:46s\n",
      "epoch 49 | loss: 6.75293 |  0:04:53s\n",
      "epoch 50 | loss: 6.65521 |  0:04:58s\n",
      "epoch 51 | loss: 6.91064 |  0:05:04s\n",
      "epoch 52 | loss: 6.91684 |  0:05:10s\n",
      "epoch 53 | loss: 6.53231 |  0:05:15s\n",
      "epoch 54 | loss: 6.35019 |  0:05:22s\n",
      "epoch 55 | loss: 6.34378 |  0:05:27s\n",
      "epoch 56 | loss: 6.26534 |  0:05:33s\n",
      "epoch 57 | loss: 6.48428 |  0:05:39s\n",
      "epoch 58 | loss: 6.36238 |  0:05:45s\n",
      "epoch 59 | loss: 6.36012 |  0:05:50s\n",
      "epoch 60 | loss: 6.69881 |  0:05:56s\n",
      "epoch 61 | loss: 6.28772 |  0:06:02s\n",
      "epoch 62 | loss: 6.2371  |  0:06:08s\n",
      "epoch 63 | loss: 6.29925 |  0:06:13s\n",
      "epoch 64 | loss: 6.17255 |  0:06:19s\n",
      "epoch 65 | loss: 6.16983 |  0:06:25s\n",
      "epoch 66 | loss: 5.9811  |  0:06:31s\n",
      "epoch 67 | loss: 6.03175 |  0:06:37s\n",
      "epoch 68 | loss: 5.89009 |  0:06:42s\n",
      "epoch 69 | loss: 5.95818 |  0:06:48s\n",
      "epoch 70 | loss: 5.88707 |  0:06:54s\n",
      "epoch 71 | loss: 5.70273 |  0:07:00s\n",
      "epoch 72 | loss: 5.70041 |  0:07:06s\n",
      "epoch 73 | loss: 5.96061 |  0:07:12s\n",
      "epoch 74 | loss: 5.90368 |  0:07:17s\n",
      "epoch 75 | loss: 5.91602 |  0:07:23s\n",
      "epoch 76 | loss: 5.56177 |  0:07:29s\n",
      "epoch 77 | loss: 5.95099 |  0:07:35s\n",
      "epoch 78 | loss: 5.82098 |  0:07:40s\n",
      "epoch 79 | loss: 5.52966 |  0:07:46s\n",
      "epoch 80 | loss: 5.62376 |  0:07:52s\n",
      "epoch 81 | loss: 5.57116 |  0:07:58s\n",
      "epoch 82 | loss: 5.62646 |  0:08:04s\n",
      "epoch 83 | loss: 5.50113 |  0:08:10s\n",
      "epoch 84 | loss: 5.48628 |  0:08:15s\n",
      "epoch 85 | loss: 5.39236 |  0:08:21s\n",
      "epoch 86 | loss: 5.30217 |  0:08:27s\n",
      "epoch 87 | loss: 5.21967 |  0:08:33s\n",
      "epoch 88 | loss: 5.34271 |  0:08:39s\n",
      "epoch 89 | loss: 5.16432 |  0:08:44s\n",
      "epoch 90 | loss: 5.33916 |  0:08:50s\n",
      "epoch 91 | loss: 5.19566 |  0:08:56s\n",
      "epoch 92 | loss: 5.3519  |  0:09:02s\n",
      "epoch 93 | loss: 5.19005 |  0:09:07s\n",
      "epoch 94 | loss: 5.16484 |  0:09:13s\n",
      "epoch 95 | loss: 5.23988 |  0:09:19s\n",
      "epoch 96 | loss: 5.19815 |  0:09:25s\n",
      "epoch 97 | loss: 5.05634 |  0:09:30s\n",
      "epoch 98 | loss: 5.10864 |  0:09:37s\n",
      "epoch 99 | loss: 5.63892 |  0:09:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 0\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 164.60427|  0:00:05s\n",
      "epoch 1  | loss: 58.7035 |  0:00:11s\n",
      "epoch 2  | loss: 53.45619|  0:00:17s\n",
      "epoch 3  | loss: 67.5195 |  0:00:23s\n",
      "epoch 4  | loss: 62.53139|  0:00:28s\n",
      "epoch 5  | loss: 58.89404|  0:00:34s\n",
      "epoch 6  | loss: 54.07446|  0:00:40s\n",
      "epoch 7  | loss: 47.61735|  0:00:45s\n",
      "epoch 8  | loss: 25.61539|  0:00:52s\n",
      "epoch 9  | loss: 21.78918|  0:00:57s\n",
      "epoch 10 | loss: 19.65609|  0:01:03s\n",
      "epoch 11 | loss: 16.99924|  0:01:08s\n",
      "epoch 12 | loss: 15.2565 |  0:01:14s\n",
      "epoch 13 | loss: 14.83114|  0:01:20s\n",
      "epoch 14 | loss: 13.2723 |  0:01:26s\n",
      "epoch 15 | loss: 12.62599|  0:01:32s\n",
      "epoch 16 | loss: 12.19202|  0:01:38s\n",
      "epoch 17 | loss: 12.00259|  0:01:43s\n",
      "epoch 18 | loss: 11.74335|  0:01:50s\n",
      "epoch 19 | loss: 11.88574|  0:01:55s\n",
      "epoch 20 | loss: 11.15073|  0:02:01s\n",
      "epoch 21 | loss: 10.62078|  0:02:07s\n",
      "epoch 22 | loss: 10.55577|  0:02:13s\n",
      "epoch 23 | loss: 10.73876|  0:02:19s\n",
      "epoch 24 | loss: 10.72869|  0:02:25s\n",
      "epoch 25 | loss: 10.26787|  0:02:31s\n",
      "epoch 26 | loss: 10.85079|  0:02:37s\n",
      "epoch 27 | loss: 10.28942|  0:02:43s\n",
      "epoch 28 | loss: 10.39227|  0:02:49s\n",
      "epoch 29 | loss: 9.57665 |  0:02:56s\n",
      "epoch 30 | loss: 9.76483 |  0:03:02s\n",
      "epoch 31 | loss: 9.22004 |  0:03:08s\n",
      "epoch 32 | loss: 9.17861 |  0:03:14s\n",
      "epoch 33 | loss: 8.99067 |  0:03:20s\n",
      "epoch 34 | loss: 8.96773 |  0:03:26s\n",
      "epoch 35 | loss: 8.73783 |  0:03:32s\n",
      "epoch 36 | loss: 8.49827 |  0:03:38s\n",
      "epoch 37 | loss: 8.56761 |  0:03:44s\n",
      "epoch 38 | loss: 8.6731  |  0:03:50s\n",
      "epoch 39 | loss: 8.38938 |  0:03:56s\n",
      "epoch 40 | loss: 8.55025 |  0:04:02s\n",
      "epoch 41 | loss: 8.31268 |  0:04:08s\n",
      "epoch 42 | loss: 7.99091 |  0:04:14s\n",
      "epoch 43 | loss: 8.11386 |  0:04:20s\n",
      "epoch 44 | loss: 8.04402 |  0:04:26s\n",
      "epoch 45 | loss: 7.93306 |  0:04:32s\n",
      "epoch 46 | loss: 7.77385 |  0:04:38s\n",
      "epoch 47 | loss: 7.87518 |  0:04:44s\n",
      "epoch 48 | loss: 7.75299 |  0:04:50s\n",
      "epoch 49 | loss: 7.55119 |  0:04:56s\n",
      "epoch 50 | loss: 7.71432 |  0:05:02s\n",
      "epoch 51 | loss: 7.27744 |  0:05:08s\n",
      "epoch 52 | loss: 7.34193 |  0:05:14s\n",
      "epoch 53 | loss: 7.99248 |  0:05:20s\n",
      "epoch 54 | loss: 7.47102 |  0:05:26s\n",
      "epoch 55 | loss: 7.28826 |  0:05:32s\n",
      "epoch 56 | loss: 7.11289 |  0:05:38s\n",
      "epoch 57 | loss: 7.31127 |  0:05:44s\n",
      "epoch 58 | loss: 7.12157 |  0:05:50s\n",
      "epoch 59 | loss: 6.90223 |  0:05:56s\n",
      "epoch 60 | loss: 6.86191 |  0:06:02s\n",
      "epoch 61 | loss: 6.96669 |  0:06:08s\n",
      "epoch 62 | loss: 7.03471 |  0:06:14s\n",
      "epoch 63 | loss: 6.69388 |  0:06:20s\n",
      "epoch 64 | loss: 6.95156 |  0:06:26s\n",
      "epoch 65 | loss: 6.88279 |  0:06:33s\n",
      "epoch 66 | loss: 6.79207 |  0:06:38s\n",
      "epoch 67 | loss: 6.48591 |  0:06:44s\n",
      "epoch 68 | loss: 6.59907 |  0:06:50s\n",
      "epoch 69 | loss: 6.4951  |  0:06:56s\n",
      "epoch 70 | loss: 7.00677 |  0:07:02s\n",
      "epoch 71 | loss: 6.72805 |  0:07:08s\n",
      "epoch 72 | loss: 6.56742 |  0:07:14s\n",
      "epoch 73 | loss: 6.32122 |  0:07:20s\n",
      "epoch 74 | loss: 6.65176 |  0:07:26s\n",
      "epoch 75 | loss: 6.40819 |  0:07:32s\n",
      "epoch 76 | loss: 6.60939 |  0:07:39s\n",
      "epoch 77 | loss: 6.71155 |  0:07:45s\n",
      "epoch 78 | loss: 6.38155 |  0:07:51s\n",
      "epoch 79 | loss: 6.29798 |  0:07:56s\n",
      "epoch 80 | loss: 6.04812 |  0:08:02s\n",
      "epoch 81 | loss: 6.32394 |  0:08:09s\n",
      "epoch 82 | loss: 6.16401 |  0:08:15s\n",
      "epoch 83 | loss: 6.12456 |  0:08:21s\n",
      "epoch 84 | loss: 6.18927 |  0:08:27s\n",
      "epoch 85 | loss: 6.0715  |  0:08:32s\n",
      "epoch 86 | loss: 5.7109  |  0:08:39s\n",
      "epoch 87 | loss: 5.87001 |  0:08:45s\n",
      "epoch 88 | loss: 5.76838 |  0:08:51s\n",
      "epoch 89 | loss: 5.77426 |  0:08:57s\n",
      "epoch 90 | loss: 5.88767 |  0:09:03s\n",
      "epoch 91 | loss: 5.76791 |  0:09:09s\n",
      "epoch 92 | loss: 6.01117 |  0:09:15s\n",
      "epoch 93 | loss: 5.82019 |  0:09:21s\n",
      "epoch 94 | loss: 5.81681 |  0:09:27s\n",
      "epoch 95 | loss: 5.7393  |  0:09:33s\n",
      "epoch 96 | loss: 6.288   |  0:09:39s\n",
      "epoch 97 | loss: 5.65941 |  0:09:45s\n",
      "epoch 98 | loss: 5.88425 |  0:09:51s\n",
      "epoch 99 | loss: 5.83135 |  0:09:57s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 1\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 171.53009|  0:00:05s\n",
      "epoch 1  | loss: 50.5067 |  0:00:12s\n",
      "epoch 2  | loss: 44.03956|  0:00:18s\n",
      "epoch 3  | loss: 35.57527|  0:00:24s\n",
      "epoch 4  | loss: 48.0548 |  0:00:30s\n",
      "epoch 5  | loss: 47.24249|  0:00:36s\n",
      "epoch 6  | loss: 31.64064|  0:00:42s\n",
      "epoch 7  | loss: 22.27455|  0:00:48s\n",
      "epoch 8  | loss: 18.68129|  0:00:54s\n",
      "epoch 9  | loss: 16.57962|  0:01:00s\n",
      "epoch 10 | loss: 14.74604|  0:01:06s\n",
      "epoch 11 | loss: 13.79289|  0:01:12s\n",
      "epoch 12 | loss: 13.03807|  0:01:19s\n",
      "epoch 13 | loss: 12.31187|  0:01:25s\n",
      "epoch 14 | loss: 11.78018|  0:01:31s\n",
      "epoch 15 | loss: 11.4771 |  0:01:37s\n",
      "epoch 16 | loss: 11.01372|  0:01:43s\n",
      "epoch 17 | loss: 10.77414|  0:01:49s\n",
      "epoch 18 | loss: 10.44246|  0:01:55s\n",
      "epoch 19 | loss: 10.23873|  0:02:01s\n",
      "epoch 20 | loss: 10.63397|  0:02:07s\n",
      "epoch 21 | loss: 9.97501 |  0:02:14s\n",
      "epoch 22 | loss: 9.75448 |  0:02:20s\n",
      "epoch 23 | loss: 9.41176 |  0:02:26s\n",
      "epoch 24 | loss: 9.55837 |  0:02:32s\n",
      "epoch 25 | loss: 9.39098 |  0:02:38s\n",
      "epoch 26 | loss: 9.32781 |  0:02:44s\n",
      "epoch 27 | loss: 9.00508 |  0:02:50s\n",
      "epoch 28 | loss: 8.95903 |  0:02:56s\n",
      "epoch 29 | loss: 8.73425 |  0:03:02s\n",
      "epoch 30 | loss: 8.72084 |  0:03:08s\n",
      "epoch 31 | loss: 8.51694 |  0:03:14s\n",
      "epoch 32 | loss: 8.50412 |  0:03:21s\n",
      "epoch 33 | loss: 8.24893 |  0:03:27s\n",
      "epoch 34 | loss: 8.254   |  0:03:33s\n",
      "epoch 35 | loss: 8.50237 |  0:03:39s\n",
      "epoch 36 | loss: 8.23601 |  0:03:45s\n",
      "epoch 37 | loss: 8.6328  |  0:03:51s\n",
      "epoch 38 | loss: 8.10497 |  0:03:57s\n",
      "epoch 39 | loss: 7.94324 |  0:04:03s\n",
      "epoch 40 | loss: 7.71177 |  0:04:09s\n",
      "epoch 41 | loss: 7.57895 |  0:04:15s\n",
      "epoch 42 | loss: 7.51263 |  0:04:22s\n",
      "epoch 43 | loss: 7.30079 |  0:04:28s\n",
      "epoch 44 | loss: 7.45126 |  0:04:34s\n",
      "epoch 45 | loss: 7.64221 |  0:04:40s\n",
      "epoch 46 | loss: 7.18108 |  0:04:46s\n",
      "epoch 47 | loss: 7.39322 |  0:04:52s\n",
      "epoch 48 | loss: 7.15394 |  0:04:58s\n",
      "epoch 49 | loss: 7.30686 |  0:05:04s\n",
      "epoch 50 | loss: 7.11245 |  0:05:10s\n",
      "epoch 51 | loss: 7.09688 |  0:05:16s\n",
      "epoch 52 | loss: 7.06907 |  0:05:22s\n",
      "epoch 53 | loss: 7.16416 |  0:05:28s\n",
      "epoch 54 | loss: 6.84201 |  0:05:34s\n",
      "epoch 55 | loss: 6.99348 |  0:05:40s\n",
      "epoch 56 | loss: 7.10149 |  0:05:46s\n",
      "epoch 57 | loss: 6.90648 |  0:05:53s\n",
      "epoch 58 | loss: 6.82128 |  0:05:59s\n",
      "epoch 59 | loss: 6.64963 |  0:06:05s\n",
      "epoch 60 | loss: 6.84466 |  0:06:11s\n",
      "epoch 61 | loss: 6.54179 |  0:06:17s\n",
      "epoch 62 | loss: 6.62151 |  0:06:23s\n",
      "epoch 63 | loss: 6.50569 |  0:06:29s\n",
      "epoch 64 | loss: 6.32036 |  0:06:35s\n",
      "epoch 65 | loss: 6.3913  |  0:06:41s\n",
      "epoch 66 | loss: 6.24403 |  0:06:47s\n",
      "epoch 67 | loss: 6.21328 |  0:06:53s\n",
      "epoch 68 | loss: 6.36664 |  0:07:00s\n",
      "epoch 69 | loss: 6.26015 |  0:07:06s\n",
      "epoch 70 | loss: 6.51576 |  0:07:12s\n",
      "epoch 71 | loss: 6.27508 |  0:07:18s\n",
      "epoch 72 | loss: 6.08075 |  0:07:24s\n",
      "epoch 73 | loss: 6.20378 |  0:07:31s\n",
      "epoch 74 | loss: 6.25688 |  0:07:36s\n",
      "epoch 75 | loss: 5.9402  |  0:07:42s\n",
      "epoch 76 | loss: 5.86167 |  0:07:48s\n",
      "epoch 77 | loss: 5.74476 |  0:07:54s\n",
      "epoch 78 | loss: 5.96219 |  0:08:01s\n",
      "epoch 79 | loss: 5.91203 |  0:08:07s\n",
      "epoch 80 | loss: 5.89    |  0:08:12s\n",
      "epoch 81 | loss: 5.85467 |  0:08:19s\n",
      "epoch 82 | loss: 5.7039  |  0:08:25s\n",
      "epoch 83 | loss: 5.96085 |  0:08:31s\n",
      "epoch 84 | loss: 5.64444 |  0:08:37s\n",
      "epoch 85 | loss: 5.99668 |  0:08:43s\n",
      "epoch 86 | loss: 5.81629 |  0:08:49s\n",
      "epoch 87 | loss: 5.74762 |  0:08:55s\n",
      "epoch 88 | loss: 5.6728  |  0:09:01s\n",
      "epoch 89 | loss: 5.69555 |  0:09:07s\n",
      "epoch 90 | loss: 6.00488 |  0:09:13s\n",
      "epoch 91 | loss: 5.63693 |  0:09:19s\n",
      "epoch 92 | loss: 5.63529 |  0:09:25s\n",
      "epoch 93 | loss: 5.64061 |  0:09:31s\n",
      "epoch 94 | loss: 5.56458 |  0:09:38s\n",
      "epoch 95 | loss: 5.62997 |  0:09:44s\n",
      "epoch 96 | loss: 5.52424 |  0:09:50s\n",
      "epoch 97 | loss: 5.43863 |  0:09:56s\n",
      "epoch 98 | loss: 5.61009 |  0:10:02s\n",
      "epoch 99 | loss: 5.61767 |  0:10:08s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 2\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 167.26479|  0:00:05s\n",
      "epoch 1  | loss: 61.99379|  0:00:11s\n",
      "epoch 2  | loss: 63.62171|  0:00:17s\n",
      "epoch 3  | loss: 40.0143 |  0:00:24s\n",
      "epoch 4  | loss: 33.45835|  0:00:30s\n",
      "epoch 5  | loss: 31.21544|  0:00:36s\n",
      "epoch 6  | loss: 25.30482|  0:00:42s\n",
      "epoch 7  | loss: 20.0346 |  0:00:48s\n",
      "epoch 8  | loss: 18.08479|  0:00:54s\n",
      "epoch 9  | loss: 16.5862 |  0:01:00s\n",
      "epoch 10 | loss: 15.38046|  0:01:07s\n",
      "epoch 11 | loss: 14.73791|  0:01:12s\n",
      "epoch 12 | loss: 14.58513|  0:01:18s\n",
      "epoch 13 | loss: 13.73673|  0:01:24s\n",
      "epoch 14 | loss: 12.8915 |  0:01:31s\n",
      "epoch 15 | loss: 12.51692|  0:01:36s\n",
      "epoch 16 | loss: 12.33789|  0:01:43s\n",
      "epoch 17 | loss: 11.96   |  0:01:48s\n",
      "epoch 18 | loss: 11.61084|  0:01:54s\n",
      "epoch 19 | loss: 10.76894|  0:02:01s\n",
      "epoch 20 | loss: 10.73442|  0:02:07s\n",
      "epoch 21 | loss: 10.40049|  0:02:13s\n",
      "epoch 22 | loss: 10.57005|  0:02:19s\n",
      "epoch 23 | loss: 10.19379|  0:02:25s\n",
      "epoch 24 | loss: 10.13037|  0:02:31s\n",
      "epoch 25 | loss: 10.08006|  0:02:37s\n",
      "epoch 26 | loss: 9.91865 |  0:02:43s\n",
      "epoch 27 | loss: 10.09601|  0:02:49s\n",
      "epoch 28 | loss: 9.92522 |  0:02:55s\n",
      "epoch 29 | loss: 9.33087 |  0:03:01s\n",
      "epoch 30 | loss: 9.47163 |  0:03:08s\n",
      "epoch 31 | loss: 8.90033 |  0:03:13s\n",
      "epoch 32 | loss: 8.87936 |  0:03:19s\n",
      "epoch 33 | loss: 8.77033 |  0:03:25s\n",
      "epoch 34 | loss: 8.71532 |  0:03:32s\n",
      "epoch 35 | loss: 8.2707  |  0:03:38s\n",
      "epoch 36 | loss: 8.51026 |  0:03:44s\n",
      "epoch 37 | loss: 8.52984 |  0:03:50s\n",
      "epoch 38 | loss: 8.54644 |  0:03:56s\n",
      "epoch 39 | loss: 8.53143 |  0:04:03s\n",
      "epoch 40 | loss: 9.39403 |  0:04:08s\n",
      "epoch 41 | loss: 8.47795 |  0:04:14s\n",
      "epoch 42 | loss: 8.4103  |  0:04:20s\n",
      "epoch 43 | loss: 7.98112 |  0:04:26s\n",
      "epoch 44 | loss: 8.0627  |  0:04:32s\n",
      "epoch 45 | loss: 7.75424 |  0:04:39s\n",
      "epoch 46 | loss: 7.82751 |  0:04:44s\n",
      "epoch 47 | loss: 7.61545 |  0:04:50s\n",
      "epoch 48 | loss: 7.65642 |  0:04:56s\n",
      "epoch 49 | loss: 7.46525 |  0:05:02s\n",
      "epoch 50 | loss: 7.28298 |  0:05:09s\n",
      "epoch 51 | loss: 7.07771 |  0:05:15s\n",
      "epoch 52 | loss: 7.17089 |  0:05:21s\n",
      "epoch 53 | loss: 7.23155 |  0:05:27s\n",
      "epoch 54 | loss: 7.13963 |  0:05:33s\n",
      "epoch 55 | loss: 6.91235 |  0:05:39s\n",
      "epoch 56 | loss: 6.72358 |  0:05:45s\n",
      "epoch 57 | loss: 7.0829  |  0:05:51s\n",
      "epoch 58 | loss: 7.27322 |  0:05:57s\n",
      "epoch 59 | loss: 6.71459 |  0:06:03s\n",
      "epoch 60 | loss: 6.77942 |  0:06:09s\n",
      "epoch 61 | loss: 6.7805  |  0:06:15s\n",
      "epoch 62 | loss: 6.61876 |  0:06:21s\n",
      "epoch 63 | loss: 6.71281 |  0:06:27s\n",
      "epoch 64 | loss: 6.84335 |  0:06:33s\n",
      "epoch 65 | loss: 6.28319 |  0:06:40s\n",
      "epoch 66 | loss: 6.26946 |  0:06:46s\n",
      "epoch 67 | loss: 6.434   |  0:06:52s\n",
      "epoch 68 | loss: 6.44907 |  0:06:57s\n",
      "epoch 69 | loss: 6.31186 |  0:07:03s\n",
      "epoch 70 | loss: 6.35224 |  0:07:10s\n",
      "epoch 71 | loss: 6.16334 |  0:07:16s\n",
      "epoch 72 | loss: 6.3682  |  0:07:22s\n",
      "epoch 73 | loss: 6.12151 |  0:07:28s\n",
      "epoch 74 | loss: 6.033   |  0:07:34s\n",
      "epoch 75 | loss: 6.04822 |  0:07:40s\n",
      "epoch 76 | loss: 5.97478 |  0:07:46s\n",
      "epoch 77 | loss: 5.96986 |  0:07:52s\n",
      "epoch 78 | loss: 5.88985 |  0:07:58s\n",
      "epoch 79 | loss: 5.81541 |  0:08:04s\n",
      "epoch 80 | loss: 5.81697 |  0:08:10s\n",
      "epoch 81 | loss: 5.83054 |  0:08:16s\n",
      "epoch 82 | loss: 5.98945 |  0:08:22s\n",
      "epoch 83 | loss: 5.79421 |  0:08:28s\n",
      "epoch 84 | loss: 5.75595 |  0:08:34s\n",
      "epoch 85 | loss: 5.72182 |  0:08:40s\n",
      "epoch 86 | loss: 5.76699 |  0:08:47s\n",
      "epoch 87 | loss: 5.7123  |  0:08:53s\n",
      "epoch 88 | loss: 5.91116 |  0:08:58s\n",
      "epoch 89 | loss: 5.62655 |  0:09:05s\n",
      "epoch 90 | loss: 5.6329  |  0:09:11s\n",
      "epoch 91 | loss: 5.32347 |  0:09:17s\n",
      "epoch 92 | loss: 5.45486 |  0:09:23s\n",
      "epoch 93 | loss: 5.46436 |  0:09:29s\n",
      "epoch 94 | loss: 5.39977 |  0:09:35s\n",
      "epoch 95 | loss: 5.27138 |  0:09:40s\n",
      "epoch 96 | loss: 5.27321 |  0:09:47s\n",
      "epoch 97 | loss: 5.20208 |  0:09:53s\n",
      "epoch 98 | loss: 5.41255 |  0:09:59s\n",
      "epoch 99 | loss: 5.37268 |  0:10:04s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 3\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 171.64374|  0:00:06s\n",
      "epoch 1  | loss: 57.18357|  0:00:12s\n",
      "epoch 2  | loss: 62.0515 |  0:00:18s\n",
      "epoch 3  | loss: 60.89714|  0:00:24s\n",
      "epoch 4  | loss: 45.76245|  0:00:30s\n",
      "epoch 5  | loss: 51.01527|  0:00:36s\n",
      "epoch 6  | loss: 42.33194|  0:00:42s\n",
      "epoch 7  | loss: 30.47065|  0:00:48s\n",
      "epoch 8  | loss: 23.24729|  0:00:54s\n",
      "epoch 9  | loss: 22.83683|  0:01:00s\n",
      "epoch 10 | loss: 17.37607|  0:01:06s\n",
      "epoch 11 | loss: 16.53611|  0:01:13s\n",
      "epoch 12 | loss: 15.91886|  0:01:18s\n",
      "epoch 13 | loss: 15.61867|  0:01:25s\n",
      "epoch 14 | loss: 15.33379|  0:01:30s\n",
      "epoch 15 | loss: 14.74385|  0:01:36s\n",
      "epoch 16 | loss: 14.81263|  0:01:43s\n",
      "epoch 17 | loss: 15.19854|  0:01:49s\n",
      "epoch 18 | loss: 15.05115|  0:01:55s\n",
      "epoch 19 | loss: 15.70735|  0:02:01s\n",
      "epoch 20 | loss: 15.05718|  0:02:06s\n",
      "epoch 21 | loss: 14.8422 |  0:02:12s\n",
      "epoch 22 | loss: 14.84849|  0:02:19s\n",
      "epoch 23 | loss: 14.5348 |  0:02:25s\n",
      "epoch 24 | loss: 14.49933|  0:02:31s\n",
      "epoch 25 | loss: 14.45284|  0:02:37s\n",
      "epoch 26 | loss: 13.77575|  0:02:43s\n",
      "epoch 27 | loss: 13.81879|  0:02:49s\n",
      "epoch 28 | loss: 13.63757|  0:02:55s\n",
      "epoch 29 | loss: 13.68526|  0:03:01s\n",
      "epoch 30 | loss: 13.88278|  0:03:07s\n",
      "epoch 31 | loss: 13.61216|  0:03:14s\n",
      "epoch 32 | loss: 13.58955|  0:03:20s\n",
      "epoch 33 | loss: 13.66302|  0:03:26s\n",
      "epoch 34 | loss: 12.85819|  0:03:32s\n",
      "epoch 35 | loss: 12.40706|  0:03:38s\n",
      "epoch 36 | loss: 14.6149 |  0:03:44s\n",
      "epoch 37 | loss: 12.67963|  0:03:50s\n",
      "epoch 38 | loss: 12.6369 |  0:03:56s\n",
      "epoch 39 | loss: 12.12833|  0:04:02s\n",
      "epoch 40 | loss: 11.46565|  0:04:08s\n",
      "epoch 41 | loss: 11.30444|  0:04:14s\n",
      "epoch 42 | loss: 10.93513|  0:04:21s\n",
      "epoch 43 | loss: 10.50632|  0:04:26s\n",
      "epoch 44 | loss: 10.85152|  0:04:33s\n",
      "epoch 45 | loss: 10.20785|  0:04:38s\n",
      "epoch 46 | loss: 9.72585 |  0:04:45s\n",
      "epoch 47 | loss: 9.81521 |  0:04:51s\n",
      "epoch 48 | loss: 9.66282 |  0:04:57s\n",
      "epoch 49 | loss: 9.55213 |  0:05:03s\n",
      "epoch 50 | loss: 9.33411 |  0:05:09s\n",
      "epoch 51 | loss: 9.09229 |  0:05:15s\n",
      "epoch 52 | loss: 8.90315 |  0:05:21s\n",
      "epoch 53 | loss: 9.02241 |  0:05:27s\n",
      "epoch 54 | loss: 8.74989 |  0:05:33s\n",
      "epoch 55 | loss: 8.60699 |  0:05:39s\n",
      "epoch 56 | loss: 8.93009 |  0:05:45s\n",
      "epoch 57 | loss: 9.13079 |  0:05:51s\n",
      "epoch 58 | loss: 9.41588 |  0:05:58s\n",
      "epoch 59 | loss: 9.26375 |  0:06:04s\n",
      "epoch 60 | loss: 9.17746 |  0:06:10s\n",
      "epoch 61 | loss: 10.02177|  0:06:15s\n",
      "epoch 62 | loss: 8.70015 |  0:06:22s\n",
      "epoch 63 | loss: 8.48321 |  0:06:28s\n",
      "epoch 64 | loss: 8.8359  |  0:06:34s\n",
      "epoch 65 | loss: 8.90931 |  0:06:40s\n",
      "epoch 66 | loss: 8.91714 |  0:06:46s\n",
      "epoch 67 | loss: 8.56097 |  0:06:52s\n",
      "epoch 68 | loss: 8.08463 |  0:06:58s\n",
      "epoch 69 | loss: 8.06521 |  0:07:04s\n",
      "epoch 70 | loss: 8.01755 |  0:07:10s\n",
      "epoch 71 | loss: 7.91744 |  0:07:16s\n",
      "epoch 72 | loss: 7.81634 |  0:07:22s\n",
      "epoch 73 | loss: 7.79201 |  0:07:29s\n",
      "epoch 74 | loss: 8.0515  |  0:07:35s\n",
      "epoch 75 | loss: 8.26677 |  0:07:41s\n",
      "epoch 76 | loss: 8.16018 |  0:07:47s\n",
      "epoch 77 | loss: 8.27785 |  0:07:53s\n",
      "epoch 78 | loss: 8.16619 |  0:07:59s\n",
      "epoch 79 | loss: 8.66078 |  0:08:05s\n",
      "epoch 80 | loss: 8.78116 |  0:08:11s\n",
      "epoch 81 | loss: 8.15437 |  0:08:17s\n",
      "epoch 82 | loss: 8.02111 |  0:08:23s\n",
      "epoch 83 | loss: 7.59405 |  0:08:29s\n",
      "epoch 84 | loss: 7.46238 |  0:08:36s\n",
      "epoch 85 | loss: 7.28152 |  0:08:41s\n",
      "epoch 86 | loss: 7.16743 |  0:08:48s\n",
      "epoch 87 | loss: 7.12959 |  0:08:53s\n",
      "epoch 88 | loss: 7.2953  |  0:08:59s\n",
      "epoch 89 | loss: 7.04111 |  0:09:06s\n",
      "epoch 90 | loss: 6.99718 |  0:09:12s\n",
      "epoch 91 | loss: 6.9383  |  0:09:18s\n",
      "epoch 92 | loss: 6.75071 |  0:09:24s\n",
      "epoch 93 | loss: 7.06087 |  0:09:30s\n",
      "epoch 94 | loss: 6.7867  |  0:09:36s\n",
      "epoch 95 | loss: 6.63275 |  0:09:42s\n",
      "epoch 96 | loss: 6.7851  |  0:09:48s\n",
      "epoch 97 | loss: 6.60064 |  0:09:54s\n",
      "epoch 98 | loss: 6.72724 |  0:10:00s\n",
      "epoch 99 | loss: 6.61172 |  0:10:06s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 4\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 165.04519|  0:00:05s\n",
      "epoch 1  | loss: 60.35921|  0:00:11s\n",
      "epoch 2  | loss: 65.56824|  0:00:17s\n",
      "epoch 3  | loss: 54.45972|  0:00:24s\n",
      "epoch 4  | loss: 39.08683|  0:00:29s\n",
      "epoch 5  | loss: 23.2007 |  0:00:35s\n",
      "epoch 6  | loss: 19.36412|  0:00:41s\n",
      "epoch 7  | loss: 18.98806|  0:00:47s\n",
      "epoch 8  | loss: 17.24276|  0:00:53s\n",
      "epoch 9  | loss: 16.23997|  0:01:00s\n",
      "epoch 10 | loss: 14.89286|  0:01:06s\n",
      "epoch 11 | loss: 14.079  |  0:01:11s\n",
      "epoch 12 | loss: 13.6905 |  0:01:17s\n",
      "epoch 13 | loss: 13.35637|  0:01:23s\n",
      "epoch 14 | loss: 12.86641|  0:01:30s\n",
      "epoch 15 | loss: 12.7359 |  0:01:35s\n",
      "epoch 16 | loss: 13.38676|  0:01:42s\n",
      "epoch 17 | loss: 12.30859|  0:01:47s\n",
      "epoch 18 | loss: 12.25797|  0:01:54s\n",
      "epoch 19 | loss: 11.96729|  0:02:00s\n",
      "epoch 20 | loss: 11.76099|  0:02:06s\n",
      "epoch 21 | loss: 11.53233|  0:02:12s\n",
      "epoch 22 | loss: 11.76751|  0:02:18s\n",
      "epoch 23 | loss: 12.56747|  0:02:24s\n",
      "epoch 24 | loss: 12.19041|  0:02:30s\n",
      "epoch 25 | loss: 11.68565|  0:02:36s\n",
      "epoch 26 | loss: 11.7874 |  0:02:42s\n",
      "epoch 27 | loss: 11.45732|  0:02:48s\n",
      "epoch 28 | loss: 12.26364|  0:02:54s\n",
      "epoch 29 | loss: 11.34983|  0:03:00s\n",
      "epoch 30 | loss: 11.34718|  0:03:06s\n",
      "epoch 31 | loss: 10.74293|  0:03:12s\n",
      "epoch 32 | loss: 10.14328|  0:03:18s\n",
      "epoch 33 | loss: 9.95841 |  0:03:24s\n",
      "epoch 34 | loss: 9.87894 |  0:03:30s\n",
      "epoch 35 | loss: 9.8112  |  0:03:36s\n",
      "epoch 36 | loss: 9.25889 |  0:03:42s\n",
      "epoch 37 | loss: 9.64719 |  0:03:48s\n",
      "epoch 38 | loss: 9.75397 |  0:03:54s\n",
      "epoch 39 | loss: 9.42744 |  0:04:00s\n",
      "epoch 40 | loss: 9.18147 |  0:04:06s\n",
      "epoch 41 | loss: 9.32099 |  0:04:12s\n",
      "epoch 42 | loss: 8.97325 |  0:04:18s\n",
      "epoch 43 | loss: 8.78914 |  0:04:24s\n",
      "epoch 44 | loss: 8.75735 |  0:04:30s\n",
      "epoch 45 | loss: 8.56793 |  0:04:37s\n",
      "epoch 46 | loss: 9.16367 |  0:04:42s\n",
      "epoch 47 | loss: 8.57767 |  0:04:48s\n",
      "epoch 48 | loss: 8.37256 |  0:04:54s\n",
      "epoch 49 | loss: 8.39396 |  0:05:00s\n",
      "epoch 50 | loss: 8.18362 |  0:05:07s\n",
      "epoch 51 | loss: 8.18548 |  0:05:13s\n",
      "epoch 52 | loss: 8.16637 |  0:05:19s\n",
      "epoch 53 | loss: 8.14515 |  0:05:25s\n",
      "epoch 54 | loss: 7.8211  |  0:05:30s\n",
      "epoch 55 | loss: 7.90133 |  0:05:36s\n",
      "epoch 56 | loss: 7.77686 |  0:05:43s\n",
      "epoch 57 | loss: 7.62219 |  0:05:48s\n",
      "epoch 58 | loss: 7.76657 |  0:05:55s\n",
      "epoch 59 | loss: 7.43898 |  0:06:01s\n",
      "epoch 60 | loss: 7.4354  |  0:06:07s\n",
      "epoch 61 | loss: 7.39716 |  0:06:13s\n",
      "epoch 62 | loss: 7.29392 |  0:06:19s\n",
      "epoch 63 | loss: 7.0157  |  0:06:25s\n",
      "epoch 64 | loss: 7.05527 |  0:06:31s\n",
      "epoch 65 | loss: 7.00953 |  0:06:37s\n",
      "epoch 66 | loss: 6.86978 |  0:06:43s\n",
      "epoch 67 | loss: 6.98276 |  0:06:49s\n",
      "epoch 68 | loss: 6.9974  |  0:06:55s\n",
      "epoch 69 | loss: 6.94711 |  0:07:01s\n",
      "epoch 70 | loss: 6.9848  |  0:07:07s\n",
      "epoch 71 | loss: 6.75203 |  0:07:13s\n",
      "epoch 72 | loss: 6.84916 |  0:07:19s\n",
      "epoch 73 | loss: 6.59468 |  0:07:25s\n",
      "epoch 74 | loss: 6.65903 |  0:07:31s\n",
      "epoch 75 | loss: 6.66578 |  0:07:37s\n",
      "epoch 76 | loss: 6.50925 |  0:07:43s\n",
      "epoch 77 | loss: 6.50719 |  0:07:49s\n",
      "epoch 78 | loss: 6.58816 |  0:07:55s\n",
      "epoch 79 | loss: 6.38546 |  0:08:01s\n",
      "epoch 80 | loss: 6.5345  |  0:08:07s\n",
      "epoch 81 | loss: 6.44649 |  0:08:13s\n",
      "epoch 82 | loss: 6.33479 |  0:08:19s\n",
      "epoch 83 | loss: 6.5195  |  0:08:25s\n",
      "epoch 84 | loss: 6.32385 |  0:08:31s\n",
      "epoch 85 | loss: 6.23783 |  0:08:37s\n",
      "epoch 86 | loss: 6.14778 |  0:08:43s\n",
      "epoch 87 | loss: 6.12984 |  0:08:50s\n",
      "epoch 88 | loss: 6.01116 |  0:08:55s\n",
      "epoch 89 | loss: 6.13409 |  0:09:01s\n",
      "epoch 90 | loss: 6.19166 |  0:09:07s\n",
      "epoch 91 | loss: 5.88229 |  0:09:13s\n",
      "epoch 92 | loss: 6.04138 |  0:09:20s\n",
      "epoch 93 | loss: 6.10181 |  0:09:26s\n",
      "epoch 94 | loss: 6.02759 |  0:09:32s\n",
      "epoch 95 | loss: 5.8577  |  0:09:37s\n",
      "epoch 96 | loss: 5.80199 |  0:09:43s\n",
      "epoch 97 | loss: 5.81651 |  0:09:50s\n",
      "epoch 98 | loss: 5.69567 |  0:09:56s\n",
      "epoch 99 | loss: 5.73426 |  0:10:02s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 5\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 170.038 |  0:00:05s\n",
      "epoch 1  | loss: 50.70493|  0:00:12s\n",
      "epoch 2  | loss: 50.65112|  0:00:18s\n",
      "epoch 3  | loss: 57.85317|  0:00:24s\n",
      "epoch 4  | loss: 38.53175|  0:00:30s\n",
      "epoch 5  | loss: 33.5551 |  0:00:36s\n",
      "epoch 6  | loss: 26.73555|  0:00:42s\n",
      "epoch 7  | loss: 22.91053|  0:00:48s\n",
      "epoch 8  | loss: 19.80857|  0:00:54s\n",
      "epoch 9  | loss: 25.19165|  0:01:00s\n",
      "epoch 10 | loss: 19.01862|  0:01:06s\n",
      "epoch 11 | loss: 14.19413|  0:01:12s\n",
      "epoch 12 | loss: 13.12487|  0:01:19s\n",
      "epoch 13 | loss: 12.3435 |  0:01:25s\n",
      "epoch 14 | loss: 12.27687|  0:01:30s\n",
      "epoch 15 | loss: 12.95649|  0:01:37s\n",
      "epoch 16 | loss: 12.03741|  0:01:42s\n",
      "epoch 17 | loss: 11.78921|  0:01:49s\n",
      "epoch 18 | loss: 11.15846|  0:01:55s\n",
      "epoch 19 | loss: 10.83299|  0:02:01s\n",
      "epoch 20 | loss: 10.29162|  0:02:07s\n",
      "epoch 21 | loss: 10.33778|  0:02:13s\n",
      "epoch 22 | loss: 10.22113|  0:02:19s\n",
      "epoch 23 | loss: 9.65845 |  0:02:25s\n",
      "epoch 24 | loss: 9.60113 |  0:02:31s\n",
      "epoch 25 | loss: 9.92254 |  0:02:37s\n",
      "epoch 26 | loss: 9.77482 |  0:02:43s\n",
      "epoch 27 | loss: 9.27954 |  0:02:49s\n",
      "epoch 28 | loss: 9.21397 |  0:02:55s\n",
      "epoch 29 | loss: 9.13515 |  0:03:01s\n",
      "epoch 30 | loss: 9.48311 |  0:03:07s\n",
      "epoch 31 | loss: 8.74441 |  0:03:13s\n",
      "epoch 32 | loss: 8.79902 |  0:03:20s\n",
      "epoch 33 | loss: 8.56088 |  0:03:26s\n",
      "epoch 34 | loss: 8.67609 |  0:03:32s\n",
      "epoch 35 | loss: 8.6309  |  0:03:38s\n",
      "epoch 36 | loss: 8.34034 |  0:03:44s\n",
      "epoch 37 | loss: 8.25804 |  0:03:50s\n",
      "epoch 38 | loss: 8.10203 |  0:03:56s\n",
      "epoch 39 | loss: 8.31882 |  0:04:03s\n",
      "epoch 40 | loss: 8.44752 |  0:04:09s\n",
      "epoch 41 | loss: 8.0029  |  0:04:15s\n",
      "epoch 42 | loss: 7.88327 |  0:04:21s\n",
      "epoch 43 | loss: 7.58771 |  0:04:28s\n",
      "epoch 44 | loss: 7.36553 |  0:04:34s\n",
      "epoch 45 | loss: 7.57065 |  0:04:40s\n",
      "epoch 46 | loss: 7.46125 |  0:04:46s\n",
      "epoch 47 | loss: 7.39778 |  0:04:51s\n",
      "epoch 48 | loss: 7.3069  |  0:04:58s\n",
      "epoch 49 | loss: 7.24916 |  0:05:04s\n",
      "epoch 50 | loss: 7.2941  |  0:05:10s\n",
      "epoch 51 | loss: 7.23959 |  0:05:16s\n",
      "epoch 52 | loss: 7.26048 |  0:05:22s\n",
      "epoch 53 | loss: 7.02407 |  0:05:28s\n",
      "epoch 54 | loss: 7.04136 |  0:05:34s\n",
      "epoch 55 | loss: 7.0653  |  0:05:40s\n",
      "epoch 56 | loss: 7.16953 |  0:05:46s\n",
      "epoch 57 | loss: 7.01363 |  0:05:52s\n",
      "epoch 58 | loss: 6.86139 |  0:05:59s\n",
      "epoch 59 | loss: 6.84406 |  0:06:05s\n",
      "epoch 60 | loss: 6.6199  |  0:06:11s\n",
      "epoch 61 | loss: 6.59309 |  0:06:17s\n",
      "epoch 62 | loss: 6.79112 |  0:06:23s\n",
      "epoch 63 | loss: 6.53529 |  0:06:29s\n",
      "epoch 64 | loss: 6.47081 |  0:06:35s\n",
      "epoch 65 | loss: 6.44654 |  0:06:41s\n",
      "epoch 66 | loss: 6.38087 |  0:06:47s\n",
      "epoch 67 | loss: 6.44328 |  0:06:53s\n",
      "epoch 68 | loss: 6.19285 |  0:06:59s\n",
      "epoch 69 | loss: 6.40953 |  0:07:05s\n",
      "epoch 70 | loss: 6.17681 |  0:07:11s\n",
      "epoch 71 | loss: 6.37002 |  0:07:17s\n",
      "epoch 72 | loss: 6.1167  |  0:07:23s\n",
      "epoch 73 | loss: 5.98635 |  0:07:29s\n",
      "epoch 74 | loss: 6.07103 |  0:07:36s\n",
      "epoch 75 | loss: 5.97383 |  0:07:42s\n",
      "epoch 76 | loss: 5.97598 |  0:07:48s\n",
      "epoch 77 | loss: 6.04591 |  0:07:54s\n",
      "epoch 78 | loss: 6.01763 |  0:07:59s\n",
      "epoch 79 | loss: 5.9859  |  0:08:06s\n",
      "epoch 80 | loss: 6.0396  |  0:08:12s\n",
      "epoch 81 | loss: 6.21319 |  0:08:18s\n",
      "epoch 82 | loss: 6.28036 |  0:08:24s\n",
      "epoch 83 | loss: 6.05411 |  0:08:30s\n",
      "epoch 84 | loss: 5.95638 |  0:08:37s\n",
      "epoch 85 | loss: 5.78336 |  0:08:42s\n",
      "epoch 86 | loss: 5.62975 |  0:08:48s\n",
      "epoch 87 | loss: 5.82719 |  0:08:54s\n",
      "epoch 88 | loss: 5.5021  |  0:09:00s\n",
      "epoch 89 | loss: 5.53601 |  0:09:07s\n",
      "epoch 90 | loss: 5.73829 |  0:09:13s\n",
      "epoch 91 | loss: 5.61443 |  0:09:19s\n",
      "epoch 92 | loss: 5.68519 |  0:09:25s\n",
      "epoch 93 | loss: 5.53009 |  0:09:31s\n",
      "epoch 94 | loss: 5.53628 |  0:09:37s\n",
      "epoch 95 | loss: 5.49252 |  0:09:43s\n",
      "epoch 96 | loss: 5.36425 |  0:09:49s\n",
      "epoch 97 | loss: 5.35641 |  0:09:55s\n",
      "epoch 98 | loss: 5.35637 |  0:10:01s\n",
      "epoch 99 | loss: 5.44879 |  0:10:07s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 6\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 154.15219|  0:00:06s\n",
      "epoch 1  | loss: 55.34657|  0:00:12s\n",
      "epoch 2  | loss: 55.72579|  0:00:18s\n",
      "epoch 3  | loss: 50.47389|  0:00:24s\n",
      "epoch 4  | loss: 32.03361|  0:00:30s\n",
      "epoch 5  | loss: 30.51052|  0:00:36s\n",
      "epoch 6  | loss: 26.76539|  0:00:42s\n",
      "epoch 7  | loss: 22.1861 |  0:00:48s\n",
      "epoch 8  | loss: 19.56634|  0:00:54s\n",
      "epoch 9  | loss: 19.30529|  0:01:00s\n",
      "epoch 10 | loss: 18.57977|  0:01:06s\n",
      "epoch 11 | loss: 16.84032|  0:01:12s\n",
      "epoch 12 | loss: 16.18114|  0:01:19s\n",
      "epoch 13 | loss: 16.13832|  0:01:24s\n",
      "epoch 14 | loss: 14.9748 |  0:01:31s\n",
      "epoch 15 | loss: 14.16376|  0:01:37s\n",
      "epoch 16 | loss: 13.04145|  0:01:43s\n",
      "epoch 17 | loss: 12.47504|  0:01:49s\n",
      "epoch 18 | loss: 12.13367|  0:01:55s\n",
      "epoch 19 | loss: 11.57936|  0:02:01s\n",
      "epoch 20 | loss: 11.08082|  0:02:07s\n",
      "epoch 21 | loss: 10.80369|  0:02:13s\n",
      "epoch 22 | loss: 10.46869|  0:02:19s\n",
      "epoch 23 | loss: 10.15006|  0:02:25s\n",
      "epoch 24 | loss: 10.18727|  0:02:31s\n",
      "epoch 25 | loss: 10.37363|  0:02:38s\n",
      "epoch 26 | loss: 9.68392 |  0:02:44s\n",
      "epoch 27 | loss: 9.56928 |  0:02:50s\n",
      "epoch 28 | loss: 9.28679 |  0:02:56s\n",
      "epoch 29 | loss: 8.96411 |  0:03:02s\n",
      "epoch 30 | loss: 8.93913 |  0:03:08s\n",
      "epoch 31 | loss: 9.05925 |  0:03:14s\n",
      "epoch 32 | loss: 8.70946 |  0:03:20s\n",
      "epoch 33 | loss: 8.74757 |  0:03:26s\n",
      "epoch 34 | loss: 8.69111 |  0:03:32s\n",
      "epoch 35 | loss: 8.35865 |  0:03:38s\n",
      "epoch 36 | loss: 8.27571 |  0:03:45s\n",
      "epoch 37 | loss: 8.18054 |  0:03:50s\n",
      "epoch 38 | loss: 8.01613 |  0:03:56s\n",
      "epoch 39 | loss: 7.91089 |  0:04:02s\n",
      "epoch 40 | loss: 7.97376 |  0:04:09s\n",
      "epoch 41 | loss: 7.97633 |  0:04:15s\n",
      "epoch 42 | loss: 7.68956 |  0:04:21s\n",
      "epoch 43 | loss: 7.60355 |  0:04:27s\n",
      "epoch 44 | loss: 7.82156 |  0:04:33s\n",
      "epoch 45 | loss: 7.46528 |  0:04:39s\n",
      "epoch 46 | loss: 7.74356 |  0:04:45s\n",
      "epoch 47 | loss: 7.42139 |  0:04:51s\n",
      "epoch 48 | loss: 7.79459 |  0:04:57s\n",
      "epoch 49 | loss: 7.2656  |  0:05:03s\n",
      "epoch 50 | loss: 7.24789 |  0:05:10s\n",
      "epoch 51 | loss: 7.26655 |  0:05:16s\n",
      "epoch 52 | loss: 8.70258 |  0:05:22s\n",
      "epoch 53 | loss: 7.85057 |  0:05:28s\n",
      "epoch 54 | loss: 7.34442 |  0:05:34s\n",
      "epoch 55 | loss: 7.6874  |  0:05:39s\n",
      "epoch 56 | loss: 7.19735 |  0:05:46s\n",
      "epoch 57 | loss: 7.13565 |  0:05:52s\n",
      "epoch 58 | loss: 7.14601 |  0:05:58s\n",
      "epoch 59 | loss: 7.2048  |  0:06:04s\n",
      "epoch 60 | loss: 6.84191 |  0:06:10s\n",
      "epoch 61 | loss: 6.92902 |  0:06:16s\n",
      "epoch 62 | loss: 6.92411 |  0:06:22s\n",
      "epoch 63 | loss: 6.61138 |  0:06:28s\n",
      "epoch 64 | loss: 6.72242 |  0:06:34s\n",
      "epoch 65 | loss: 6.62136 |  0:06:40s\n",
      "epoch 66 | loss: 6.47916 |  0:06:46s\n",
      "epoch 67 | loss: 6.49869 |  0:06:53s\n",
      "epoch 68 | loss: 6.70607 |  0:06:58s\n",
      "epoch 69 | loss: 6.59871 |  0:07:04s\n",
      "epoch 70 | loss: 6.21728 |  0:07:10s\n",
      "epoch 71 | loss: 6.33472 |  0:07:17s\n",
      "epoch 72 | loss: 6.30871 |  0:07:23s\n",
      "epoch 73 | loss: 6.8368  |  0:07:29s\n",
      "epoch 74 | loss: 6.30074 |  0:07:35s\n",
      "epoch 75 | loss: 6.35598 |  0:07:41s\n",
      "epoch 76 | loss: 6.22726 |  0:07:47s\n",
      "epoch 77 | loss: 6.29276 |  0:07:53s\n",
      "epoch 78 | loss: 6.10002 |  0:07:59s\n",
      "epoch 79 | loss: 5.98351 |  0:08:05s\n",
      "epoch 80 | loss: 5.92327 |  0:08:11s\n",
      "epoch 81 | loss: 5.92234 |  0:08:18s\n",
      "epoch 82 | loss: 5.9048  |  0:08:24s\n",
      "epoch 83 | loss: 5.79982 |  0:08:30s\n",
      "epoch 84 | loss: 5.69256 |  0:08:35s\n",
      "epoch 85 | loss: 5.61675 |  0:08:42s\n",
      "epoch 86 | loss: 5.69465 |  0:08:47s\n",
      "epoch 87 | loss: 5.70536 |  0:08:54s\n",
      "epoch 88 | loss: 5.5627  |  0:09:00s\n",
      "epoch 89 | loss: 5.74756 |  0:09:06s\n",
      "epoch 90 | loss: 5.48428 |  0:09:12s\n",
      "epoch 91 | loss: 5.66512 |  0:09:18s\n",
      "epoch 92 | loss: 5.73729 |  0:09:24s\n",
      "epoch 93 | loss: 5.48452 |  0:09:30s\n",
      "epoch 94 | loss: 5.47934 |  0:09:36s\n",
      "epoch 95 | loss: 5.41699 |  0:09:42s\n",
      "epoch 96 | loss: 5.81649 |  0:09:48s\n",
      "epoch 97 | loss: 5.53292 |  0:09:54s\n",
      "epoch 98 | loss: 5.53031 |  0:10:01s\n",
      "epoch 99 | loss: 5.35234 |  0:10:07s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 7\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 175.06024|  0:00:06s\n",
      "epoch 1  | loss: 59.14259|  0:00:12s\n",
      "epoch 2  | loss: 45.60483|  0:00:18s\n",
      "epoch 3  | loss: 38.18067|  0:00:24s\n",
      "epoch 4  | loss: 25.73842|  0:00:30s\n",
      "epoch 5  | loss: 23.06533|  0:00:36s\n",
      "epoch 6  | loss: 18.50047|  0:00:42s\n",
      "epoch 7  | loss: 16.3701 |  0:00:48s\n",
      "epoch 8  | loss: 15.39783|  0:00:54s\n",
      "epoch 9  | loss: 14.96944|  0:01:00s\n",
      "epoch 10 | loss: 14.56397|  0:01:06s\n",
      "epoch 11 | loss: 13.23509|  0:01:12s\n",
      "epoch 12 | loss: 12.93709|  0:01:19s\n",
      "epoch 13 | loss: 12.74125|  0:01:25s\n",
      "epoch 14 | loss: 11.83229|  0:01:30s\n",
      "epoch 15 | loss: 11.47748|  0:01:37s\n",
      "epoch 16 | loss: 11.13571|  0:01:42s\n",
      "epoch 17 | loss: 10.95332|  0:01:49s\n",
      "epoch 18 | loss: 10.91302|  0:01:55s\n",
      "epoch 19 | loss: 11.08446|  0:02:01s\n",
      "epoch 20 | loss: 10.78942|  0:02:07s\n",
      "epoch 21 | loss: 10.44462|  0:02:13s\n",
      "epoch 22 | loss: 10.53504|  0:02:19s\n",
      "epoch 23 | loss: 10.66832|  0:02:25s\n",
      "epoch 24 | loss: 10.18909|  0:02:31s\n",
      "epoch 25 | loss: 10.06101|  0:02:37s\n",
      "epoch 26 | loss: 9.95227 |  0:02:43s\n",
      "epoch 27 | loss: 9.64185 |  0:02:50s\n",
      "epoch 28 | loss: 9.5011  |  0:02:56s\n",
      "epoch 29 | loss: 9.20413 |  0:03:02s\n",
      "epoch 30 | loss: 9.14179 |  0:03:08s\n",
      "epoch 31 | loss: 9.3046  |  0:03:14s\n",
      "epoch 32 | loss: 9.01387 |  0:03:20s\n",
      "epoch 33 | loss: 8.91253 |  0:03:26s\n",
      "epoch 34 | loss: 9.00535 |  0:03:32s\n",
      "epoch 35 | loss: 8.76765 |  0:03:38s\n",
      "epoch 36 | loss: 8.92989 |  0:03:44s\n",
      "epoch 37 | loss: 8.62262 |  0:03:51s\n",
      "epoch 38 | loss: 8.67705 |  0:03:57s\n",
      "epoch 39 | loss: 8.61673 |  0:04:03s\n",
      "epoch 40 | loss: 8.33238 |  0:04:09s\n",
      "epoch 41 | loss: 8.26895 |  0:04:14s\n",
      "epoch 42 | loss: 8.59233 |  0:04:21s\n",
      "epoch 43 | loss: 8.18489 |  0:04:27s\n",
      "epoch 44 | loss: 8.2639  |  0:04:33s\n",
      "epoch 45 | loss: 8.20814 |  0:04:39s\n",
      "epoch 46 | loss: 8.54377 |  0:04:45s\n",
      "epoch 47 | loss: 8.00946 |  0:04:51s\n",
      "epoch 48 | loss: 8.32244 |  0:04:57s\n",
      "epoch 49 | loss: 7.88118 |  0:05:03s\n",
      "epoch 50 | loss: 7.67737 |  0:05:08s\n",
      "epoch 51 | loss: 7.7748  |  0:05:14s\n",
      "epoch 52 | loss: 7.52628 |  0:05:20s\n",
      "epoch 53 | loss: 7.36873 |  0:05:27s\n",
      "epoch 54 | loss: 7.41419 |  0:05:33s\n",
      "epoch 55 | loss: 7.42718 |  0:05:39s\n",
      "epoch 56 | loss: 7.52464 |  0:05:45s\n",
      "epoch 57 | loss: 7.07394 |  0:05:51s\n",
      "epoch 58 | loss: 7.40172 |  0:05:58s\n",
      "epoch 59 | loss: 7.22503 |  0:06:04s\n",
      "epoch 60 | loss: 7.03064 |  0:06:10s\n",
      "epoch 61 | loss: 6.92276 |  0:06:16s\n",
      "epoch 62 | loss: 7.1674  |  0:06:22s\n",
      "epoch 63 | loss: 7.1413  |  0:06:28s\n",
      "epoch 64 | loss: 7.02226 |  0:06:35s\n",
      "epoch 65 | loss: 7.19424 |  0:06:41s\n",
      "epoch 66 | loss: 7.23404 |  0:06:47s\n",
      "epoch 67 | loss: 7.17609 |  0:06:53s\n",
      "epoch 68 | loss: 6.96862 |  0:06:59s\n",
      "epoch 69 | loss: 6.72279 |  0:07:05s\n",
      "epoch 70 | loss: 6.89917 |  0:07:11s\n",
      "epoch 71 | loss: 6.66727 |  0:07:17s\n",
      "epoch 72 | loss: 6.84606 |  0:07:23s\n",
      "epoch 73 | loss: 6.69429 |  0:07:30s\n",
      "epoch 74 | loss: 6.66587 |  0:07:36s\n",
      "epoch 75 | loss: 6.74946 |  0:07:42s\n",
      "epoch 76 | loss: 6.47665 |  0:07:48s\n",
      "epoch 77 | loss: 6.41625 |  0:07:54s\n",
      "epoch 78 | loss: 6.35589 |  0:08:00s\n",
      "epoch 79 | loss: 6.39348 |  0:08:06s\n",
      "epoch 80 | loss: 6.26472 |  0:08:12s\n",
      "epoch 81 | loss: 6.45888 |  0:08:18s\n",
      "epoch 82 | loss: 6.55467 |  0:08:24s\n",
      "epoch 83 | loss: 6.59844 |  0:08:30s\n",
      "epoch 84 | loss: 6.49214 |  0:08:36s\n",
      "epoch 85 | loss: 6.35925 |  0:08:42s\n",
      "epoch 86 | loss: 6.44654 |  0:08:48s\n",
      "epoch 87 | loss: 6.4386  |  0:08:54s\n",
      "epoch 88 | loss: 6.27901 |  0:09:00s\n",
      "epoch 89 | loss: 6.17684 |  0:09:07s\n",
      "epoch 90 | loss: 5.99407 |  0:09:13s\n",
      "epoch 91 | loss: 6.23778 |  0:09:19s\n",
      "epoch 92 | loss: 6.04951 |  0:09:25s\n",
      "epoch 93 | loss: 5.88265 |  0:09:31s\n",
      "epoch 94 | loss: 5.89286 |  0:09:37s\n",
      "epoch 95 | loss: 5.98761 |  0:09:43s\n",
      "epoch 96 | loss: 5.69364 |  0:09:49s\n",
      "epoch 97 | loss: 5.67814 |  0:09:55s\n",
      "epoch 98 | loss: 5.62516 |  0:10:01s\n",
      "epoch 99 | loss: 5.63755 |  0:10:07s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 8\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 173.25333|  0:00:05s\n",
      "epoch 1  | loss: 54.62258|  0:00:11s\n",
      "epoch 2  | loss: 50.04884|  0:00:17s\n",
      "epoch 3  | loss: 41.75919|  0:00:23s\n",
      "epoch 4  | loss: 27.34584|  0:00:30s\n",
      "epoch 5  | loss: 26.47696|  0:00:36s\n",
      "epoch 6  | loss: 24.12832|  0:00:42s\n",
      "epoch 7  | loss: 23.3881 |  0:00:48s\n",
      "epoch 8  | loss: 20.02292|  0:00:54s\n",
      "epoch 9  | loss: 19.4037 |  0:01:00s\n",
      "epoch 10 | loss: 20.68352|  0:01:07s\n",
      "epoch 11 | loss: 17.62099|  0:01:12s\n",
      "epoch 12 | loss: 16.52531|  0:01:19s\n",
      "epoch 13 | loss: 16.10772|  0:01:25s\n",
      "epoch 14 | loss: 16.78787|  0:01:31s\n",
      "epoch 15 | loss: 17.08025|  0:01:37s\n",
      "epoch 16 | loss: 18.75699|  0:01:43s\n",
      "epoch 17 | loss: 19.22579|  0:01:50s\n",
      "epoch 18 | loss: 14.98789|  0:01:56s\n",
      "epoch 19 | loss: 14.96105|  0:02:02s\n",
      "epoch 20 | loss: 13.59231|  0:02:08s\n",
      "epoch 21 | loss: 13.89765|  0:02:14s\n",
      "epoch 22 | loss: 12.99299|  0:02:20s\n",
      "epoch 23 | loss: 14.69492|  0:02:26s\n",
      "epoch 24 | loss: 13.70352|  0:02:33s\n",
      "epoch 25 | loss: 12.74414|  0:02:39s\n",
      "epoch 26 | loss: 12.23325|  0:02:45s\n",
      "epoch 27 | loss: 11.88021|  0:02:51s\n",
      "epoch 28 | loss: 11.79597|  0:02:57s\n",
      "epoch 29 | loss: 11.16174|  0:03:04s\n",
      "epoch 30 | loss: 10.99354|  0:03:10s\n",
      "epoch 31 | loss: 10.59352|  0:03:16s\n",
      "epoch 32 | loss: 10.38751|  0:03:22s\n",
      "epoch 33 | loss: 10.3218 |  0:03:28s\n",
      "epoch 34 | loss: 10.37231|  0:03:34s\n",
      "epoch 35 | loss: 10.31211|  0:03:40s\n",
      "epoch 36 | loss: 10.20964|  0:03:46s\n",
      "epoch 37 | loss: 9.71409 |  0:03:52s\n",
      "epoch 38 | loss: 9.47172 |  0:03:58s\n",
      "epoch 39 | loss: 9.51644 |  0:04:05s\n",
      "epoch 40 | loss: 9.48712 |  0:04:11s\n",
      "epoch 41 | loss: 9.14332 |  0:04:17s\n",
      "epoch 42 | loss: 9.07511 |  0:04:23s\n",
      "epoch 43 | loss: 8.97242 |  0:04:30s\n",
      "epoch 44 | loss: 9.19403 |  0:04:36s\n",
      "epoch 45 | loss: 8.82092 |  0:04:42s\n",
      "epoch 46 | loss: 8.87254 |  0:04:49s\n",
      "epoch 47 | loss: 8.86186 |  0:04:55s\n",
      "epoch 48 | loss: 8.43939 |  0:05:01s\n",
      "epoch 49 | loss: 8.3134  |  0:05:07s\n",
      "epoch 50 | loss: 8.40882 |  0:05:13s\n",
      "epoch 51 | loss: 8.25835 |  0:05:19s\n",
      "epoch 52 | loss: 8.15687 |  0:05:25s\n",
      "epoch 53 | loss: 8.11591 |  0:05:31s\n",
      "epoch 54 | loss: 8.11473 |  0:05:37s\n",
      "epoch 55 | loss: 7.71586 |  0:05:43s\n",
      "epoch 56 | loss: 7.667   |  0:05:49s\n",
      "epoch 57 | loss: 7.7562  |  0:05:54s\n",
      "epoch 58 | loss: 7.7767  |  0:06:00s\n",
      "epoch 59 | loss: 8.03545 |  0:06:06s\n",
      "epoch 60 | loss: 7.82823 |  0:06:12s\n",
      "epoch 61 | loss: 7.91381 |  0:06:18s\n",
      "epoch 62 | loss: 7.56004 |  0:06:24s\n",
      "epoch 63 | loss: 7.45713 |  0:06:30s\n",
      "epoch 64 | loss: 7.2439  |  0:06:36s\n",
      "epoch 65 | loss: 7.23085 |  0:06:41s\n",
      "epoch 66 | loss: 7.01971 |  0:06:47s\n",
      "epoch 67 | loss: 7.06292 |  0:06:53s\n",
      "epoch 68 | loss: 6.82181 |  0:06:59s\n",
      "epoch 69 | loss: 6.50229 |  0:07:04s\n",
      "epoch 70 | loss: 6.4712  |  0:07:10s\n",
      "epoch 71 | loss: 6.6552  |  0:07:17s\n",
      "epoch 72 | loss: 6.56993 |  0:07:22s\n",
      "epoch 73 | loss: 6.71304 |  0:07:28s\n",
      "epoch 74 | loss: 6.60839 |  0:07:34s\n",
      "epoch 75 | loss: 6.56933 |  0:07:39s\n",
      "epoch 76 | loss: 6.56386 |  0:07:46s\n",
      "epoch 77 | loss: 6.64374 |  0:07:51s\n",
      "epoch 78 | loss: 6.6008  |  0:07:57s\n",
      "epoch 79 | loss: 6.55621 |  0:08:03s\n",
      "epoch 80 | loss: 6.60795 |  0:08:08s\n",
      "epoch 81 | loss: 6.79902 |  0:08:14s\n",
      "epoch 82 | loss: 6.85145 |  0:08:20s\n",
      "epoch 83 | loss: 7.10718 |  0:08:26s\n",
      "epoch 84 | loss: 6.81788 |  0:08:32s\n",
      "epoch 85 | loss: 6.49375 |  0:08:38s\n",
      "epoch 86 | loss: 6.46172 |  0:08:44s\n",
      "epoch 87 | loss: 6.3745  |  0:08:50s\n",
      "epoch 88 | loss: 6.26768 |  0:08:56s\n",
      "epoch 89 | loss: 6.26039 |  0:09:02s\n",
      "epoch 90 | loss: 6.11451 |  0:09:08s\n",
      "epoch 91 | loss: 6.18037 |  0:09:13s\n",
      "epoch 92 | loss: 6.28076 |  0:09:20s\n",
      "epoch 93 | loss: 6.23518 |  0:09:25s\n",
      "epoch 94 | loss: 5.93604 |  0:09:31s\n",
      "epoch 95 | loss: 5.95575 |  0:09:37s\n",
      "epoch 96 | loss: 5.90305 |  0:09:43s\n",
      "epoch 97 | loss: 5.75454 |  0:09:48s\n",
      "epoch 98 | loss: 5.89891 |  0:09:55s\n",
      "epoch 99 | loss: 5.90676 |  0:10:01s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 9\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 166.97343|  0:00:05s\n",
      "epoch 1  | loss: 53.51365|  0:00:11s\n",
      "epoch 2  | loss: 48.14065|  0:00:18s\n",
      "epoch 3  | loss: 45.18373|  0:00:23s\n",
      "epoch 4  | loss: 28.25241|  0:00:29s\n",
      "epoch 5  | loss: 20.22558|  0:00:35s\n",
      "epoch 6  | loss: 17.69614|  0:00:41s\n",
      "epoch 7  | loss: 15.46486|  0:00:47s\n",
      "epoch 8  | loss: 14.50076|  0:00:52s\n",
      "epoch 9  | loss: 13.60959|  0:00:58s\n",
      "epoch 10 | loss: 12.45052|  0:01:04s\n",
      "epoch 11 | loss: 12.14819|  0:01:10s\n",
      "epoch 12 | loss: 11.9569 |  0:01:16s\n",
      "epoch 13 | loss: 11.1765 |  0:01:22s\n",
      "epoch 14 | loss: 11.27151|  0:01:28s\n",
      "epoch 15 | loss: 10.85159|  0:01:33s\n",
      "epoch 16 | loss: 11.15954|  0:01:39s\n",
      "epoch 17 | loss: 11.231  |  0:01:45s\n",
      "epoch 18 | loss: 10.72691|  0:01:51s\n",
      "epoch 19 | loss: 10.11669|  0:01:57s\n",
      "epoch 20 | loss: 10.12513|  0:02:02s\n",
      "epoch 21 | loss: 9.96215 |  0:02:08s\n",
      "epoch 22 | loss: 9.84329 |  0:02:14s\n",
      "epoch 23 | loss: 9.80804 |  0:02:21s\n",
      "epoch 24 | loss: 10.07655|  0:02:26s\n",
      "epoch 25 | loss: 9.62996 |  0:02:32s\n",
      "epoch 26 | loss: 9.41682 |  0:02:38s\n",
      "epoch 27 | loss: 9.22612 |  0:02:44s\n",
      "epoch 28 | loss: 9.59606 |  0:02:49s\n",
      "epoch 29 | loss: 9.11278 |  0:02:56s\n",
      "epoch 30 | loss: 8.95914 |  0:03:01s\n",
      "epoch 31 | loss: 8.81019 |  0:03:07s\n",
      "epoch 32 | loss: 8.78128 |  0:03:13s\n",
      "epoch 33 | loss: 8.62782 |  0:03:19s\n",
      "epoch 34 | loss: 8.35459 |  0:03:25s\n",
      "epoch 35 | loss: 8.72011 |  0:03:31s\n",
      "epoch 36 | loss: 8.51066 |  0:03:37s\n",
      "epoch 37 | loss: 8.6934  |  0:03:42s\n",
      "epoch 38 | loss: 8.87772 |  0:03:48s\n",
      "epoch 39 | loss: 8.54789 |  0:03:54s\n",
      "epoch 40 | loss: 8.42157 |  0:04:00s\n",
      "epoch 41 | loss: 8.03115 |  0:04:06s\n",
      "epoch 42 | loss: 8.11625 |  0:04:12s\n",
      "epoch 43 | loss: 7.69097 |  0:04:17s\n",
      "epoch 44 | loss: 7.60291 |  0:04:23s\n",
      "epoch 45 | loss: 7.83033 |  0:04:29s\n",
      "epoch 46 | loss: 7.68401 |  0:04:35s\n",
      "epoch 47 | loss: 7.63239 |  0:04:41s\n",
      "epoch 48 | loss: 7.4672  |  0:04:47s\n",
      "epoch 49 | loss: 7.18549 |  0:04:53s\n",
      "epoch 50 | loss: 7.18183 |  0:04:59s\n",
      "epoch 51 | loss: 7.18325 |  0:05:05s\n",
      "epoch 52 | loss: 7.08776 |  0:05:10s\n",
      "epoch 53 | loss: 7.15144 |  0:05:16s\n",
      "epoch 54 | loss: 6.96325 |  0:05:22s\n",
      "epoch 55 | loss: 6.87376 |  0:05:28s\n",
      "epoch 56 | loss: 7.1881  |  0:05:34s\n",
      "epoch 57 | loss: 6.69171 |  0:05:40s\n",
      "epoch 58 | loss: 6.905   |  0:05:45s\n",
      "epoch 59 | loss: 6.75856 |  0:05:51s\n",
      "epoch 60 | loss: 6.74789 |  0:05:57s\n",
      "epoch 61 | loss: 6.67604 |  0:06:03s\n",
      "epoch 62 | loss: 6.56063 |  0:06:08s\n",
      "epoch 63 | loss: 6.77359 |  0:06:14s\n",
      "epoch 64 | loss: 6.64062 |  0:06:20s\n",
      "epoch 65 | loss: 6.3395  |  0:06:26s\n",
      "epoch 66 | loss: 6.27797 |  0:06:32s\n",
      "epoch 67 | loss: 6.36589 |  0:06:38s\n",
      "epoch 68 | loss: 6.34552 |  0:06:44s\n",
      "epoch 69 | loss: 6.29461 |  0:06:50s\n",
      "epoch 70 | loss: 6.38642 |  0:06:56s\n",
      "epoch 71 | loss: 6.19723 |  0:07:02s\n",
      "epoch 72 | loss: 6.22304 |  0:07:08s\n",
      "epoch 73 | loss: 6.04191 |  0:07:14s\n",
      "epoch 74 | loss: 6.1592  |  0:07:20s\n",
      "epoch 75 | loss: 6.22945 |  0:07:25s\n",
      "epoch 76 | loss: 6.12036 |  0:07:31s\n",
      "epoch 77 | loss: 6.11808 |  0:07:37s\n",
      "epoch 78 | loss: 6.30921 |  0:07:43s\n",
      "epoch 79 | loss: 6.33839 |  0:07:49s\n",
      "epoch 80 | loss: 6.21395 |  0:07:55s\n",
      "epoch 81 | loss: 6.2545  |  0:08:01s\n",
      "epoch 82 | loss: 6.07159 |  0:08:07s\n",
      "epoch 83 | loss: 5.85607 |  0:08:13s\n",
      "epoch 84 | loss: 5.98948 |  0:08:18s\n",
      "epoch 85 | loss: 5.96547 |  0:08:24s\n",
      "epoch 86 | loss: 6.06979 |  0:08:30s\n",
      "epoch 87 | loss: 5.70832 |  0:08:36s\n",
      "epoch 88 | loss: 5.85917 |  0:08:42s\n",
      "epoch 89 | loss: 5.59405 |  0:08:48s\n",
      "epoch 90 | loss: 5.6255  |  0:08:54s\n",
      "epoch 91 | loss: 5.82019 |  0:09:00s\n",
      "epoch 92 | loss: 5.76623 |  0:09:06s\n",
      "epoch 93 | loss: 5.66705 |  0:09:12s\n",
      "epoch 94 | loss: 5.56774 |  0:09:18s\n",
      "epoch 95 | loss: 5.42343 |  0:09:24s\n",
      "epoch 96 | loss: 5.37187 |  0:09:30s\n",
      "epoch 97 | loss: 5.56516 |  0:09:35s\n",
      "epoch 98 | loss: 5.56431 |  0:09:42s\n",
      "epoch 99 | loss: 5.70971 |  0:09:47s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 10\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 166.09281|  0:00:05s\n",
      "epoch 1  | loss: 62.19358|  0:00:11s\n",
      "epoch 2  | loss: 70.73761|  0:00:17s\n",
      "epoch 3  | loss: 64.12415|  0:00:23s\n",
      "epoch 4  | loss: 57.13139|  0:00:29s\n",
      "epoch 5  | loss: 50.10267|  0:00:35s\n",
      "epoch 6  | loss: 31.39161|  0:00:42s\n",
      "epoch 7  | loss: 25.62897|  0:00:48s\n",
      "epoch 8  | loss: 27.87552|  0:00:54s\n",
      "epoch 9  | loss: 30.15709|  0:01:00s\n",
      "epoch 10 | loss: 30.26875|  0:01:06s\n",
      "epoch 11 | loss: 27.73031|  0:01:12s\n",
      "epoch 12 | loss: 23.02892|  0:01:18s\n",
      "epoch 13 | loss: 21.25149|  0:01:24s\n",
      "epoch 14 | loss: 28.21028|  0:01:30s\n",
      "epoch 15 | loss: 23.94212|  0:01:35s\n",
      "epoch 16 | loss: 24.28773|  0:01:41s\n",
      "epoch 17 | loss: 18.13912|  0:01:47s\n",
      "epoch 18 | loss: 15.4186 |  0:01:53s\n",
      "epoch 19 | loss: 14.27811|  0:01:59s\n",
      "epoch 20 | loss: 15.27939|  0:02:05s\n",
      "epoch 21 | loss: 13.72179|  0:02:11s\n",
      "epoch 22 | loss: 13.56334|  0:02:17s\n",
      "epoch 23 | loss: 12.59337|  0:02:23s\n",
      "epoch 24 | loss: 12.00428|  0:02:29s\n",
      "epoch 25 | loss: 12.14475|  0:02:35s\n",
      "epoch 26 | loss: 11.20175|  0:02:41s\n",
      "epoch 27 | loss: 10.97763|  0:02:47s\n",
      "epoch 28 | loss: 11.17402|  0:02:53s\n",
      "epoch 29 | loss: 10.65144|  0:02:59s\n",
      "epoch 30 | loss: 10.25532|  0:03:05s\n",
      "epoch 31 | loss: 10.00287|  0:03:11s\n",
      "epoch 32 | loss: 9.91228 |  0:03:17s\n",
      "epoch 33 | loss: 9.81948 |  0:03:22s\n",
      "epoch 34 | loss: 9.54232 |  0:03:29s\n",
      "epoch 35 | loss: 9.81207 |  0:03:35s\n",
      "epoch 36 | loss: 9.29459 |  0:03:41s\n",
      "epoch 37 | loss: 9.27044 |  0:03:46s\n",
      "epoch 38 | loss: 9.29006 |  0:03:52s\n",
      "epoch 39 | loss: 8.89728 |  0:03:58s\n",
      "epoch 40 | loss: 8.61932 |  0:04:04s\n",
      "epoch 41 | loss: 8.62374 |  0:04:10s\n",
      "epoch 42 | loss: 8.61047 |  0:04:16s\n",
      "epoch 43 | loss: 8.29981 |  0:04:22s\n",
      "epoch 44 | loss: 8.36495 |  0:04:28s\n",
      "epoch 45 | loss: 8.06365 |  0:04:34s\n",
      "epoch 46 | loss: 8.13227 |  0:04:40s\n",
      "epoch 47 | loss: 7.98929 |  0:04:46s\n",
      "epoch 48 | loss: 8.29593 |  0:04:52s\n",
      "epoch 49 | loss: 7.95659 |  0:04:58s\n",
      "epoch 50 | loss: 7.90265 |  0:05:04s\n",
      "epoch 51 | loss: 7.63083 |  0:05:10s\n",
      "epoch 52 | loss: 7.61433 |  0:05:16s\n",
      "epoch 53 | loss: 7.67438 |  0:05:21s\n",
      "epoch 54 | loss: 7.33114 |  0:05:27s\n",
      "epoch 55 | loss: 7.45613 |  0:05:33s\n",
      "epoch 56 | loss: 7.58706 |  0:05:39s\n",
      "epoch 57 | loss: 7.28169 |  0:05:45s\n",
      "epoch 58 | loss: 7.08356 |  0:05:51s\n",
      "epoch 59 | loss: 7.16624 |  0:05:57s\n",
      "epoch 60 | loss: 7.02431 |  0:06:03s\n",
      "epoch 61 | loss: 6.96281 |  0:06:09s\n",
      "epoch 62 | loss: 6.88664 |  0:06:14s\n",
      "epoch 63 | loss: 6.86863 |  0:06:20s\n",
      "epoch 64 | loss: 6.81293 |  0:06:26s\n",
      "epoch 65 | loss: 6.82179 |  0:06:32s\n",
      "epoch 66 | loss: 6.84816 |  0:06:38s\n",
      "epoch 67 | loss: 6.7024  |  0:06:44s\n",
      "epoch 68 | loss: 6.59261 |  0:06:49s\n",
      "epoch 69 | loss: 6.62774 |  0:06:55s\n",
      "epoch 70 | loss: 6.56192 |  0:07:01s\n",
      "epoch 71 | loss: 6.83796 |  0:07:07s\n",
      "epoch 72 | loss: 6.59966 |  0:07:13s\n",
      "epoch 73 | loss: 6.43344 |  0:07:19s\n",
      "epoch 74 | loss: 6.42325 |  0:07:25s\n",
      "epoch 75 | loss: 6.28255 |  0:07:31s\n",
      "epoch 76 | loss: 6.58038 |  0:07:37s\n",
      "epoch 77 | loss: 6.27323 |  0:07:43s\n",
      "epoch 78 | loss: 6.25939 |  0:07:48s\n",
      "epoch 79 | loss: 6.23829 |  0:07:54s\n",
      "epoch 80 | loss: 6.30994 |  0:08:00s\n",
      "epoch 81 | loss: 6.23037 |  0:08:05s\n",
      "epoch 82 | loss: 6.072   |  0:08:12s\n",
      "epoch 83 | loss: 5.8827  |  0:08:17s\n",
      "epoch 84 | loss: 6.10254 |  0:08:23s\n",
      "epoch 85 | loss: 6.12292 |  0:08:29s\n",
      "epoch 86 | loss: 6.0074  |  0:08:35s\n",
      "epoch 87 | loss: 6.08749 |  0:08:41s\n",
      "epoch 88 | loss: 5.87217 |  0:08:47s\n",
      "epoch 89 | loss: 5.90644 |  0:08:52s\n",
      "epoch 90 | loss: 5.99137 |  0:08:58s\n",
      "epoch 91 | loss: 5.88755 |  0:09:04s\n",
      "epoch 92 | loss: 6.00981 |  0:09:10s\n",
      "epoch 93 | loss: 5.97494 |  0:09:16s\n",
      "epoch 94 | loss: 5.82189 |  0:09:21s\n",
      "epoch 95 | loss: 5.86397 |  0:09:27s\n",
      "epoch 96 | loss: 5.73745 |  0:09:33s\n",
      "epoch 97 | loss: 5.78663 |  0:09:38s\n",
      "epoch 98 | loss: 5.6072  |  0:09:45s\n",
      "epoch 99 | loss: 5.75228 |  0:09:50s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 11\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 159.34956|  0:00:05s\n",
      "epoch 1  | loss: 52.40249|  0:00:11s\n",
      "epoch 2  | loss: 56.65104|  0:00:17s\n",
      "epoch 3  | loss: 49.45182|  0:00:23s\n",
      "epoch 4  | loss: 46.07979|  0:00:29s\n",
      "epoch 5  | loss: 28.19848|  0:00:34s\n",
      "epoch 6  | loss: 21.10132|  0:00:40s\n",
      "epoch 7  | loss: 18.42416|  0:00:46s\n",
      "epoch 8  | loss: 15.92263|  0:00:52s\n",
      "epoch 9  | loss: 15.49945|  0:00:58s\n",
      "epoch 10 | loss: 14.24403|  0:01:04s\n",
      "epoch 11 | loss: 13.1012 |  0:01:09s\n",
      "epoch 12 | loss: 12.53556|  0:01:15s\n",
      "epoch 13 | loss: 12.4577 |  0:01:21s\n",
      "epoch 14 | loss: 12.0068 |  0:01:27s\n",
      "epoch 15 | loss: 11.96397|  0:01:33s\n",
      "epoch 16 | loss: 11.22415|  0:01:38s\n",
      "epoch 17 | loss: 10.84426|  0:01:44s\n",
      "epoch 18 | loss: 10.41286|  0:01:50s\n",
      "epoch 19 | loss: 10.34392|  0:01:56s\n",
      "epoch 20 | loss: 10.0023 |  0:02:02s\n",
      "epoch 21 | loss: 9.74174 |  0:02:08s\n",
      "epoch 22 | loss: 9.72343 |  0:02:13s\n",
      "epoch 23 | loss: 9.60258 |  0:02:19s\n",
      "epoch 24 | loss: 9.78452 |  0:02:25s\n",
      "epoch 25 | loss: 9.03388 |  0:02:31s\n",
      "epoch 26 | loss: 8.99132 |  0:02:37s\n",
      "epoch 27 | loss: 8.90568 |  0:02:43s\n",
      "epoch 28 | loss: 9.16645 |  0:02:48s\n",
      "epoch 29 | loss: 8.88552 |  0:02:55s\n",
      "epoch 30 | loss: 8.80214 |  0:03:01s\n",
      "epoch 31 | loss: 8.56886 |  0:03:06s\n",
      "epoch 32 | loss: 8.35625 |  0:03:12s\n",
      "epoch 33 | loss: 8.50555 |  0:03:18s\n",
      "epoch 34 | loss: 8.14346 |  0:03:24s\n",
      "epoch 35 | loss: 8.33072 |  0:03:30s\n",
      "epoch 36 | loss: 7.96798 |  0:03:35s\n",
      "epoch 37 | loss: 7.90373 |  0:03:41s\n",
      "epoch 38 | loss: 7.77341 |  0:03:47s\n",
      "epoch 39 | loss: 7.82068 |  0:03:52s\n",
      "epoch 40 | loss: 7.49862 |  0:03:59s\n",
      "epoch 41 | loss: 7.54587 |  0:04:05s\n",
      "epoch 42 | loss: 7.5121  |  0:04:11s\n",
      "epoch 43 | loss: 7.40138 |  0:04:16s\n",
      "epoch 44 | loss: 7.34608 |  0:04:22s\n",
      "epoch 45 | loss: 7.27023 |  0:04:28s\n",
      "epoch 46 | loss: 6.94786 |  0:04:34s\n",
      "epoch 47 | loss: 7.03636 |  0:04:40s\n",
      "epoch 48 | loss: 6.84636 |  0:04:46s\n",
      "epoch 49 | loss: 6.83323 |  0:04:51s\n",
      "epoch 50 | loss: 6.9489  |  0:04:57s\n",
      "epoch 51 | loss: 6.76868 |  0:05:03s\n",
      "epoch 52 | loss: 6.83334 |  0:05:09s\n",
      "epoch 53 | loss: 6.76985 |  0:05:15s\n",
      "epoch 54 | loss: 6.58463 |  0:05:20s\n",
      "epoch 55 | loss: 6.54117 |  0:05:26s\n",
      "epoch 56 | loss: 7.07591 |  0:05:32s\n",
      "epoch 57 | loss: 6.62218 |  0:05:38s\n",
      "epoch 58 | loss: 6.5827  |  0:05:44s\n",
      "epoch 59 | loss: 6.38702 |  0:05:50s\n",
      "epoch 60 | loss: 6.40588 |  0:05:55s\n",
      "epoch 61 | loss: 6.38962 |  0:06:01s\n",
      "epoch 62 | loss: 6.31107 |  0:06:07s\n",
      "epoch 63 | loss: 6.21326 |  0:06:13s\n",
      "epoch 64 | loss: 6.30842 |  0:06:19s\n",
      "epoch 65 | loss: 6.05397 |  0:06:25s\n",
      "epoch 66 | loss: 5.91852 |  0:06:30s\n",
      "epoch 67 | loss: 6.00809 |  0:06:36s\n",
      "epoch 68 | loss: 6.06203 |  0:06:42s\n",
      "epoch 69 | loss: 5.9223  |  0:06:48s\n",
      "epoch 70 | loss: 5.8254  |  0:06:54s\n",
      "epoch 71 | loss: 5.94075 |  0:06:59s\n",
      "epoch 72 | loss: 6.01216 |  0:07:06s\n",
      "epoch 73 | loss: 5.85073 |  0:07:11s\n",
      "epoch 74 | loss: 5.62549 |  0:07:17s\n",
      "epoch 75 | loss: 5.68109 |  0:07:23s\n",
      "epoch 76 | loss: 5.79276 |  0:07:29s\n",
      "epoch 77 | loss: 5.65983 |  0:07:35s\n",
      "epoch 78 | loss: 5.6137  |  0:07:41s\n",
      "epoch 79 | loss: 5.60698 |  0:07:47s\n",
      "epoch 80 | loss: 5.4972  |  0:07:53s\n",
      "epoch 81 | loss: 5.57401 |  0:07:58s\n",
      "epoch 82 | loss: 5.54648 |  0:08:04s\n",
      "epoch 83 | loss: 5.46543 |  0:08:10s\n",
      "epoch 84 | loss: 5.45561 |  0:08:16s\n",
      "epoch 85 | loss: 5.33811 |  0:08:22s\n",
      "epoch 86 | loss: 5.41959 |  0:08:27s\n",
      "epoch 87 | loss: 5.37335 |  0:08:33s\n",
      "epoch 88 | loss: 5.50418 |  0:08:39s\n",
      "epoch 89 | loss: 5.4798  |  0:08:45s\n",
      "epoch 90 | loss: 5.31491 |  0:08:51s\n",
      "epoch 91 | loss: 5.17822 |  0:08:57s\n",
      "epoch 92 | loss: 5.3041  |  0:09:02s\n",
      "epoch 93 | loss: 5.18404 |  0:09:08s\n",
      "epoch 94 | loss: 5.13485 |  0:09:14s\n",
      "epoch 95 | loss: 5.31425 |  0:09:20s\n",
      "epoch 96 | loss: 5.22354 |  0:09:26s\n",
      "epoch 97 | loss: 5.17234 |  0:09:31s\n",
      "epoch 98 | loss: 5.10318 |  0:09:37s\n",
      "epoch 99 | loss: 5.21962 |  0:09:43s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 12\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 153.32543|  0:00:05s\n",
      "epoch 1  | loss: 51.6131 |  0:00:11s\n",
      "epoch 2  | loss: 47.68713|  0:00:17s\n",
      "epoch 3  | loss: 51.64427|  0:00:22s\n",
      "epoch 4  | loss: 58.64314|  0:00:28s\n",
      "epoch 5  | loss: 42.88591|  0:00:34s\n",
      "epoch 6  | loss: 26.51127|  0:00:40s\n",
      "epoch 7  | loss: 23.04429|  0:00:46s\n",
      "epoch 8  | loss: 19.97029|  0:00:51s\n",
      "epoch 9  | loss: 19.1572 |  0:00:58s\n",
      "epoch 10 | loss: 18.19298|  0:01:03s\n",
      "epoch 11 | loss: 15.99019|  0:01:09s\n",
      "epoch 12 | loss: 15.17232|  0:01:15s\n",
      "epoch 13 | loss: 13.50751|  0:01:21s\n",
      "epoch 14 | loss: 12.71089|  0:01:27s\n",
      "epoch 15 | loss: 12.2399 |  0:01:33s\n",
      "epoch 16 | loss: 12.74578|  0:01:39s\n",
      "epoch 17 | loss: 11.53334|  0:01:44s\n",
      "epoch 18 | loss: 11.38436|  0:01:50s\n",
      "epoch 19 | loss: 10.50662|  0:01:56s\n",
      "epoch 20 | loss: 10.31631|  0:02:02s\n",
      "epoch 21 | loss: 10.24589|  0:02:07s\n",
      "epoch 22 | loss: 10.21486|  0:02:13s\n",
      "epoch 23 | loss: 9.93588 |  0:02:19s\n",
      "epoch 24 | loss: 9.98058 |  0:02:25s\n",
      "epoch 25 | loss: 9.70031 |  0:02:31s\n",
      "epoch 26 | loss: 9.48173 |  0:02:37s\n",
      "epoch 27 | loss: 9.53345 |  0:02:42s\n",
      "epoch 28 | loss: 9.2268  |  0:02:48s\n",
      "epoch 29 | loss: 9.38833 |  0:02:54s\n",
      "epoch 30 | loss: 9.40149 |  0:03:00s\n",
      "epoch 31 | loss: 9.01726 |  0:03:06s\n",
      "epoch 32 | loss: 8.87273 |  0:03:12s\n",
      "epoch 33 | loss: 8.77485 |  0:03:18s\n",
      "epoch 34 | loss: 8.91846 |  0:03:23s\n",
      "epoch 35 | loss: 8.65663 |  0:03:29s\n",
      "epoch 36 | loss: 8.8877  |  0:03:35s\n",
      "epoch 37 | loss: 8.62446 |  0:03:41s\n",
      "epoch 38 | loss: 8.32279 |  0:03:47s\n",
      "epoch 39 | loss: 8.50616 |  0:03:52s\n",
      "epoch 40 | loss: 8.39442 |  0:03:58s\n",
      "epoch 41 | loss: 8.73512 |  0:04:04s\n",
      "epoch 42 | loss: 8.3661  |  0:04:10s\n",
      "epoch 43 | loss: 8.04928 |  0:04:16s\n",
      "epoch 44 | loss: 8.20506 |  0:04:22s\n",
      "epoch 45 | loss: 7.96735 |  0:04:27s\n",
      "epoch 46 | loss: 7.93    |  0:04:33s\n",
      "epoch 47 | loss: 7.77838 |  0:04:39s\n",
      "epoch 48 | loss: 7.48813 |  0:04:45s\n",
      "epoch 49 | loss: 7.87613 |  0:04:51s\n",
      "epoch 50 | loss: 7.69929 |  0:04:57s\n",
      "epoch 51 | loss: 7.58616 |  0:05:02s\n",
      "epoch 52 | loss: 7.36828 |  0:05:08s\n",
      "epoch 53 | loss: 7.39909 |  0:05:14s\n",
      "epoch 54 | loss: 7.18299 |  0:05:20s\n",
      "epoch 55 | loss: 7.0869  |  0:05:26s\n",
      "epoch 56 | loss: 7.15738 |  0:05:32s\n",
      "epoch 57 | loss: 7.04339 |  0:05:37s\n",
      "epoch 58 | loss: 7.05893 |  0:05:43s\n",
      "epoch 59 | loss: 7.06815 |  0:05:49s\n",
      "epoch 60 | loss: 7.07709 |  0:05:55s\n",
      "epoch 61 | loss: 6.90318 |  0:06:00s\n",
      "epoch 62 | loss: 6.74046 |  0:06:06s\n",
      "epoch 63 | loss: 6.97309 |  0:06:12s\n",
      "epoch 64 | loss: 7.07889 |  0:06:18s\n",
      "epoch 65 | loss: 6.90649 |  0:06:24s\n",
      "epoch 66 | loss: 6.71444 |  0:06:30s\n",
      "epoch 67 | loss: 6.79965 |  0:06:36s\n",
      "epoch 68 | loss: 6.76308 |  0:06:42s\n",
      "epoch 69 | loss: 6.4419  |  0:06:47s\n",
      "epoch 70 | loss: 6.60928 |  0:06:53s\n",
      "epoch 71 | loss: 6.73948 |  0:06:59s\n",
      "epoch 72 | loss: 6.51637 |  0:07:04s\n",
      "epoch 73 | loss: 6.29241 |  0:07:10s\n",
      "epoch 74 | loss: 6.32753 |  0:07:16s\n",
      "epoch 75 | loss: 6.50752 |  0:07:22s\n",
      "epoch 76 | loss: 6.31924 |  0:07:28s\n",
      "epoch 77 | loss: 6.35412 |  0:07:33s\n",
      "epoch 78 | loss: 6.32584 |  0:07:39s\n",
      "epoch 79 | loss: 6.10931 |  0:07:46s\n",
      "epoch 80 | loss: 6.31338 |  0:07:51s\n",
      "epoch 81 | loss: 6.12558 |  0:07:57s\n",
      "epoch 82 | loss: 6.05332 |  0:08:03s\n",
      "epoch 83 | loss: 6.12178 |  0:08:08s\n",
      "epoch 84 | loss: 6.39366 |  0:08:15s\n",
      "epoch 85 | loss: 6.19986 |  0:08:21s\n",
      "epoch 86 | loss: 6.06528 |  0:08:27s\n",
      "epoch 87 | loss: 6.05046 |  0:08:32s\n",
      "epoch 88 | loss: 6.23051 |  0:08:38s\n",
      "epoch 89 | loss: 5.87961 |  0:08:43s\n",
      "epoch 90 | loss: 5.97773 |  0:08:50s\n",
      "epoch 91 | loss: 5.87721 |  0:08:55s\n",
      "epoch 92 | loss: 5.91792 |  0:09:01s\n",
      "epoch 93 | loss: 5.85389 |  0:09:07s\n",
      "epoch 94 | loss: 5.70637 |  0:09:12s\n",
      "epoch 95 | loss: 5.67232 |  0:09:19s\n",
      "epoch 96 | loss: 5.90651 |  0:09:25s\n",
      "epoch 97 | loss: 5.96896 |  0:09:30s\n",
      "epoch 98 | loss: 6.03202 |  0:09:36s\n",
      "epoch 99 | loss: 5.9812  |  0:09:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 13\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 158.77225|  0:00:06s\n",
      "epoch 1  | loss: 45.91581|  0:00:12s\n",
      "epoch 2  | loss: 50.92583|  0:00:17s\n",
      "epoch 3  | loss: 47.01344|  0:00:23s\n",
      "epoch 4  | loss: 39.82056|  0:00:29s\n",
      "epoch 5  | loss: 39.63048|  0:00:35s\n",
      "epoch 6  | loss: 29.57563|  0:00:41s\n",
      "epoch 7  | loss: 24.45551|  0:00:46s\n",
      "epoch 8  | loss: 22.26411|  0:00:52s\n",
      "epoch 9  | loss: 25.23871|  0:00:58s\n",
      "epoch 10 | loss: 20.30001|  0:01:04s\n",
      "epoch 11 | loss: 18.97765|  0:01:10s\n",
      "epoch 12 | loss: 18.76426|  0:01:16s\n",
      "epoch 13 | loss: 17.87481|  0:01:21s\n",
      "epoch 14 | loss: 17.0977 |  0:01:27s\n",
      "epoch 15 | loss: 15.76246|  0:01:33s\n",
      "epoch 16 | loss: 15.1006 |  0:01:39s\n",
      "epoch 17 | loss: 13.7733 |  0:01:45s\n",
      "epoch 18 | loss: 12.73533|  0:01:51s\n",
      "epoch 19 | loss: 12.44132|  0:01:56s\n",
      "epoch 20 | loss: 11.60356|  0:02:02s\n",
      "epoch 21 | loss: 11.03598|  0:02:08s\n",
      "epoch 22 | loss: 11.01181|  0:02:14s\n",
      "epoch 23 | loss: 10.74828|  0:02:20s\n",
      "epoch 24 | loss: 10.47842|  0:02:25s\n",
      "epoch 25 | loss: 10.0369 |  0:02:32s\n",
      "epoch 26 | loss: 9.79736 |  0:02:37s\n",
      "epoch 27 | loss: 10.10528|  0:02:43s\n",
      "epoch 28 | loss: 9.85297 |  0:02:49s\n",
      "epoch 29 | loss: 9.55457 |  0:02:55s\n",
      "epoch 30 | loss: 9.55193 |  0:03:00s\n",
      "epoch 31 | loss: 9.50184 |  0:03:06s\n",
      "epoch 32 | loss: 9.08963 |  0:03:12s\n",
      "epoch 33 | loss: 9.19884 |  0:03:18s\n",
      "epoch 34 | loss: 8.77481 |  0:03:24s\n",
      "epoch 35 | loss: 9.02127 |  0:03:30s\n",
      "epoch 36 | loss: 8.64991 |  0:03:36s\n",
      "epoch 37 | loss: 8.67789 |  0:03:42s\n",
      "epoch 38 | loss: 8.43535 |  0:03:48s\n",
      "epoch 39 | loss: 8.50466 |  0:03:53s\n",
      "epoch 40 | loss: 8.20974 |  0:03:59s\n",
      "epoch 41 | loss: 8.21606 |  0:04:05s\n",
      "epoch 42 | loss: 8.12045 |  0:04:10s\n",
      "epoch 43 | loss: 8.11421 |  0:04:17s\n",
      "epoch 44 | loss: 7.72981 |  0:04:22s\n",
      "epoch 45 | loss: 7.65171 |  0:04:28s\n",
      "epoch 46 | loss: 7.53177 |  0:04:34s\n",
      "epoch 47 | loss: 8.10068 |  0:04:39s\n",
      "epoch 48 | loss: 8.04333 |  0:04:46s\n",
      "epoch 49 | loss: 8.2882  |  0:04:52s\n",
      "epoch 50 | loss: 7.96764 |  0:04:57s\n",
      "epoch 51 | loss: 7.27561 |  0:05:03s\n",
      "epoch 52 | loss: 7.22102 |  0:05:09s\n",
      "epoch 53 | loss: 7.46401 |  0:05:15s\n",
      "epoch 54 | loss: 7.48049 |  0:05:21s\n",
      "epoch 55 | loss: 7.39144 |  0:05:27s\n",
      "epoch 56 | loss: 7.21434 |  0:05:32s\n",
      "epoch 57 | loss: 7.11818 |  0:05:38s\n",
      "epoch 58 | loss: 7.08647 |  0:05:44s\n",
      "epoch 59 | loss: 7.0397  |  0:05:50s\n",
      "epoch 60 | loss: 6.68812 |  0:05:56s\n",
      "epoch 61 | loss: 6.76235 |  0:06:01s\n",
      "epoch 62 | loss: 6.53588 |  0:06:07s\n",
      "epoch 63 | loss: 6.58372 |  0:06:13s\n",
      "epoch 64 | loss: 6.4286  |  0:06:19s\n",
      "epoch 65 | loss: 6.5155  |  0:06:25s\n",
      "epoch 66 | loss: 6.35313 |  0:06:31s\n",
      "epoch 67 | loss: 6.3356  |  0:06:36s\n",
      "epoch 68 | loss: 6.27366 |  0:06:42s\n",
      "epoch 69 | loss: 6.20266 |  0:06:48s\n",
      "epoch 70 | loss: 6.1297  |  0:06:54s\n",
      "epoch 71 | loss: 6.35055 |  0:07:00s\n",
      "epoch 72 | loss: 6.37682 |  0:07:05s\n",
      "epoch 73 | loss: 6.09325 |  0:07:11s\n",
      "epoch 74 | loss: 5.99802 |  0:07:17s\n",
      "epoch 75 | loss: 6.06198 |  0:07:23s\n",
      "epoch 76 | loss: 6.07049 |  0:07:29s\n",
      "epoch 77 | loss: 6.14874 |  0:07:34s\n",
      "epoch 78 | loss: 5.79534 |  0:07:40s\n",
      "epoch 79 | loss: 6.01758 |  0:07:46s\n",
      "epoch 80 | loss: 5.92528 |  0:07:52s\n",
      "epoch 81 | loss: 6.00501 |  0:07:58s\n",
      "epoch 82 | loss: 5.76021 |  0:08:04s\n",
      "epoch 83 | loss: 5.78501 |  0:08:09s\n",
      "epoch 84 | loss: 6.19258 |  0:08:15s\n",
      "epoch 85 | loss: 5.64608 |  0:08:21s\n",
      "epoch 86 | loss: 5.83493 |  0:08:27s\n",
      "epoch 87 | loss: 5.77491 |  0:08:33s\n",
      "epoch 88 | loss: 5.63773 |  0:08:39s\n",
      "epoch 89 | loss: 5.51858 |  0:08:44s\n",
      "epoch 90 | loss: 5.58628 |  0:08:50s\n",
      "epoch 91 | loss: 5.4513  |  0:08:57s\n",
      "epoch 92 | loss: 5.58955 |  0:09:02s\n",
      "epoch 93 | loss: 5.30905 |  0:09:08s\n",
      "epoch 94 | loss: 5.57008 |  0:09:14s\n",
      "epoch 95 | loss: 5.35012 |  0:09:19s\n",
      "epoch 96 | loss: 5.42876 |  0:09:25s\n",
      "epoch 97 | loss: 5.23365 |  0:09:32s\n",
      "epoch 98 | loss: 5.38653 |  0:09:37s\n",
      "epoch 99 | loss: 5.45234 |  0:09:43s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 14\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 161.45791|  0:00:05s\n",
      "epoch 1  | loss: 65.02277|  0:00:12s\n",
      "epoch 2  | loss: 50.35265|  0:00:17s\n",
      "epoch 3  | loss: 43.97412|  0:00:23s\n",
      "epoch 4  | loss: 38.07441|  0:00:29s\n",
      "epoch 5  | loss: 30.18169|  0:00:34s\n",
      "epoch 6  | loss: 25.20919|  0:00:40s\n",
      "epoch 7  | loss: 22.52837|  0:00:46s\n",
      "epoch 8  | loss: 19.35051|  0:00:52s\n",
      "epoch 9  | loss: 16.90754|  0:00:58s\n",
      "epoch 10 | loss: 15.0406 |  0:01:03s\n",
      "epoch 11 | loss: 13.5456 |  0:01:09s\n",
      "epoch 12 | loss: 12.90413|  0:01:15s\n",
      "epoch 13 | loss: 12.10188|  0:01:21s\n",
      "epoch 14 | loss: 12.01004|  0:01:27s\n",
      "epoch 15 | loss: 11.83268|  0:01:32s\n",
      "epoch 16 | loss: 11.18894|  0:01:38s\n",
      "epoch 17 | loss: 10.75411|  0:01:44s\n",
      "epoch 18 | loss: 10.47456|  0:01:50s\n",
      "epoch 19 | loss: 10.07134|  0:01:56s\n",
      "epoch 20 | loss: 9.9237  |  0:02:02s\n",
      "epoch 21 | loss: 9.81378 |  0:02:07s\n",
      "epoch 22 | loss: 9.6753  |  0:02:14s\n",
      "epoch 23 | loss: 9.49935 |  0:02:19s\n",
      "epoch 24 | loss: 9.20795 |  0:02:25s\n",
      "epoch 25 | loss: 9.44625 |  0:02:31s\n",
      "epoch 26 | loss: 9.32576 |  0:02:37s\n",
      "epoch 27 | loss: 9.01004 |  0:02:42s\n",
      "epoch 28 | loss: 8.88205 |  0:02:49s\n",
      "epoch 29 | loss: 8.68762 |  0:02:54s\n",
      "epoch 30 | loss: 8.55736 |  0:03:00s\n",
      "epoch 31 | loss: 8.34866 |  0:03:06s\n",
      "epoch 32 | loss: 8.49707 |  0:03:12s\n",
      "epoch 33 | loss: 8.18647 |  0:03:18s\n",
      "epoch 34 | loss: 8.33024 |  0:03:23s\n",
      "epoch 35 | loss: 8.16813 |  0:03:29s\n",
      "epoch 36 | loss: 8.05231 |  0:03:35s\n",
      "epoch 37 | loss: 8.01569 |  0:03:41s\n",
      "epoch 38 | loss: 7.94825 |  0:03:46s\n",
      "epoch 39 | loss: 7.95341 |  0:03:53s\n",
      "epoch 40 | loss: 7.67645 |  0:03:58s\n",
      "epoch 41 | loss: 7.47386 |  0:04:04s\n",
      "epoch 42 | loss: 7.59529 |  0:04:10s\n",
      "epoch 43 | loss: 7.30603 |  0:04:16s\n",
      "epoch 44 | loss: 7.19311 |  0:04:22s\n",
      "epoch 45 | loss: 7.22886 |  0:04:28s\n",
      "epoch 46 | loss: 7.24354 |  0:04:33s\n",
      "epoch 47 | loss: 7.04451 |  0:04:39s\n",
      "epoch 48 | loss: 6.97699 |  0:04:45s\n",
      "epoch 49 | loss: 6.98446 |  0:04:51s\n",
      "epoch 50 | loss: 6.88052 |  0:04:57s\n",
      "epoch 51 | loss: 6.98456 |  0:05:02s\n",
      "epoch 52 | loss: 6.87701 |  0:05:08s\n",
      "epoch 53 | loss: 6.59224 |  0:05:14s\n",
      "epoch 54 | loss: 6.62428 |  0:05:19s\n",
      "epoch 55 | loss: 6.72805 |  0:05:26s\n",
      "epoch 56 | loss: 6.60108 |  0:05:31s\n",
      "epoch 57 | loss: 6.49889 |  0:05:37s\n",
      "epoch 58 | loss: 6.48224 |  0:05:43s\n",
      "epoch 59 | loss: 6.38849 |  0:05:48s\n",
      "epoch 60 | loss: 6.31483 |  0:05:55s\n",
      "epoch 61 | loss: 6.37905 |  0:06:01s\n",
      "epoch 62 | loss: 6.19026 |  0:06:06s\n",
      "epoch 63 | loss: 6.13345 |  0:06:12s\n",
      "epoch 64 | loss: 5.98627 |  0:06:18s\n",
      "epoch 65 | loss: 6.23687 |  0:06:24s\n",
      "epoch 66 | loss: 5.87552 |  0:06:30s\n",
      "epoch 67 | loss: 6.15805 |  0:06:35s\n",
      "epoch 68 | loss: 5.98644 |  0:06:41s\n",
      "epoch 69 | loss: 5.95667 |  0:06:47s\n",
      "epoch 70 | loss: 5.89138 |  0:06:53s\n",
      "epoch 71 | loss: 6.03816 |  0:07:00s\n",
      "epoch 72 | loss: 5.78824 |  0:07:05s\n",
      "epoch 73 | loss: 5.76065 |  0:07:11s\n",
      "epoch 74 | loss: 5.6812  |  0:07:17s\n",
      "epoch 75 | loss: 5.76941 |  0:07:22s\n",
      "epoch 76 | loss: 5.88148 |  0:07:28s\n",
      "epoch 77 | loss: 5.69488 |  0:07:34s\n",
      "epoch 78 | loss: 5.97239 |  0:07:40s\n",
      "epoch 79 | loss: 5.71219 |  0:07:46s\n",
      "epoch 80 | loss: 5.58402 |  0:07:51s\n",
      "epoch 81 | loss: 5.73104 |  0:07:57s\n",
      "epoch 82 | loss: 5.56651 |  0:08:03s\n",
      "epoch 83 | loss: 5.6479  |  0:08:09s\n",
      "epoch 84 | loss: 5.36289 |  0:08:14s\n",
      "epoch 85 | loss: 5.4835  |  0:08:20s\n",
      "epoch 86 | loss: 5.48119 |  0:08:26s\n",
      "epoch 87 | loss: 5.36412 |  0:08:32s\n",
      "epoch 88 | loss: 5.38247 |  0:08:38s\n",
      "epoch 89 | loss: 5.44398 |  0:08:43s\n",
      "epoch 90 | loss: 5.35755 |  0:08:49s\n",
      "epoch 91 | loss: 5.27829 |  0:08:55s\n",
      "epoch 92 | loss: 5.27783 |  0:09:01s\n",
      "epoch 93 | loss: 5.24283 |  0:09:07s\n",
      "epoch 94 | loss: 5.11153 |  0:09:13s\n",
      "epoch 95 | loss: 5.20264 |  0:09:18s\n",
      "epoch 96 | loss: 5.1149  |  0:09:24s\n",
      "epoch 97 | loss: 5.0202  |  0:09:30s\n",
      "epoch 98 | loss: 5.17326 |  0:09:36s\n",
      "epoch 99 | loss: 5.21413 |  0:09:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 15\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 178.49161|  0:00:05s\n",
      "epoch 1  | loss: 56.51623|  0:00:11s\n",
      "epoch 2  | loss: 58.87238|  0:00:17s\n",
      "epoch 3  | loss: 58.62665|  0:00:23s\n",
      "epoch 4  | loss: 55.6844 |  0:00:29s\n",
      "epoch 5  | loss: 42.98124|  0:00:34s\n",
      "epoch 6  | loss: 23.05368|  0:00:40s\n",
      "epoch 7  | loss: 18.90457|  0:00:46s\n",
      "epoch 8  | loss: 17.31298|  0:00:52s\n",
      "epoch 9  | loss: 15.81303|  0:00:58s\n",
      "epoch 10 | loss: 14.88314|  0:01:03s\n",
      "epoch 11 | loss: 13.37446|  0:01:09s\n",
      "epoch 12 | loss: 13.47243|  0:01:15s\n",
      "epoch 13 | loss: 12.64215|  0:01:21s\n",
      "epoch 14 | loss: 12.25779|  0:01:26s\n",
      "epoch 15 | loss: 12.11707|  0:01:32s\n",
      "epoch 16 | loss: 11.81472|  0:01:38s\n",
      "epoch 17 | loss: 12.05462|  0:01:44s\n",
      "epoch 18 | loss: 11.71629|  0:01:49s\n",
      "epoch 19 | loss: 11.34829|  0:01:55s\n",
      "epoch 20 | loss: 11.43674|  0:02:01s\n",
      "epoch 21 | loss: 11.37905|  0:02:07s\n",
      "epoch 22 | loss: 11.70874|  0:02:13s\n",
      "epoch 23 | loss: 10.80935|  0:02:18s\n",
      "epoch 24 | loss: 10.69781|  0:02:25s\n",
      "epoch 25 | loss: 10.75669|  0:02:30s\n",
      "epoch 26 | loss: 10.38878|  0:02:36s\n",
      "epoch 27 | loss: 10.17157|  0:02:42s\n",
      "epoch 28 | loss: 9.79293 |  0:02:47s\n",
      "epoch 29 | loss: 9.51063 |  0:02:53s\n",
      "epoch 30 | loss: 9.63841 |  0:02:59s\n",
      "epoch 31 | loss: 9.63544 |  0:03:05s\n",
      "epoch 32 | loss: 9.19239 |  0:03:11s\n",
      "epoch 33 | loss: 9.05301 |  0:03:17s\n",
      "epoch 34 | loss: 9.00832 |  0:03:22s\n",
      "epoch 35 | loss: 8.97344 |  0:03:29s\n",
      "epoch 36 | loss: 8.84132 |  0:03:34s\n",
      "epoch 37 | loss: 8.72163 |  0:03:40s\n",
      "epoch 38 | loss: 8.56122 |  0:03:46s\n",
      "epoch 39 | loss: 8.37364 |  0:03:52s\n",
      "epoch 40 | loss: 8.4994  |  0:03:58s\n",
      "epoch 41 | loss: 8.62379 |  0:04:03s\n",
      "epoch 42 | loss: 8.07365 |  0:04:09s\n",
      "epoch 43 | loss: 7.98564 |  0:04:15s\n",
      "epoch 44 | loss: 7.90932 |  0:04:21s\n",
      "epoch 45 | loss: 7.99265 |  0:04:26s\n",
      "epoch 46 | loss: 7.83037 |  0:04:33s\n",
      "epoch 47 | loss: 7.87216 |  0:04:38s\n",
      "epoch 48 | loss: 7.58043 |  0:04:44s\n",
      "epoch 49 | loss: 7.57909 |  0:04:50s\n",
      "epoch 50 | loss: 7.59634 |  0:04:55s\n",
      "epoch 51 | loss: 7.90459 |  0:05:02s\n",
      "epoch 52 | loss: 7.51154 |  0:05:07s\n",
      "epoch 53 | loss: 7.28101 |  0:05:13s\n",
      "epoch 54 | loss: 7.14343 |  0:05:19s\n",
      "epoch 55 | loss: 7.07171 |  0:05:24s\n",
      "epoch 56 | loss: 7.1004  |  0:05:31s\n",
      "epoch 57 | loss: 7.18993 |  0:05:36s\n",
      "epoch 58 | loss: 7.47513 |  0:05:42s\n",
      "epoch 59 | loss: 7.16508 |  0:05:48s\n",
      "epoch 60 | loss: 6.84753 |  0:05:53s\n",
      "epoch 61 | loss: 6.62724 |  0:05:59s\n",
      "epoch 62 | loss: 6.86689 |  0:06:05s\n",
      "epoch 63 | loss: 6.91898 |  0:06:11s\n",
      "epoch 64 | loss: 6.77524 |  0:06:17s\n",
      "epoch 65 | loss: 6.89125 |  0:06:22s\n",
      "epoch 66 | loss: 6.50755 |  0:06:28s\n",
      "epoch 67 | loss: 6.66838 |  0:06:34s\n",
      "epoch 68 | loss: 6.72816 |  0:06:40s\n",
      "epoch 69 | loss: 6.39385 |  0:06:46s\n",
      "epoch 70 | loss: 6.50112 |  0:06:51s\n",
      "epoch 71 | loss: 6.42505 |  0:06:57s\n",
      "epoch 72 | loss: 6.2685  |  0:07:03s\n",
      "epoch 73 | loss: 6.38428 |  0:07:09s\n",
      "epoch 74 | loss: 6.52279 |  0:07:15s\n",
      "epoch 75 | loss: 6.46683 |  0:07:20s\n",
      "epoch 76 | loss: 6.36366 |  0:07:26s\n",
      "epoch 77 | loss: 6.36805 |  0:07:32s\n",
      "epoch 78 | loss: 6.45279 |  0:07:38s\n",
      "epoch 79 | loss: 6.30236 |  0:07:44s\n",
      "epoch 80 | loss: 6.37574 |  0:07:49s\n",
      "epoch 81 | loss: 6.43719 |  0:07:55s\n",
      "epoch 82 | loss: 6.48311 |  0:08:01s\n",
      "epoch 83 | loss: 6.38643 |  0:08:07s\n",
      "epoch 84 | loss: 6.11321 |  0:08:13s\n",
      "epoch 85 | loss: 6.09505 |  0:08:18s\n",
      "epoch 86 | loss: 6.344   |  0:08:24s\n",
      "epoch 87 | loss: 6.47734 |  0:08:30s\n",
      "epoch 88 | loss: 6.29448 |  0:08:36s\n",
      "epoch 89 | loss: 6.374   |  0:08:42s\n",
      "epoch 90 | loss: 6.5002  |  0:08:48s\n",
      "epoch 91 | loss: 5.85097 |  0:08:53s\n",
      "epoch 92 | loss: 5.94034 |  0:08:59s\n",
      "epoch 93 | loss: 6.03895 |  0:09:05s\n",
      "epoch 94 | loss: 5.93018 |  0:09:11s\n",
      "epoch 95 | loss: 5.8721  |  0:09:17s\n",
      "epoch 96 | loss: 5.81758 |  0:09:23s\n",
      "epoch 97 | loss: 5.76416 |  0:09:28s\n",
      "epoch 98 | loss: 5.85949 |  0:09:34s\n",
      "epoch 99 | loss: 5.69902 |  0:09:40s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 16\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 161.31129|  0:00:05s\n",
      "epoch 1  | loss: 55.40113|  0:00:11s\n",
      "epoch 2  | loss: 47.39788|  0:00:17s\n",
      "epoch 3  | loss: 45.95607|  0:00:22s\n",
      "epoch 4  | loss: 41.01898|  0:00:29s\n",
      "epoch 5  | loss: 24.51948|  0:00:34s\n",
      "epoch 6  | loss: 17.47437|  0:00:40s\n",
      "epoch 7  | loss: 14.57192|  0:00:46s\n",
      "epoch 8  | loss: 13.43216|  0:00:52s\n",
      "epoch 9  | loss: 13.11074|  0:00:57s\n",
      "epoch 10 | loss: 12.55298|  0:01:04s\n",
      "epoch 11 | loss: 11.714  |  0:01:09s\n",
      "epoch 12 | loss: 11.76911|  0:01:15s\n",
      "epoch 13 | loss: 11.5087 |  0:01:21s\n",
      "epoch 14 | loss: 11.73956|  0:01:27s\n",
      "epoch 15 | loss: 11.21534|  0:01:33s\n",
      "epoch 16 | loss: 10.62396|  0:01:39s\n",
      "epoch 17 | loss: 10.32216|  0:01:44s\n",
      "epoch 18 | loss: 10.07262|  0:01:50s\n",
      "epoch 19 | loss: 9.98665 |  0:01:56s\n",
      "epoch 20 | loss: 9.48407 |  0:02:02s\n",
      "epoch 21 | loss: 9.47992 |  0:02:08s\n",
      "epoch 22 | loss: 9.70542 |  0:02:13s\n",
      "epoch 23 | loss: 9.39728 |  0:02:19s\n",
      "epoch 24 | loss: 9.13898 |  0:02:25s\n",
      "epoch 25 | loss: 9.57826 |  0:02:31s\n",
      "epoch 26 | loss: 9.00931 |  0:02:37s\n",
      "epoch 27 | loss: 9.31286 |  0:02:42s\n",
      "epoch 28 | loss: 8.5744  |  0:02:48s\n",
      "epoch 29 | loss: 8.62079 |  0:02:54s\n",
      "epoch 30 | loss: 8.54098 |  0:03:00s\n",
      "epoch 31 | loss: 8.53982 |  0:03:06s\n",
      "epoch 32 | loss: 8.49348 |  0:03:12s\n",
      "epoch 33 | loss: 8.18071 |  0:03:17s\n",
      "epoch 34 | loss: 8.50667 |  0:03:23s\n",
      "epoch 35 | loss: 8.24929 |  0:03:29s\n",
      "epoch 36 | loss: 8.09704 |  0:03:35s\n",
      "epoch 37 | loss: 7.8771  |  0:03:41s\n",
      "epoch 38 | loss: 7.89047 |  0:03:46s\n",
      "epoch 39 | loss: 7.80969 |  0:03:52s\n",
      "epoch 40 | loss: 7.85465 |  0:03:58s\n",
      "epoch 41 | loss: 7.60654 |  0:04:03s\n",
      "epoch 42 | loss: 7.39989 |  0:04:10s\n",
      "epoch 43 | loss: 7.45192 |  0:04:15s\n",
      "epoch 44 | loss: 7.26014 |  0:04:21s\n",
      "epoch 45 | loss: 7.4148  |  0:04:27s\n",
      "epoch 46 | loss: 7.18564 |  0:04:33s\n",
      "epoch 47 | loss: 7.12618 |  0:04:39s\n",
      "epoch 48 | loss: 7.05593 |  0:04:45s\n",
      "epoch 49 | loss: 6.9782  |  0:04:50s\n",
      "epoch 50 | loss: 7.23296 |  0:04:56s\n",
      "epoch 51 | loss: 7.20197 |  0:05:02s\n",
      "epoch 52 | loss: 6.9157  |  0:05:07s\n",
      "epoch 53 | loss: 6.86971 |  0:05:14s\n",
      "epoch 54 | loss: 6.75062 |  0:05:20s\n",
      "epoch 55 | loss: 6.85693 |  0:05:25s\n",
      "epoch 56 | loss: 6.87347 |  0:05:31s\n",
      "epoch 57 | loss: 6.81806 |  0:05:37s\n",
      "epoch 58 | loss: 6.81532 |  0:05:43s\n",
      "epoch 59 | loss: 6.97735 |  0:05:49s\n",
      "epoch 60 | loss: 6.58945 |  0:05:54s\n",
      "epoch 61 | loss: 6.51385 |  0:06:00s\n",
      "epoch 62 | loss: 6.37039 |  0:06:06s\n",
      "epoch 63 | loss: 6.26864 |  0:06:12s\n",
      "epoch 64 | loss: 6.44909 |  0:06:18s\n",
      "epoch 65 | loss: 6.40335 |  0:06:23s\n",
      "epoch 66 | loss: 6.25776 |  0:06:29s\n",
      "epoch 67 | loss: 6.11494 |  0:06:35s\n",
      "epoch 68 | loss: 6.12247 |  0:06:41s\n",
      "epoch 69 | loss: 6.14042 |  0:06:47s\n",
      "epoch 70 | loss: 5.9508  |  0:06:53s\n",
      "epoch 71 | loss: 6.01828 |  0:06:58s\n",
      "epoch 72 | loss: 6.06356 |  0:07:04s\n",
      "epoch 73 | loss: 6.03819 |  0:07:10s\n",
      "epoch 74 | loss: 5.8993  |  0:07:16s\n",
      "epoch 75 | loss: 5.80459 |  0:07:21s\n",
      "epoch 76 | loss: 5.86749 |  0:07:27s\n",
      "epoch 77 | loss: 5.70188 |  0:07:33s\n",
      "epoch 78 | loss: 5.5496  |  0:07:39s\n",
      "epoch 79 | loss: 5.58074 |  0:07:44s\n",
      "epoch 80 | loss: 5.59865 |  0:07:51s\n",
      "epoch 81 | loss: 5.4538  |  0:07:56s\n",
      "epoch 82 | loss: 5.77048 |  0:08:02s\n",
      "epoch 83 | loss: 5.30664 |  0:08:08s\n",
      "epoch 84 | loss: 5.53186 |  0:08:14s\n",
      "epoch 85 | loss: 5.54878 |  0:08:20s\n",
      "epoch 86 | loss: 5.55173 |  0:08:26s\n",
      "epoch 87 | loss: 5.39386 |  0:08:32s\n",
      "epoch 88 | loss: 5.62658 |  0:08:37s\n",
      "epoch 89 | loss: 5.29716 |  0:08:43s\n",
      "epoch 90 | loss: 5.41932 |  0:08:49s\n",
      "epoch 91 | loss: 5.26044 |  0:08:55s\n",
      "epoch 92 | loss: 5.41644 |  0:09:00s\n",
      "epoch 93 | loss: 5.49743 |  0:09:06s\n",
      "epoch 94 | loss: 5.27167 |  0:09:12s\n",
      "epoch 95 | loss: 5.15791 |  0:09:18s\n",
      "epoch 96 | loss: 5.40027 |  0:09:24s\n",
      "epoch 97 | loss: 5.11568 |  0:09:30s\n",
      "epoch 98 | loss: 5.33824 |  0:09:35s\n",
      "epoch 99 | loss: 5.30439 |  0:09:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 17\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 169.46079|  0:00:06s\n",
      "epoch 1  | loss: 50.39759|  0:00:11s\n",
      "epoch 2  | loss: 42.77008|  0:00:17s\n",
      "epoch 3  | loss: 44.79586|  0:00:23s\n",
      "epoch 4  | loss: 35.93852|  0:00:28s\n",
      "epoch 5  | loss: 31.94879|  0:00:34s\n",
      "epoch 6  | loss: 23.02367|  0:00:40s\n",
      "epoch 7  | loss: 17.23494|  0:00:46s\n",
      "epoch 8  | loss: 15.34003|  0:00:52s\n",
      "epoch 9  | loss: 14.3053 |  0:00:58s\n",
      "epoch 10 | loss: 13.49971|  0:01:03s\n",
      "epoch 11 | loss: 13.18831|  0:01:10s\n",
      "epoch 12 | loss: 12.68855|  0:01:15s\n",
      "epoch 13 | loss: 12.0029 |  0:01:21s\n",
      "epoch 14 | loss: 11.93382|  0:01:26s\n",
      "epoch 15 | loss: 11.61834|  0:01:32s\n",
      "epoch 16 | loss: 11.19638|  0:01:38s\n",
      "epoch 17 | loss: 10.74082|  0:01:44s\n",
      "epoch 18 | loss: 10.58771|  0:01:50s\n",
      "epoch 19 | loss: 10.55961|  0:01:55s\n",
      "epoch 20 | loss: 9.99132 |  0:02:01s\n",
      "epoch 21 | loss: 10.11291|  0:02:07s\n",
      "epoch 22 | loss: 9.52019 |  0:02:13s\n",
      "epoch 23 | loss: 9.74017 |  0:02:19s\n",
      "epoch 24 | loss: 9.32249 |  0:02:25s\n",
      "epoch 25 | loss: 9.42131 |  0:02:30s\n",
      "epoch 26 | loss: 9.01513 |  0:02:36s\n",
      "epoch 27 | loss: 8.85753 |  0:02:42s\n",
      "epoch 28 | loss: 8.65184 |  0:02:48s\n",
      "epoch 29 | loss: 9.00128 |  0:02:53s\n",
      "epoch 30 | loss: 8.83468 |  0:02:59s\n",
      "epoch 31 | loss: 8.41751 |  0:03:05s\n",
      "epoch 32 | loss: 8.43407 |  0:03:10s\n",
      "epoch 33 | loss: 8.13818 |  0:03:16s\n",
      "epoch 34 | loss: 8.24834 |  0:03:22s\n",
      "epoch 35 | loss: 8.09218 |  0:03:28s\n",
      "epoch 36 | loss: 7.88474 |  0:03:34s\n",
      "epoch 37 | loss: 7.96133 |  0:03:39s\n",
      "epoch 38 | loss: 7.77964 |  0:03:46s\n",
      "epoch 39 | loss: 7.48032 |  0:03:51s\n",
      "epoch 40 | loss: 7.6408  |  0:03:57s\n",
      "epoch 41 | loss: 7.39093 |  0:04:03s\n",
      "epoch 42 | loss: 7.20291 |  0:04:08s\n",
      "epoch 43 | loss: 7.20206 |  0:04:14s\n",
      "epoch 44 | loss: 7.13291 |  0:04:20s\n",
      "epoch 45 | loss: 7.10148 |  0:04:26s\n",
      "epoch 46 | loss: 7.14966 |  0:04:31s\n",
      "epoch 47 | loss: 7.04736 |  0:04:37s\n",
      "epoch 48 | loss: 7.18774 |  0:04:43s\n",
      "epoch 49 | loss: 6.79889 |  0:04:49s\n",
      "epoch 50 | loss: 6.86649 |  0:04:55s\n",
      "epoch 51 | loss: 6.85977 |  0:05:00s\n",
      "epoch 52 | loss: 6.83507 |  0:05:06s\n",
      "epoch 53 | loss: 6.7159  |  0:05:12s\n",
      "epoch 54 | loss: 6.70157 |  0:05:18s\n",
      "epoch 55 | loss: 6.90431 |  0:05:24s\n",
      "epoch 56 | loss: 6.65714 |  0:05:29s\n",
      "epoch 57 | loss: 6.57399 |  0:05:35s\n",
      "epoch 58 | loss: 6.58191 |  0:05:41s\n",
      "epoch 59 | loss: 6.65331 |  0:05:46s\n",
      "epoch 60 | loss: 6.46673 |  0:05:53s\n",
      "epoch 61 | loss: 6.06244 |  0:05:58s\n",
      "epoch 62 | loss: 6.32495 |  0:06:04s\n",
      "epoch 63 | loss: 6.0581  |  0:06:10s\n",
      "epoch 64 | loss: 6.26608 |  0:06:16s\n",
      "epoch 65 | loss: 6.30569 |  0:06:22s\n",
      "epoch 66 | loss: 6.55382 |  0:06:27s\n",
      "epoch 67 | loss: 6.36556 |  0:06:33s\n",
      "epoch 68 | loss: 6.34531 |  0:06:39s\n",
      "epoch 69 | loss: 6.3247  |  0:06:44s\n",
      "epoch 70 | loss: 6.06287 |  0:06:50s\n",
      "epoch 71 | loss: 6.36147 |  0:06:56s\n",
      "epoch 72 | loss: 5.83511 |  0:07:02s\n",
      "epoch 73 | loss: 5.89819 |  0:07:08s\n",
      "epoch 74 | loss: 5.78801 |  0:07:13s\n",
      "epoch 75 | loss: 5.78298 |  0:07:19s\n",
      "epoch 76 | loss: 5.60243 |  0:07:25s\n",
      "epoch 77 | loss: 5.92493 |  0:07:31s\n",
      "epoch 78 | loss: 5.59742 |  0:07:37s\n",
      "epoch 79 | loss: 5.57056 |  0:07:43s\n",
      "epoch 80 | loss: 5.47683 |  0:07:48s\n",
      "epoch 81 | loss: 5.42235 |  0:07:55s\n",
      "epoch 82 | loss: 5.55826 |  0:08:00s\n",
      "epoch 83 | loss: 5.41991 |  0:08:06s\n",
      "epoch 84 | loss: 5.54019 |  0:08:12s\n",
      "epoch 85 | loss: 5.38933 |  0:08:17s\n",
      "epoch 86 | loss: 5.37729 |  0:08:23s\n",
      "epoch 87 | loss: 5.3287  |  0:08:29s\n",
      "epoch 88 | loss: 5.38052 |  0:08:35s\n",
      "epoch 89 | loss: 5.38441 |  0:08:41s\n",
      "epoch 90 | loss: 5.36133 |  0:08:46s\n",
      "epoch 91 | loss: 5.128   |  0:08:52s\n",
      "epoch 92 | loss: 5.4364  |  0:08:58s\n",
      "epoch 93 | loss: 5.3609  |  0:09:04s\n",
      "epoch 94 | loss: 5.29697 |  0:09:10s\n",
      "epoch 95 | loss: 5.15928 |  0:09:16s\n",
      "epoch 96 | loss: 5.25913 |  0:09:21s\n",
      "epoch 97 | loss: 5.34035 |  0:09:27s\n",
      "epoch 98 | loss: 5.20202 |  0:09:33s\n",
      "epoch 99 | loss: 5.1051  |  0:09:39s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 18\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning:\n",
      "\n",
      "Device used : cuda\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning:\n",
      "\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 167.31278|  0:00:05s\n",
      "epoch 1  | loss: 59.87645|  0:00:11s\n",
      "epoch 2  | loss: 58.13297|  0:00:17s\n",
      "epoch 3  | loss: 58.43914|  0:00:22s\n",
      "epoch 4  | loss: 39.34564|  0:00:28s\n",
      "epoch 5  | loss: 24.10736|  0:00:34s\n",
      "epoch 6  | loss: 18.77135|  0:00:40s\n",
      "epoch 7  | loss: 16.86721|  0:00:46s\n",
      "epoch 8  | loss: 14.83415|  0:00:52s\n",
      "epoch 9  | loss: 14.34832|  0:00:57s\n",
      "epoch 10 | loss: 13.15623|  0:01:03s\n",
      "epoch 11 | loss: 12.52542|  0:01:09s\n",
      "epoch 12 | loss: 12.1796 |  0:01:15s\n",
      "epoch 13 | loss: 11.87858|  0:01:21s\n",
      "epoch 14 | loss: 11.481  |  0:01:27s\n",
      "epoch 15 | loss: 11.14743|  0:01:32s\n",
      "epoch 16 | loss: 10.9185 |  0:01:38s\n",
      "epoch 17 | loss: 10.7169 |  0:01:44s\n",
      "epoch 18 | loss: 10.29304|  0:01:50s\n",
      "epoch 19 | loss: 10.0611 |  0:01:56s\n",
      "epoch 20 | loss: 9.84489 |  0:02:02s\n",
      "epoch 21 | loss: 9.83032 |  0:02:07s\n",
      "epoch 22 | loss: 9.76993 |  0:02:13s\n",
      "epoch 23 | loss: 9.56082 |  0:02:19s\n",
      "epoch 24 | loss: 9.35826 |  0:02:25s\n",
      "epoch 25 | loss: 9.14902 |  0:02:31s\n",
      "epoch 26 | loss: 9.0242  |  0:02:36s\n",
      "epoch 27 | loss: 8.89973 |  0:02:42s\n",
      "epoch 28 | loss: 8.77732 |  0:02:48s\n",
      "epoch 29 | loss: 8.81774 |  0:02:54s\n",
      "epoch 30 | loss: 8.61177 |  0:03:00s\n",
      "epoch 31 | loss: 8.20248 |  0:03:06s\n",
      "epoch 32 | loss: 8.366   |  0:03:11s\n",
      "epoch 33 | loss: 8.17371 |  0:03:17s\n",
      "epoch 34 | loss: 8.0355  |  0:03:23s\n",
      "epoch 35 | loss: 7.98996 |  0:03:29s\n",
      "epoch 36 | loss: 7.99038 |  0:03:35s\n",
      "epoch 37 | loss: 7.77567 |  0:03:40s\n",
      "epoch 38 | loss: 8.17438 |  0:03:46s\n",
      "epoch 39 | loss: 7.85057 |  0:03:52s\n",
      "epoch 40 | loss: 7.68932 |  0:03:58s\n",
      "epoch 41 | loss: 7.62543 |  0:04:04s\n",
      "epoch 42 | loss: 7.50307 |  0:04:09s\n",
      "epoch 43 | loss: 7.36938 |  0:04:15s\n",
      "epoch 44 | loss: 7.36915 |  0:04:21s\n",
      "epoch 45 | loss: 7.03631 |  0:04:27s\n",
      "epoch 46 | loss: 7.04936 |  0:04:33s\n",
      "epoch 47 | loss: 6.8741  |  0:04:39s\n",
      "epoch 48 | loss: 6.85731 |  0:04:45s\n",
      "epoch 49 | loss: 6.71339 |  0:04:50s\n",
      "epoch 50 | loss: 6.77281 |  0:04:56s\n",
      "epoch 51 | loss: 6.93717 |  0:05:02s\n",
      "epoch 52 | loss: 6.59731 |  0:05:08s\n",
      "epoch 53 | loss: 6.70857 |  0:05:13s\n",
      "epoch 54 | loss: 6.71961 |  0:05:19s\n",
      "epoch 55 | loss: 6.43072 |  0:05:25s\n",
      "epoch 56 | loss: 6.53782 |  0:05:31s\n",
      "epoch 57 | loss: 6.50092 |  0:05:37s\n",
      "epoch 58 | loss: 6.51641 |  0:05:43s\n",
      "epoch 59 | loss: 6.34195 |  0:05:48s\n",
      "epoch 60 | loss: 6.40045 |  0:05:54s\n",
      "epoch 61 | loss: 6.24687 |  0:06:00s\n",
      "epoch 62 | loss: 6.25052 |  0:06:06s\n",
      "epoch 63 | loss: 6.35196 |  0:06:12s\n",
      "epoch 64 | loss: 6.22871 |  0:06:18s\n",
      "epoch 65 | loss: 6.37762 |  0:06:23s\n",
      "epoch 66 | loss: 6.3305  |  0:06:29s\n",
      "epoch 67 | loss: 6.09089 |  0:06:36s\n",
      "epoch 68 | loss: 6.02162 |  0:06:41s\n",
      "epoch 69 | loss: 6.01027 |  0:06:47s\n",
      "epoch 70 | loss: 5.96893 |  0:06:53s\n",
      "epoch 71 | loss: 5.8449  |  0:06:58s\n",
      "epoch 72 | loss: 5.70371 |  0:07:04s\n",
      "epoch 73 | loss: 5.65073 |  0:07:10s\n",
      "epoch 74 | loss: 5.8649  |  0:07:16s\n",
      "epoch 75 | loss: 5.54698 |  0:07:22s\n",
      "epoch 76 | loss: 5.76876 |  0:07:27s\n",
      "epoch 77 | loss: 5.58145 |  0:07:34s\n",
      "epoch 78 | loss: 5.83261 |  0:07:39s\n",
      "epoch 79 | loss: 5.51413 |  0:07:45s\n",
      "epoch 80 | loss: 5.65608 |  0:07:51s\n",
      "epoch 81 | loss: 5.41577 |  0:07:57s\n",
      "epoch 82 | loss: 5.43876 |  0:08:02s\n",
      "epoch 83 | loss: 5.532   |  0:08:08s\n",
      "epoch 84 | loss: 5.93196 |  0:08:14s\n",
      "epoch 85 | loss: 5.60111 |  0:08:20s\n",
      "epoch 86 | loss: 5.31231 |  0:08:26s\n",
      "epoch 87 | loss: 5.41088 |  0:08:31s\n",
      "epoch 88 | loss: 5.42874 |  0:08:38s\n",
      "epoch 89 | loss: 5.25796 |  0:08:43s\n",
      "epoch 90 | loss: 5.22684 |  0:08:50s\n",
      "epoch 91 | loss: 5.15952 |  0:08:55s\n",
      "epoch 92 | loss: 5.2995  |  0:09:01s\n",
      "epoch 93 | loss: 5.29975 |  0:09:07s\n",
      "epoch 94 | loss: 5.25442 |  0:09:13s\n",
      "epoch 95 | loss: 5.22624 |  0:09:19s\n",
      "epoch 96 | loss: 5.1132  |  0:09:25s\n",
      "epoch 97 | loss: 4.90301 |  0:09:30s\n",
      "epoch 98 | loss: 5.16875 |  0:09:36s\n",
      "epoch 99 | loss: 5.22558 |  0:09:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 19\n"
     ]
    }
   ],
   "source": [
    "use_df = pd.read_csv(\"../input/perov-scaled-data/scaled_trainable.csv\")\n",
    "tar_col = \"JV_default_PCE_numeric\"\n",
    "model_name = \"tabnet_reg\"\n",
    "fold_dict = joblib.load(\"../input/perov-fold-data/fold_data_export.z\")\n",
    "optim = \"adam\"\n",
    "k_folds = 20\n",
    "trial_fold = random.choice([x for x in range(k_folds)])\n",
    "num_trials = 15\n",
    "\n",
    "trial_data,best_params = train_trial(fold_dict = fold_dict,\n",
    "      fold = trial_fold,\n",
    "      k_folds=k_folds,\n",
    "      model_name=model_name,\n",
    "      sc_df=use_df,\n",
    "      tar_col=tar_col,\n",
    "      optim = optim,\n",
    "      optim_trial = num_trials)\n",
    "for key,value in trial_data.items():\n",
    "  print(f\"{key}: {value['rmse']}\")\n",
    "print(f\"[++] Ended the training process for fold {trial_fold}\")\n",
    "\n",
    "\n",
    "main_folds = [x for x in range(k_folds)]\n",
    "for fold in main_folds:\n",
    "    train(fold_dict = fold_dict,\n",
    "          fold = fold,\n",
    "          k_folds=k_folds,\n",
    "          model_name=model_name,\n",
    "          sc_df=use_df,\n",
    "          tar_col=tar_col,\n",
    "          optim = optim,\n",
    "          best_params = best_params)\n",
    "    print(f\"[++] Ended the training process for fold {fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86105012",
   "metadata": {
    "papermill": {
     "duration": 0.185059,
     "end_time": "2023-02-17T20:00:10.452828",
     "exception": false,
     "start_time": "2023-02-17T20:00:10.267769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18925.821158,
   "end_time": "2023-02-17T20:00:13.389403",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-17T14:44:47.568245",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
