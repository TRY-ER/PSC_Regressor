{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f5d70",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-01-01T09:03:44.812037Z",
     "iopub.status.busy": "2023-01-01T09:03:44.811328Z",
     "iopub.status.idle": "2023-01-01T09:03:57.133162Z",
     "shell.execute_reply": "2023-01-01T09:03:57.131818Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 12.329777,
     "end_time": "2023-01-01T09:03:57.136118",
     "exception": false,
     "start_time": "2023-01-01T09:03:44.806341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan  1 09:03:45 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (2.10.1)\r\n",
      "Requirement already satisfied: xgboost==1.6.1 in /opt/conda/lib/python3.7/site-packages (1.6.1)\r\n",
      "Collecting pytorch-tabnet\r\n",
      "  Downloading pytorch_tabnet-4.0-py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m311.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost==1.6.1) (1.21.6)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost==1.6.1) (1.7.3)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna) (6.6.0)\r\n",
      "Requirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna) (3.10.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (21.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.64.0)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.39)\r\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna) (0.8.2)\r\n",
      "Requirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna) (1.8.1)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna) (6.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.0.2)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.11.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (3.0.9)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (4.12.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (4.1.1)\r\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (5.8.0)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.2.1)\r\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.5.0)\r\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.3.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.9.0)\r\n",
      "Requirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (0.5.1)\r\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (2.4.2)\r\n",
      "Requirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\r\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna) (2.0.1)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "! pip install optuna xgboost==1.6.1 pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e051ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T09:03:57.148030Z",
     "iopub.status.busy": "2023-01-01T09:03:57.147609Z",
     "iopub.status.idle": "2023-01-01T09:04:00.342850Z",
     "shell.execute_reply": "2023-01-01T09:04:00.341871Z"
    },
    "papermill": {
     "duration": 3.204885,
     "end_time": "2023-01-01T09:04:00.345357",
     "exception": false,
     "start_time": "2023-01-01T09:03:57.140472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from xgboost import XGBRegressor\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,mean_absolute_error,r2_score\n",
    "import optuna as opt\n",
    "import joblib\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead2612d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T09:04:00.353982Z",
     "iopub.status.busy": "2023-01-01T09:04:00.353525Z",
     "iopub.status.idle": "2023-01-01T09:04:00.362149Z",
     "shell.execute_reply": "2023-01-01T09:04:00.361274Z"
    },
    "papermill": {
     "duration": 0.015201,
     "end_time": "2023-01-01T09:04:00.364203",
     "exception": false,
     "start_time": "2023-01-01T09:04:00.349002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_save_cv_model(i,model_name,model,optim,mse_loss,trial_data,output_path=\"./\"):\n",
    "\n",
    "    ''' This function saves cross validation model in the corresponding directory ( if the path does not exist it creates the path for it'''\n",
    "\n",
    "\n",
    "    if os.path.exists(os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}\")):\n",
    "        joblib.dump(model, os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/{i}_model.z\"))\n",
    "        with open(os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/mse_loss.txt\"),\"w+\") as file:file.write(f\"{mse_loss}\")\n",
    "        joblib.dump(trial_data, os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/{i}_trial_data.z\"))\n",
    "    else:\n",
    "        os.mkdir(os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}\"))\n",
    "        joblib.dump(model, os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/{i}_model.z\"))\n",
    "        with open(os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/mse_loss.txt\"),\"w+\") as file:file.write(f\"{mse_loss}\")\n",
    "        joblib.dump(trial_data, os.path.join(output_path,f\"trial_{i}_{model_name}_{optim}/{i}_trial_data.z\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30692503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T09:04:00.371999Z",
     "iopub.status.busy": "2023-01-01T09:04:00.371709Z",
     "iopub.status.idle": "2023-01-01T09:04:00.378500Z",
     "shell.execute_reply": "2023-01-01T09:04:00.377474Z"
    },
    "papermill": {
     "duration": 0.013059,
     "end_time": "2023-01-01T09:04:00.380573",
     "exception": false,
     "start_time": "2023-01-01T09:04:00.367514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_cv_model(i,model_name,model,optim,mse_loss,output_path=\"./\"):\n",
    "\n",
    "    ''' This function saves cross validation model in the corresponding directory ( if the path does not exist it creates the path for it'''\n",
    "\n",
    "\n",
    "    if os.path.exists(os.path.join(output_path,f\"{i}_{model_name}_{optim}\")):\n",
    "        joblib.dump(model, os.path.join(output_path,f\"{i}_{model_name}_{optim}/{i}_model.z\"))\n",
    "        with open(os.path.join(output_path,f\"{i}_{model_name}_{optim}/losses_{fold}.txt\"),\"w+\") as file:file.write(f\"{mse_loss}\")\n",
    "    else:\n",
    "        os.mkdir(os.path.join(output_path,f\"{i}_{model_name}_{optim}\"))\n",
    "        joblib.dump(model, os.path.join(output_path,f\"{i}_{model_name}_{optim}/{i}_model.z\"))\n",
    "        with open(os.path.join(output_path,f\"{i}_{model_name}_{optim}/losses_{fold}.txt\"),\"w+\") as file:file.write(f\"{mse_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e029d7e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T09:04:00.388312Z",
     "iopub.status.busy": "2023-01-01T09:04:00.388047Z",
     "iopub.status.idle": "2023-01-01T09:04:00.393422Z",
     "shell.execute_reply": "2023-01-01T09:04:00.392384Z"
    },
    "papermill": {
     "duration": 0.011884,
     "end_time": "2023-01-01T09:04:00.395715",
     "exception": false,
     "start_time": "2023-01-01T09:04:00.383831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trial_data(trial) -> list:\n",
    "  ''' This function takes the trial objects and returns the dictionary containing the trial details for plotting and comparing purposes '''\n",
    "  trial_data = trial.get_trials()\n",
    "  value_dict = {}\n",
    "  for i in trial_data:\n",
    "    print(i.params)\n",
    "    value_dict[i.number] = {\"params\": i.params , \"rmse\": i.values}\n",
    "    print(f\"{i.number} : {i.values}\")\n",
    "  return value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd09565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T09:04:00.403527Z",
     "iopub.status.busy": "2023-01-01T09:04:00.403262Z",
     "iopub.status.idle": "2023-01-01T09:04:00.419258Z",
     "shell.execute_reply": "2023-01-01T09:04:00.418237Z"
    },
    "papermill": {
     "duration": 0.022098,
     "end_time": "2023-01-01T09:04:00.421168",
     "exception": false,
     "start_time": "2023-01-01T09:04:00.399070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_trial(fold_dict,fold,model_name,sc_df,tar_col,optim,optim_trial,k_folds,tar_cols=\"\",verbose=1):\n",
    "\n",
    "    ''' this function is used to train the model with parameters optimization using optuna and cross validation using stratified k_folds'''\n",
    "\n",
    "    y = sc_df[tar_col]\n",
    "    print(y.shape)\n",
    "    x = sc_df.drop([tar_col],axis=1)\n",
    "    print(x.shape)\n",
    "    model_name = model_name \n",
    "    def objective(trial):\n",
    "      train_index = fold_dict[fold][\"train\"]\n",
    "      test_index = fold_dict[fold][\"test\"]\n",
    "      clf = TabNetRegressor(n_d=trial.suggest_int(\"n_d\", 8, 64),\n",
    "                                    n_a =trial.suggest_int(\"n_a\", 8, 64),\n",
    "                                    n_steps = trial.suggest_int(\"n_steps\",3,10),\n",
    "                                    gamma =trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "                                    n_independent = trial.suggest_int(\"n_independent\",1,5),\n",
    "                                    n_shared = trial.suggest_int(\"n_shared\",1,5),\n",
    "                                    momentum = trial.suggest_float(\"momentum\", 0.01, 0.4),\n",
    "                                    optimizer_fn = torch.optim.Adam,\n",
    "                                    # scheduler_fn = torch.optim.lr_scheduler,\n",
    "                                    # scheduler_params = {\"gamma\" :trial.suggest_float(\"sch-gamma\", 0.5, 0.95), \"step_size\": trial.suggest_int(\"sch_step_size\", 10, 20, 2)},\n",
    "                                    verbose = verbose,\n",
    "                                    device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                                    )\n",
    "      # print(f\" train_index :: {train_index}\")\n",
    "      # print(f\" test_index :: {test_index}\")\n",
    "      X_train,X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "      # print(X_train.shape, X_test.shape)\n",
    "      X_train, X_test = X_train.to_numpy(dtype=np.float64), X_test.to_numpy(dtype=np.float64)\n",
    "      Y_train, Y_test = y.iloc[train_index].to_numpy(dtype=np.float64), y.iloc[test_index].to_numpy(np.float64)\n",
    "      # Y_train, Y_test = Y_train.to_numpy(dtype=np.float64), Y_test.to_numpy(dtype=np.float64)\n",
    "      print(X_train.shape)\n",
    "      print(Y_train.shape)\n",
    "      print(X_test.shape)\n",
    "      print(Y_test.shape)\n",
    "      Y_train = Y_train.reshape(-1,1)\n",
    "      clf.fit(X_train, Y_train)\n",
    "      Y_pred = clf.predict(X_test)\n",
    "      mse_error = mean_squared_error(Y_pred, Y_test, squared=False) \n",
    "      return mse_error\n",
    "\n",
    "    print(f\"Starting optimization for fold : [{fold}/{k_folds}]\")\n",
    "    study = opt.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=optim_trial)\n",
    "    best_params = study.best_params\n",
    "    trial_data = get_trial_data(study)\n",
    "    print(f\" Best params for fold : [{fold}/{k_folds}]\")\n",
    "    print(best_params)\n",
    "    train_index = fold_dict[fold][\"train\"]\n",
    "    test_index = fold_dict[fold][\"test\"]\n",
    "    X_train,X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "    # print(X_train.shape, X_test.shape)\n",
    "    X_train, X_test = X_train.to_numpy(dtype=np.float64), X_test.to_numpy(dtype=np.float64)\n",
    "    Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    Y_train, Y_test = Y_train.to_numpy(dtype=np.float64), Y_test.to_numpy(dtype=np.float64)\n",
    "    clf_model = TabNetRegressor(**study.best_params,device_name= \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    clf_model.fit(X_train,Y_train)\n",
    "    Y_pred = clf_model.predict(X_test)\n",
    "    error = {\"mse_error\" : mean_squared_error(Y_pred, Y_test, squared=False),\n",
    "    \"mae_error\" : mean_absolute_error(Y_pred,Y_test),\n",
    "    \"rmse_error\" : mean_squared_error(Y_pred, Y_test),\n",
    "    \"r2_score\" : r2_score(Y_pred,Y_test) }\n",
    "    # try:\n",
    "    print(\"[++] Saving the model and parameters in corresponding directories\")\n",
    "    make_save_cv_model(fold,model_name,clf_model,optim,mse_loss=error,trial_data=trial_data)\n",
    "    return trial_data,best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ed05e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T09:04:00.428831Z",
     "iopub.status.busy": "2023-01-01T09:04:00.428548Z",
     "iopub.status.idle": "2023-01-01T09:04:00.437602Z",
     "shell.execute_reply": "2023-01-01T09:04:00.436760Z"
    },
    "papermill": {
     "duration": 0.015106,
     "end_time": "2023-01-01T09:04:00.439613",
     "exception": false,
     "start_time": "2023-01-01T09:04:00.424507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(fold_dict,fold,model_name,sc_df,tar_col,optim,k_folds,best_params,tar_cols=\"\",verbose=1):\n",
    "\n",
    "    ''' this function is used to train the model with parameters optimization using optuna and cross validation using stratified k_folds'''\n",
    "\n",
    "    y = sc_df[tar_col]\n",
    "    print(y.shape)\n",
    "    x = sc_df.drop([tar_col],axis=1)\n",
    "    print(x.shape)\n",
    "    model_name = model_name \n",
    "    train_index = fold_dict[fold][\"train\"]\n",
    "    test_index = fold_dict[fold][\"test\"]\n",
    "    X_train,X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "    # print(X_train.shape, X_test.shape)\n",
    "    X_train, X_test = X_train.to_numpy(dtype=np.float64), X_test.to_numpy(dtype=np.float64)\n",
    "    Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    Y_train, Y_test = Y_train.to_numpy(dtype=np.float64), Y_test.to_numpy(dtype=np.float64)\n",
    "    clf_model = TabNetRegressor(**best_params,device_name= \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    clf_model.fit(X_train,Y_train)\n",
    "    Y_pred = clf_model.predict(X_test)\n",
    "    error = {\n",
    "    \"mse_error\" : mean_squared_error(Y_pred, Y_test, squared=False),\n",
    "    \"mae_error\" : mean_absolute_error(Y_pred,Y_test),\n",
    "    \"rmse_error\" : mean_squared_error(Y_pred, Y_test),\n",
    "    \"r2_score\" : r2_score(Y_pred,Y_test) }\n",
    "    # try:\n",
    "    print(\"[++] Saving the model and parameters in corresponding directories\")\n",
    "    save_cv_model(fold,model_name,clf_model,optim,mse_loss=error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15cfe562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-01T09:04:00.447141Z",
     "iopub.status.busy": "2023-01-01T09:04:00.446854Z",
     "iopub.status.idle": "2023-01-01T12:12:37.744430Z",
     "shell.execute_reply": "2023-01-01T12:12:37.743198Z"
    },
    "papermill": {
     "duration": 11317.303809,
     "end_time": "2023-01-01T12:12:37.746673",
     "exception": false,
     "start_time": "2023-01-01T09:04:00.442864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:04:02,309]\u001b[0m A new study created in memory with name: no-name-76564aa2-c7c1-48d9-9e7e-d0eb358b4eb4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46820,)\n",
      "(46820, 103)\n",
      "Starting optimization for fold : [0/20]\n",
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 216.60689|  0:00:04s\n",
      "epoch 1  | loss: 47.67343|  0:00:07s\n",
      "epoch 2  | loss: 39.05951|  0:00:11s\n",
      "epoch 3  | loss: 37.10037|  0:00:14s\n",
      "epoch 4  | loss: 33.52174|  0:00:18s\n",
      "epoch 5  | loss: 26.51078|  0:00:21s\n",
      "epoch 6  | loss: 22.62607|  0:00:24s\n",
      "epoch 7  | loss: 30.99096|  0:00:27s\n",
      "epoch 8  | loss: 23.31654|  0:00:31s\n",
      "epoch 9  | loss: 19.98966|  0:00:34s\n",
      "epoch 10 | loss: 18.22681|  0:00:37s\n",
      "epoch 11 | loss: 16.53621|  0:00:41s\n",
      "epoch 12 | loss: 16.12404|  0:00:44s\n",
      "epoch 13 | loss: 15.93442|  0:00:48s\n",
      "epoch 14 | loss: 16.92738|  0:00:51s\n",
      "epoch 15 | loss: 16.50148|  0:00:54s\n",
      "epoch 16 | loss: 15.27087|  0:00:57s\n",
      "epoch 17 | loss: 15.48071|  0:01:01s\n",
      "epoch 18 | loss: 14.67863|  0:01:04s\n",
      "epoch 19 | loss: 14.2742 |  0:01:08s\n",
      "epoch 20 | loss: 13.49315|  0:01:12s\n",
      "epoch 21 | loss: 12.84221|  0:01:15s\n",
      "epoch 22 | loss: 12.57966|  0:01:18s\n",
      "epoch 23 | loss: 12.63678|  0:01:21s\n",
      "epoch 24 | loss: 12.08361|  0:01:25s\n",
      "epoch 25 | loss: 11.8734 |  0:01:28s\n",
      "epoch 26 | loss: 11.84081|  0:01:31s\n",
      "epoch 27 | loss: 11.55839|  0:01:35s\n",
      "epoch 28 | loss: 11.50553|  0:01:38s\n",
      "epoch 29 | loss: 11.08579|  0:01:41s\n",
      "epoch 30 | loss: 11.43903|  0:01:45s\n",
      "epoch 31 | loss: 10.9881 |  0:01:49s\n",
      "epoch 32 | loss: 10.61587|  0:01:52s\n",
      "epoch 33 | loss: 10.53715|  0:01:55s\n",
      "epoch 34 | loss: 10.21883|  0:01:59s\n",
      "epoch 35 | loss: 10.29224|  0:02:02s\n",
      "epoch 36 | loss: 10.11652|  0:02:06s\n",
      "epoch 37 | loss: 9.88855 |  0:02:09s\n",
      "epoch 38 | loss: 9.74442 |  0:02:12s\n",
      "epoch 39 | loss: 9.94607 |  0:02:16s\n",
      "epoch 40 | loss: 9.44489 |  0:02:19s\n",
      "epoch 41 | loss: 9.46726 |  0:02:23s\n",
      "epoch 42 | loss: 9.36101 |  0:02:26s\n",
      "epoch 43 | loss: 9.2352  |  0:02:30s\n",
      "epoch 44 | loss: 9.08968 |  0:02:33s\n",
      "epoch 45 | loss: 9.23557 |  0:02:36s\n",
      "epoch 46 | loss: 9.02735 |  0:02:40s\n",
      "epoch 47 | loss: 8.80069 |  0:02:43s\n",
      "epoch 48 | loss: 8.87554 |  0:02:47s\n",
      "epoch 49 | loss: 8.8965  |  0:02:50s\n",
      "epoch 50 | loss: 8.68576 |  0:02:54s\n",
      "epoch 51 | loss: 8.50792 |  0:02:57s\n",
      "epoch 52 | loss: 8.44638 |  0:03:01s\n",
      "epoch 53 | loss: 8.51211 |  0:03:04s\n",
      "epoch 54 | loss: 8.50088 |  0:03:07s\n",
      "epoch 55 | loss: 8.22928 |  0:03:10s\n",
      "epoch 56 | loss: 8.42284 |  0:03:14s\n",
      "epoch 57 | loss: 8.36986 |  0:03:18s\n",
      "epoch 58 | loss: 8.27109 |  0:03:21s\n",
      "epoch 59 | loss: 8.17928 |  0:03:24s\n",
      "epoch 60 | loss: 8.28507 |  0:03:27s\n",
      "epoch 61 | loss: 8.27598 |  0:03:31s\n",
      "epoch 62 | loss: 8.10569 |  0:03:34s\n",
      "epoch 63 | loss: 7.93287 |  0:03:37s\n",
      "epoch 64 | loss: 7.91192 |  0:03:41s\n",
      "epoch 65 | loss: 7.94964 |  0:03:44s\n",
      "epoch 66 | loss: 8.03322 |  0:03:48s\n",
      "epoch 67 | loss: 7.86574 |  0:03:51s\n",
      "epoch 68 | loss: 7.73469 |  0:03:55s\n",
      "epoch 69 | loss: 7.71308 |  0:03:58s\n",
      "epoch 70 | loss: 7.66995 |  0:04:01s\n",
      "epoch 71 | loss: 7.7981  |  0:04:05s\n",
      "epoch 72 | loss: 7.7229  |  0:04:08s\n",
      "epoch 73 | loss: 7.57484 |  0:04:11s\n",
      "epoch 74 | loss: 7.96071 |  0:04:15s\n",
      "epoch 75 | loss: 7.60079 |  0:04:18s\n",
      "epoch 76 | loss: 7.49179 |  0:04:22s\n",
      "epoch 77 | loss: 7.45879 |  0:04:25s\n",
      "epoch 78 | loss: 7.44984 |  0:04:29s\n",
      "epoch 79 | loss: 7.32386 |  0:04:32s\n",
      "epoch 80 | loss: 7.53767 |  0:04:35s\n",
      "epoch 81 | loss: 7.23213 |  0:04:38s\n",
      "epoch 82 | loss: 7.35879 |  0:04:42s\n",
      "epoch 83 | loss: 7.45618 |  0:04:45s\n",
      "epoch 84 | loss: 7.44566 |  0:04:49s\n",
      "epoch 85 | loss: 7.24045 |  0:04:52s\n",
      "epoch 86 | loss: 7.13994 |  0:04:56s\n",
      "epoch 87 | loss: 7.30798 |  0:04:59s\n",
      "epoch 88 | loss: 7.40827 |  0:05:03s\n",
      "epoch 89 | loss: 7.27533 |  0:05:06s\n",
      "epoch 90 | loss: 7.26324 |  0:05:09s\n",
      "epoch 91 | loss: 7.22941 |  0:05:13s\n",
      "epoch 92 | loss: 7.08765 |  0:05:16s\n",
      "epoch 93 | loss: 7.15837 |  0:05:19s\n",
      "epoch 94 | loss: 7.28799 |  0:05:23s\n",
      "epoch 95 | loss: 7.07862 |  0:05:26s\n",
      "epoch 96 | loss: 7.00668 |  0:05:30s\n",
      "epoch 97 | loss: 6.92875 |  0:05:33s\n",
      "epoch 98 | loss: 7.07782 |  0:05:37s\n",
      "epoch 99 | loss: 7.11471 |  0:05:40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:09:48,992]\u001b[0m Trial 0 finished with value: 2.869403157472206 and parameters: {'n_d': 16, 'n_a': 17, 'n_steps': 8, 'gamma': 1.1792901351310356, 'n_independent': 1, 'n_shared': 3, 'momentum': 0.13446266890938988}. Best is trial 0 with value: 2.869403157472206.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 203.4597|  0:00:05s\n",
      "epoch 1  | loss: 46.97964|  0:00:11s\n",
      "epoch 2  | loss: 68.07809|  0:00:17s\n",
      "epoch 3  | loss: 45.31473|  0:00:22s\n",
      "epoch 4  | loss: 49.04182|  0:00:28s\n",
      "epoch 5  | loss: 35.63256|  0:00:34s\n",
      "epoch 6  | loss: 22.45366|  0:00:39s\n",
      "epoch 7  | loss: 19.21767|  0:00:45s\n",
      "epoch 8  | loss: 17.16901|  0:00:51s\n",
      "epoch 9  | loss: 16.09491|  0:00:56s\n",
      "epoch 10 | loss: 15.57963|  0:01:02s\n",
      "epoch 11 | loss: 13.95769|  0:01:07s\n",
      "epoch 12 | loss: 14.08625|  0:01:13s\n",
      "epoch 13 | loss: 13.45691|  0:01:19s\n",
      "epoch 14 | loss: 12.94674|  0:01:25s\n",
      "epoch 15 | loss: 12.22374|  0:01:30s\n",
      "epoch 16 | loss: 11.87382|  0:01:36s\n",
      "epoch 17 | loss: 11.447  |  0:01:41s\n",
      "epoch 18 | loss: 11.32992|  0:01:47s\n",
      "epoch 19 | loss: 11.12705|  0:01:53s\n",
      "epoch 20 | loss: 11.03266|  0:01:59s\n",
      "epoch 21 | loss: 10.52226|  0:02:04s\n",
      "epoch 22 | loss: 10.67611|  0:02:10s\n",
      "epoch 23 | loss: 10.45595|  0:02:16s\n",
      "epoch 24 | loss: 10.29227|  0:02:22s\n",
      "epoch 25 | loss: 10.4831 |  0:02:27s\n",
      "epoch 26 | loss: 10.15918|  0:02:33s\n",
      "epoch 27 | loss: 9.89394 |  0:02:38s\n",
      "epoch 28 | loss: 9.88468 |  0:02:44s\n",
      "epoch 29 | loss: 9.67882 |  0:02:50s\n",
      "epoch 30 | loss: 9.52488 |  0:02:55s\n",
      "epoch 31 | loss: 9.32697 |  0:03:01s\n",
      "epoch 32 | loss: 9.26483 |  0:03:06s\n",
      "epoch 33 | loss: 9.01376 |  0:03:12s\n",
      "epoch 34 | loss: 8.96694 |  0:03:17s\n",
      "epoch 35 | loss: 8.67218 |  0:03:24s\n",
      "epoch 36 | loss: 8.93215 |  0:03:29s\n",
      "epoch 37 | loss: 8.56116 |  0:03:35s\n",
      "epoch 38 | loss: 8.46946 |  0:03:40s\n",
      "epoch 39 | loss: 8.53494 |  0:03:46s\n",
      "epoch 40 | loss: 8.38516 |  0:03:52s\n",
      "epoch 41 | loss: 8.50871 |  0:03:58s\n",
      "epoch 42 | loss: 8.68686 |  0:04:03s\n",
      "epoch 43 | loss: 8.23584 |  0:04:09s\n",
      "epoch 44 | loss: 8.28695 |  0:04:15s\n",
      "epoch 45 | loss: 7.97188 |  0:04:20s\n",
      "epoch 46 | loss: 8.10441 |  0:04:27s\n",
      "epoch 47 | loss: 7.93459 |  0:04:32s\n",
      "epoch 48 | loss: 7.91674 |  0:04:38s\n",
      "epoch 49 | loss: 7.54619 |  0:04:43s\n",
      "epoch 50 | loss: 7.67419 |  0:04:49s\n",
      "epoch 51 | loss: 7.55289 |  0:04:55s\n",
      "epoch 52 | loss: 7.36898 |  0:05:01s\n",
      "epoch 53 | loss: 7.82644 |  0:05:06s\n",
      "epoch 54 | loss: 7.3315  |  0:05:12s\n",
      "epoch 55 | loss: 7.51792 |  0:05:17s\n",
      "epoch 56 | loss: 7.86158 |  0:05:23s\n",
      "epoch 57 | loss: 7.64179 |  0:05:29s\n",
      "epoch 58 | loss: 7.75676 |  0:05:34s\n",
      "epoch 59 | loss: 7.42905 |  0:05:40s\n",
      "epoch 60 | loss: 7.35108 |  0:05:45s\n",
      "epoch 61 | loss: 7.23368 |  0:05:51s\n",
      "epoch 62 | loss: 7.19361 |  0:05:57s\n",
      "epoch 63 | loss: 7.24036 |  0:06:02s\n",
      "epoch 64 | loss: 6.9279  |  0:06:08s\n",
      "epoch 65 | loss: 6.92202 |  0:06:13s\n",
      "epoch 66 | loss: 6.94515 |  0:06:19s\n",
      "epoch 67 | loss: 6.69118 |  0:06:24s\n",
      "epoch 68 | loss: 6.91537 |  0:06:30s\n",
      "epoch 69 | loss: 6.81104 |  0:06:36s\n",
      "epoch 70 | loss: 6.6993  |  0:06:42s\n",
      "epoch 71 | loss: 6.81482 |  0:06:47s\n",
      "epoch 72 | loss: 6.55875 |  0:06:53s\n",
      "epoch 73 | loss: 6.60742 |  0:06:58s\n",
      "epoch 74 | loss: 6.60869 |  0:07:04s\n",
      "epoch 75 | loss: 6.51905 |  0:07:10s\n",
      "epoch 76 | loss: 6.64901 |  0:07:15s\n",
      "epoch 77 | loss: 6.4022  |  0:07:21s\n",
      "epoch 78 | loss: 6.43809 |  0:07:26s\n",
      "epoch 79 | loss: 6.60663 |  0:07:32s\n",
      "epoch 80 | loss: 6.56801 |  0:07:38s\n",
      "epoch 81 | loss: 6.54091 |  0:07:44s\n",
      "epoch 82 | loss: 6.4338  |  0:07:49s\n",
      "epoch 83 | loss: 6.54861 |  0:07:55s\n",
      "epoch 84 | loss: 6.25725 |  0:08:00s\n",
      "epoch 85 | loss: 6.34698 |  0:08:06s\n",
      "epoch 86 | loss: 6.25602 |  0:08:12s\n",
      "epoch 87 | loss: 6.44281 |  0:08:18s\n",
      "epoch 88 | loss: 6.21566 |  0:08:23s\n",
      "epoch 89 | loss: 6.12295 |  0:08:29s\n",
      "epoch 90 | loss: 6.26824 |  0:08:35s\n",
      "epoch 91 | loss: 6.27499 |  0:08:41s\n",
      "epoch 92 | loss: 6.09814 |  0:08:46s\n",
      "epoch 93 | loss: 6.22315 |  0:08:52s\n",
      "epoch 94 | loss: 5.89587 |  0:08:57s\n",
      "epoch 95 | loss: 5.94287 |  0:09:03s\n",
      "epoch 96 | loss: 6.02297 |  0:09:09s\n",
      "epoch 97 | loss: 5.96523 |  0:09:15s\n",
      "epoch 98 | loss: 5.85175 |  0:09:20s\n",
      "epoch 99 | loss: 6.20809 |  0:09:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:19:20,240]\u001b[0m Trial 1 finished with value: 5.320756861847874 and parameters: {'n_d': 32, 'n_a': 19, 'n_steps': 9, 'gamma': 1.1522013032621965, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.011915537822230756}. Best is trial 0 with value: 2.869403157472206.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 144.84961|  0:00:03s\n",
      "epoch 1  | loss: 40.00561|  0:00:06s\n",
      "epoch 2  | loss: 28.3764 |  0:00:09s\n",
      "epoch 3  | loss: 24.20431|  0:00:13s\n",
      "epoch 4  | loss: 20.73274|  0:00:16s\n",
      "epoch 5  | loss: 19.45039|  0:00:19s\n",
      "epoch 6  | loss: 17.19265|  0:00:22s\n",
      "epoch 7  | loss: 17.40918|  0:00:25s\n",
      "epoch 8  | loss: 16.9175 |  0:00:28s\n",
      "epoch 9  | loss: 13.98547|  0:00:31s\n",
      "epoch 10 | loss: 14.22128|  0:00:34s\n",
      "epoch 11 | loss: 12.94984|  0:00:38s\n",
      "epoch 12 | loss: 12.51701|  0:00:41s\n",
      "epoch 13 | loss: 12.04888|  0:00:44s\n",
      "epoch 14 | loss: 11.38446|  0:00:48s\n",
      "epoch 15 | loss: 11.11344|  0:00:51s\n",
      "epoch 16 | loss: 10.71766|  0:00:54s\n",
      "epoch 17 | loss: 10.59117|  0:00:57s\n",
      "epoch 18 | loss: 10.20047|  0:01:00s\n",
      "epoch 19 | loss: 10.36955|  0:01:03s\n",
      "epoch 20 | loss: 10.30738|  0:01:06s\n",
      "epoch 21 | loss: 10.15838|  0:01:10s\n",
      "epoch 22 | loss: 9.77681 |  0:01:13s\n",
      "epoch 23 | loss: 9.56778 |  0:01:16s\n",
      "epoch 24 | loss: 9.59697 |  0:01:20s\n",
      "epoch 25 | loss: 9.35565 |  0:01:23s\n",
      "epoch 26 | loss: 9.44607 |  0:01:26s\n",
      "epoch 27 | loss: 8.9734  |  0:01:29s\n",
      "epoch 28 | loss: 8.93999 |  0:01:32s\n",
      "epoch 29 | loss: 8.88788 |  0:01:35s\n",
      "epoch 30 | loss: 8.94146 |  0:01:38s\n",
      "epoch 31 | loss: 8.55606 |  0:01:42s\n",
      "epoch 32 | loss: 8.70456 |  0:01:45s\n",
      "epoch 33 | loss: 9.14337 |  0:01:48s\n",
      "epoch 34 | loss: 8.4941  |  0:01:51s\n",
      "epoch 35 | loss: 8.56477 |  0:01:54s\n",
      "epoch 36 | loss: 8.69486 |  0:01:57s\n",
      "epoch 37 | loss: 8.57189 |  0:02:00s\n",
      "epoch 38 | loss: 8.38828 |  0:02:03s\n",
      "epoch 39 | loss: 8.11587 |  0:02:06s\n",
      "epoch 40 | loss: 7.97444 |  0:02:10s\n",
      "epoch 41 | loss: 7.80966 |  0:02:13s\n",
      "epoch 42 | loss: 7.87314 |  0:02:17s\n",
      "epoch 43 | loss: 7.64463 |  0:02:20s\n",
      "epoch 44 | loss: 8.21422 |  0:02:23s\n",
      "epoch 45 | loss: 8.83572 |  0:02:26s\n",
      "epoch 46 | loss: 8.41192 |  0:02:29s\n",
      "epoch 47 | loss: 8.29338 |  0:02:32s\n",
      "epoch 48 | loss: 7.8951  |  0:02:35s\n",
      "epoch 49 | loss: 7.79187 |  0:02:38s\n",
      "epoch 50 | loss: 7.56691 |  0:02:41s\n",
      "epoch 51 | loss: 7.40628 |  0:02:45s\n",
      "epoch 52 | loss: 7.80457 |  0:02:49s\n",
      "epoch 53 | loss: 7.76839 |  0:02:52s\n",
      "epoch 54 | loss: 7.47112 |  0:02:55s\n",
      "epoch 55 | loss: 7.31173 |  0:02:58s\n",
      "epoch 56 | loss: 7.32997 |  0:03:01s\n",
      "epoch 57 | loss: 7.12281 |  0:03:04s\n",
      "epoch 58 | loss: 6.94643 |  0:03:07s\n",
      "epoch 59 | loss: 6.91042 |  0:03:10s\n",
      "epoch 60 | loss: 6.97656 |  0:03:13s\n",
      "epoch 61 | loss: 6.79293 |  0:03:17s\n",
      "epoch 62 | loss: 6.75527 |  0:03:21s\n",
      "epoch 63 | loss: 6.73927 |  0:03:24s\n",
      "epoch 64 | loss: 6.78411 |  0:03:27s\n",
      "epoch 65 | loss: 6.7528  |  0:03:30s\n",
      "epoch 66 | loss: 6.64933 |  0:03:33s\n",
      "epoch 67 | loss: 6.40936 |  0:03:36s\n",
      "epoch 68 | loss: 6.6304  |  0:03:39s\n",
      "epoch 69 | loss: 6.39079 |  0:03:42s\n",
      "epoch 70 | loss: 6.40702 |  0:03:45s\n",
      "epoch 71 | loss: 6.45013 |  0:03:49s\n",
      "epoch 72 | loss: 6.3116  |  0:03:52s\n",
      "epoch 73 | loss: 6.16959 |  0:03:55s\n",
      "epoch 74 | loss: 6.23556 |  0:03:58s\n",
      "epoch 75 | loss: 6.22105 |  0:04:01s\n",
      "epoch 76 | loss: 6.19739 |  0:04:05s\n",
      "epoch 77 | loss: 6.24482 |  0:04:08s\n",
      "epoch 78 | loss: 6.01135 |  0:04:11s\n",
      "epoch 79 | loss: 6.01056 |  0:04:14s\n",
      "epoch 80 | loss: 5.95426 |  0:04:17s\n",
      "epoch 81 | loss: 5.86001 |  0:04:21s\n",
      "epoch 82 | loss: 5.93254 |  0:04:24s\n",
      "epoch 83 | loss: 5.93738 |  0:04:27s\n",
      "epoch 84 | loss: 5.85452 |  0:04:30s\n",
      "epoch 85 | loss: 5.82765 |  0:04:33s\n",
      "epoch 86 | loss: 5.9567  |  0:04:36s\n",
      "epoch 87 | loss: 5.86889 |  0:04:39s\n",
      "epoch 88 | loss: 5.9657  |  0:04:42s\n",
      "epoch 89 | loss: 5.71293 |  0:04:45s\n",
      "epoch 90 | loss: 5.70095 |  0:04:49s\n",
      "epoch 91 | loss: 5.59748 |  0:04:52s\n",
      "epoch 92 | loss: 5.86477 |  0:04:56s\n",
      "epoch 93 | loss: 5.80697 |  0:04:59s\n",
      "epoch 94 | loss: 5.72123 |  0:05:02s\n",
      "epoch 95 | loss: 5.56049 |  0:05:05s\n",
      "epoch 96 | loss: 5.51127 |  0:05:08s\n",
      "epoch 97 | loss: 5.55618 |  0:05:11s\n",
      "epoch 98 | loss: 5.53532 |  0:05:14s\n",
      "epoch 99 | loss: 5.54367 |  0:05:17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:24:41,034]\u001b[0m Trial 2 finished with value: 2.516404174234202 and parameters: {'n_d': 16, 'n_a': 46, 'n_steps': 5, 'gamma': 1.5243842429702306, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.07105431259368751}. Best is trial 2 with value: 2.516404174234202.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 100.97593|  0:00:03s\n",
      "epoch 1  | loss: 31.99971|  0:00:05s\n",
      "epoch 2  | loss: 24.32259|  0:00:08s\n",
      "epoch 3  | loss: 19.48737|  0:00:11s\n",
      "epoch 4  | loss: 17.37723|  0:00:13s\n",
      "epoch 5  | loss: 14.92721|  0:00:16s\n",
      "epoch 6  | loss: 13.89835|  0:00:18s\n",
      "epoch 7  | loss: 13.55658|  0:00:21s\n",
      "epoch 8  | loss: 12.80831|  0:00:24s\n",
      "epoch 9  | loss: 13.56169|  0:00:26s\n",
      "epoch 10 | loss: 12.43103|  0:00:29s\n",
      "epoch 11 | loss: 12.4652 |  0:00:32s\n",
      "epoch 12 | loss: 11.6924 |  0:00:35s\n",
      "epoch 13 | loss: 11.66641|  0:00:38s\n",
      "epoch 14 | loss: 11.57843|  0:00:40s\n",
      "epoch 15 | loss: 12.59417|  0:00:43s\n",
      "epoch 16 | loss: 11.53847|  0:00:46s\n",
      "epoch 17 | loss: 10.97828|  0:00:48s\n",
      "epoch 18 | loss: 10.73801|  0:00:51s\n",
      "epoch 19 | loss: 10.83506|  0:00:53s\n",
      "epoch 20 | loss: 10.46837|  0:00:56s\n",
      "epoch 21 | loss: 10.16997|  0:00:59s\n",
      "epoch 22 | loss: 10.43079|  0:01:02s\n",
      "epoch 23 | loss: 10.11763|  0:01:05s\n",
      "epoch 24 | loss: 9.63842 |  0:01:08s\n",
      "epoch 25 | loss: 9.6975  |  0:01:10s\n",
      "epoch 26 | loss: 9.33041 |  0:01:13s\n",
      "epoch 27 | loss: 9.19101 |  0:01:15s\n",
      "epoch 28 | loss: 9.12698 |  0:01:18s\n",
      "epoch 29 | loss: 9.03383 |  0:01:20s\n",
      "epoch 30 | loss: 8.95861 |  0:01:23s\n",
      "epoch 31 | loss: 11.13922|  0:01:26s\n",
      "epoch 32 | loss: 9.5152  |  0:01:28s\n",
      "epoch 33 | loss: 9.37898 |  0:01:31s\n",
      "epoch 34 | loss: 9.1909  |  0:01:34s\n",
      "epoch 35 | loss: 8.69188 |  0:01:37s\n",
      "epoch 36 | loss: 8.70028 |  0:01:40s\n",
      "epoch 37 | loss: 8.3656  |  0:01:42s\n",
      "epoch 38 | loss: 8.58844 |  0:01:45s\n",
      "epoch 39 | loss: 8.38059 |  0:01:47s\n",
      "epoch 40 | loss: 8.34565 |  0:01:50s\n",
      "epoch 41 | loss: 8.18204 |  0:01:52s\n",
      "epoch 42 | loss: 8.00338 |  0:01:55s\n",
      "epoch 43 | loss: 7.94312 |  0:01:58s\n",
      "epoch 44 | loss: 7.88854 |  0:02:00s\n",
      "epoch 45 | loss: 7.73038 |  0:02:03s\n",
      "epoch 46 | loss: 7.6305  |  0:02:06s\n",
      "epoch 47 | loss: 7.6659  |  0:02:09s\n",
      "epoch 48 | loss: 7.68731 |  0:02:11s\n",
      "epoch 49 | loss: 7.48631 |  0:02:14s\n",
      "epoch 50 | loss: 7.44625 |  0:02:17s\n",
      "epoch 51 | loss: 7.40693 |  0:02:19s\n",
      "epoch 52 | loss: 7.28466 |  0:02:22s\n",
      "epoch 53 | loss: 7.16202 |  0:02:25s\n",
      "epoch 54 | loss: 7.32803 |  0:02:27s\n",
      "epoch 55 | loss: 7.06291 |  0:02:30s\n",
      "epoch 56 | loss: 6.9914  |  0:02:33s\n",
      "epoch 57 | loss: 6.92145 |  0:02:35s\n",
      "epoch 58 | loss: 6.77113 |  0:02:38s\n",
      "epoch 59 | loss: 7.02374 |  0:02:41s\n",
      "epoch 60 | loss: 6.87205 |  0:02:43s\n",
      "epoch 61 | loss: 6.87346 |  0:02:46s\n",
      "epoch 62 | loss: 6.79983 |  0:02:49s\n",
      "epoch 63 | loss: 6.64337 |  0:02:51s\n",
      "epoch 64 | loss: 6.63708 |  0:02:54s\n",
      "epoch 65 | loss: 6.84561 |  0:02:57s\n",
      "epoch 66 | loss: 6.60274 |  0:02:59s\n",
      "epoch 67 | loss: 6.50482 |  0:03:02s\n",
      "epoch 68 | loss: 6.5684  |  0:03:04s\n",
      "epoch 69 | loss: 6.50002 |  0:03:07s\n",
      "epoch 70 | loss: 6.59708 |  0:03:10s\n",
      "epoch 71 | loss: 6.36808 |  0:03:13s\n",
      "epoch 72 | loss: 6.28183 |  0:03:15s\n",
      "epoch 73 | loss: 6.41776 |  0:03:18s\n",
      "epoch 74 | loss: 6.30777 |  0:03:21s\n",
      "epoch 75 | loss: 6.30668 |  0:03:23s\n",
      "epoch 76 | loss: 6.05435 |  0:03:26s\n",
      "epoch 77 | loss: 6.16816 |  0:03:28s\n",
      "epoch 78 | loss: 6.08365 |  0:03:31s\n",
      "epoch 79 | loss: 6.09519 |  0:03:34s\n",
      "epoch 80 | loss: 6.12643 |  0:03:37s\n",
      "epoch 81 | loss: 5.94015 |  0:03:39s\n",
      "epoch 82 | loss: 5.9409  |  0:03:43s\n",
      "epoch 83 | loss: 6.00837 |  0:03:45s\n",
      "epoch 84 | loss: 6.05773 |  0:03:48s\n",
      "epoch 85 | loss: 5.93927 |  0:03:50s\n",
      "epoch 86 | loss: 5.95484 |  0:03:53s\n",
      "epoch 87 | loss: 5.90778 |  0:03:56s\n",
      "epoch 88 | loss: 5.945   |  0:03:58s\n",
      "epoch 89 | loss: 5.85675 |  0:04:01s\n",
      "epoch 90 | loss: 5.76505 |  0:04:04s\n",
      "epoch 91 | loss: 5.69618 |  0:04:06s\n",
      "epoch 92 | loss: 5.78919 |  0:04:09s\n",
      "epoch 93 | loss: 5.73104 |  0:04:11s\n",
      "epoch 94 | loss: 5.61292 |  0:04:15s\n",
      "epoch 95 | loss: 5.63051 |  0:04:17s\n",
      "epoch 96 | loss: 5.54342 |  0:04:20s\n",
      "epoch 97 | loss: 5.49415 |  0:04:22s\n",
      "epoch 98 | loss: 5.5822  |  0:04:25s\n",
      "epoch 99 | loss: 5.51454 |  0:04:28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:29:11,295]\u001b[0m Trial 3 finished with value: 2.7415864774295335 and parameters: {'n_d': 54, 'n_a': 12, 'n_steps': 4, 'gamma': 1.2376124451529584, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.20715115026468467}. Best is trial 2 with value: 2.516404174234202.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 106.75698|  0:00:04s\n",
      "epoch 1  | loss: 33.39966|  0:00:08s\n",
      "epoch 2  | loss: 30.93685|  0:00:12s\n",
      "epoch 3  | loss: 27.9723 |  0:00:17s\n",
      "epoch 4  | loss: 32.31244|  0:00:21s\n",
      "epoch 5  | loss: 29.78322|  0:00:25s\n",
      "epoch 6  | loss: 23.80665|  0:00:29s\n",
      "epoch 7  | loss: 22.40031|  0:00:33s\n",
      "epoch 8  | loss: 19.28772|  0:00:37s\n",
      "epoch 9  | loss: 18.28357|  0:00:41s\n",
      "epoch 10 | loss: 19.11359|  0:00:46s\n",
      "epoch 11 | loss: 17.94972|  0:00:50s\n",
      "epoch 12 | loss: 16.61181|  0:00:54s\n",
      "epoch 13 | loss: 16.06953|  0:00:58s\n",
      "epoch 14 | loss: 17.60045|  0:01:02s\n",
      "epoch 15 | loss: 18.72873|  0:01:06s\n",
      "epoch 16 | loss: 17.19229|  0:01:10s\n",
      "epoch 17 | loss: 16.35912|  0:01:15s\n",
      "epoch 18 | loss: 16.54052|  0:01:19s\n",
      "epoch 19 | loss: 15.54951|  0:01:23s\n",
      "epoch 20 | loss: 15.29843|  0:01:27s\n",
      "epoch 21 | loss: 15.05653|  0:01:31s\n",
      "epoch 22 | loss: 15.18473|  0:01:36s\n",
      "epoch 23 | loss: 15.03303|  0:01:40s\n",
      "epoch 24 | loss: 14.62006|  0:01:44s\n",
      "epoch 25 | loss: 14.22339|  0:01:48s\n",
      "epoch 26 | loss: 14.02744|  0:01:52s\n",
      "epoch 27 | loss: 13.31105|  0:01:57s\n",
      "epoch 28 | loss: 13.48321|  0:02:01s\n",
      "epoch 29 | loss: 13.56789|  0:02:05s\n",
      "epoch 30 | loss: 12.91721|  0:02:09s\n",
      "epoch 31 | loss: 12.54655|  0:02:13s\n",
      "epoch 32 | loss: 11.91708|  0:02:18s\n",
      "epoch 33 | loss: 12.28782|  0:02:22s\n",
      "epoch 34 | loss: 11.82969|  0:02:26s\n",
      "epoch 35 | loss: 11.66961|  0:02:30s\n",
      "epoch 36 | loss: 11.44474|  0:02:34s\n",
      "epoch 37 | loss: 11.41958|  0:02:39s\n",
      "epoch 38 | loss: 11.23166|  0:02:43s\n",
      "epoch 39 | loss: 11.54072|  0:02:47s\n",
      "epoch 40 | loss: 11.19975|  0:02:52s\n",
      "epoch 41 | loss: 10.92828|  0:02:56s\n",
      "epoch 42 | loss: 10.6257 |  0:03:00s\n",
      "epoch 43 | loss: 10.67246|  0:03:05s\n",
      "epoch 44 | loss: 10.55877|  0:03:09s\n",
      "epoch 45 | loss: 10.18516|  0:03:13s\n",
      "epoch 46 | loss: 10.11641|  0:03:17s\n",
      "epoch 47 | loss: 10.07539|  0:03:21s\n",
      "epoch 48 | loss: 9.94596 |  0:03:26s\n",
      "epoch 49 | loss: 9.68826 |  0:03:30s\n",
      "epoch 50 | loss: 9.90884 |  0:03:34s\n",
      "epoch 51 | loss: 9.54751 |  0:03:38s\n",
      "epoch 52 | loss: 9.38286 |  0:03:42s\n",
      "epoch 53 | loss: 9.29191 |  0:03:47s\n",
      "epoch 54 | loss: 9.21016 |  0:03:51s\n",
      "epoch 55 | loss: 9.65133 |  0:03:55s\n",
      "epoch 56 | loss: 9.16099 |  0:03:59s\n",
      "epoch 57 | loss: 9.07346 |  0:04:04s\n",
      "epoch 58 | loss: 8.92461 |  0:04:08s\n",
      "epoch 59 | loss: 8.83324 |  0:04:12s\n",
      "epoch 60 | loss: 8.6353  |  0:04:16s\n",
      "epoch 61 | loss: 8.76787 |  0:04:20s\n",
      "epoch 62 | loss: 8.58227 |  0:04:24s\n",
      "epoch 63 | loss: 8.46579 |  0:04:29s\n",
      "epoch 64 | loss: 8.51533 |  0:04:33s\n",
      "epoch 65 | loss: 8.22072 |  0:04:37s\n",
      "epoch 66 | loss: 8.26442 |  0:04:42s\n",
      "epoch 67 | loss: 8.15806 |  0:04:46s\n",
      "epoch 68 | loss: 7.98926 |  0:04:50s\n",
      "epoch 69 | loss: 8.16898 |  0:04:54s\n",
      "epoch 70 | loss: 8.0344  |  0:04:59s\n",
      "epoch 71 | loss: 7.88123 |  0:05:03s\n",
      "epoch 72 | loss: 8.28067 |  0:05:07s\n",
      "epoch 73 | loss: 7.88747 |  0:05:11s\n",
      "epoch 74 | loss: 7.74405 |  0:05:16s\n",
      "epoch 75 | loss: 8.00328 |  0:05:20s\n",
      "epoch 76 | loss: 7.86912 |  0:05:24s\n",
      "epoch 77 | loss: 7.60467 |  0:05:28s\n",
      "epoch 78 | loss: 7.43016 |  0:05:32s\n",
      "epoch 79 | loss: 7.60915 |  0:05:37s\n",
      "epoch 80 | loss: 7.18825 |  0:05:41s\n",
      "epoch 81 | loss: 7.26247 |  0:05:45s\n",
      "epoch 82 | loss: 7.33668 |  0:05:49s\n",
      "epoch 83 | loss: 7.23238 |  0:05:53s\n",
      "epoch 84 | loss: 7.27623 |  0:05:58s\n",
      "epoch 85 | loss: 7.00481 |  0:06:02s\n",
      "epoch 86 | loss: 7.00517 |  0:06:06s\n",
      "epoch 87 | loss: 7.14384 |  0:06:10s\n",
      "epoch 88 | loss: 7.01205 |  0:06:15s\n",
      "epoch 89 | loss: 7.03007 |  0:06:19s\n",
      "epoch 90 | loss: 6.98007 |  0:06:23s\n",
      "epoch 91 | loss: 6.77113 |  0:06:27s\n",
      "epoch 92 | loss: 7.93418 |  0:06:32s\n",
      "epoch 93 | loss: 7.01836 |  0:06:36s\n",
      "epoch 94 | loss: 6.82917 |  0:06:40s\n",
      "epoch 95 | loss: 6.67278 |  0:06:44s\n",
      "epoch 96 | loss: 6.75251 |  0:06:48s\n",
      "epoch 97 | loss: 6.71809 |  0:06:53s\n",
      "epoch 98 | loss: 6.63862 |  0:06:57s\n",
      "epoch 99 | loss: 6.49283 |  0:07:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:36:17,405]\u001b[0m Trial 4 finished with value: 7.551546365862773 and parameters: {'n_d': 61, 'n_a': 59, 'n_steps': 10, 'gamma': 1.036939251682052, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.30606686654228815}. Best is trial 2 with value: 2.516404174234202.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 140.63922|  0:00:02s\n",
      "epoch 1  | loss: 21.68483|  0:00:04s\n",
      "epoch 2  | loss: 17.00409|  0:00:06s\n",
      "epoch 3  | loss: 14.58514|  0:00:08s\n",
      "epoch 4  | loss: 13.82962|  0:00:10s\n",
      "epoch 5  | loss: 13.17514|  0:00:13s\n",
      "epoch 6  | loss: 12.02091|  0:00:15s\n",
      "epoch 7  | loss: 11.64226|  0:00:17s\n",
      "epoch 8  | loss: 11.26689|  0:00:19s\n",
      "epoch 9  | loss: 11.04391|  0:00:21s\n",
      "epoch 10 | loss: 10.69631|  0:00:24s\n",
      "epoch 11 | loss: 10.11417|  0:00:26s\n",
      "epoch 12 | loss: 9.93395 |  0:00:28s\n",
      "epoch 13 | loss: 9.7511  |  0:00:30s\n",
      "epoch 14 | loss: 9.5955  |  0:00:33s\n",
      "epoch 15 | loss: 9.19763 |  0:00:35s\n",
      "epoch 16 | loss: 9.19922 |  0:00:37s\n",
      "epoch 17 | loss: 8.78517 |  0:00:39s\n",
      "epoch 18 | loss: 8.66822 |  0:00:41s\n",
      "epoch 19 | loss: 8.48399 |  0:00:43s\n",
      "epoch 20 | loss: 8.16837 |  0:00:46s\n",
      "epoch 21 | loss: 8.09795 |  0:00:48s\n",
      "epoch 22 | loss: 7.98528 |  0:00:50s\n",
      "epoch 23 | loss: 7.93842 |  0:00:52s\n",
      "epoch 24 | loss: 8.0487  |  0:00:54s\n",
      "epoch 25 | loss: 7.78798 |  0:00:56s\n",
      "epoch 26 | loss: 7.61309 |  0:00:58s\n",
      "epoch 27 | loss: 7.33067 |  0:01:01s\n",
      "epoch 28 | loss: 7.50076 |  0:01:03s\n",
      "epoch 29 | loss: 7.36108 |  0:01:05s\n",
      "epoch 30 | loss: 7.06541 |  0:01:07s\n",
      "epoch 31 | loss: 6.84296 |  0:01:09s\n",
      "epoch 32 | loss: 6.97225 |  0:01:12s\n",
      "epoch 33 | loss: 6.92578 |  0:01:14s\n",
      "epoch 34 | loss: 6.8631  |  0:01:16s\n",
      "epoch 35 | loss: 6.67229 |  0:01:18s\n",
      "epoch 36 | loss: 6.45499 |  0:01:20s\n",
      "epoch 37 | loss: 6.61349 |  0:01:23s\n",
      "epoch 38 | loss: 6.48908 |  0:01:25s\n",
      "epoch 39 | loss: 6.28448 |  0:01:27s\n",
      "epoch 40 | loss: 6.1672  |  0:01:29s\n",
      "epoch 41 | loss: 6.24297 |  0:01:31s\n",
      "epoch 42 | loss: 6.25367 |  0:01:33s\n",
      "epoch 43 | loss: 6.23839 |  0:01:36s\n",
      "epoch 44 | loss: 6.11575 |  0:01:38s\n",
      "epoch 45 | loss: 5.97403 |  0:01:40s\n",
      "epoch 46 | loss: 5.9313  |  0:01:43s\n",
      "epoch 47 | loss: 5.81392 |  0:01:45s\n",
      "epoch 48 | loss: 5.84404 |  0:01:47s\n",
      "epoch 49 | loss: 5.90607 |  0:01:49s\n",
      "epoch 50 | loss: 5.75447 |  0:01:52s\n",
      "epoch 51 | loss: 5.65696 |  0:01:54s\n",
      "epoch 52 | loss: 5.77962 |  0:01:56s\n",
      "epoch 53 | loss: 5.57355 |  0:01:58s\n",
      "epoch 54 | loss: 5.75166 |  0:02:00s\n",
      "epoch 55 | loss: 5.45908 |  0:02:03s\n",
      "epoch 56 | loss: 5.42023 |  0:02:05s\n",
      "epoch 57 | loss: 5.53612 |  0:02:07s\n",
      "epoch 58 | loss: 5.56356 |  0:02:09s\n",
      "epoch 59 | loss: 5.51776 |  0:02:11s\n",
      "epoch 60 | loss: 5.41732 |  0:02:13s\n",
      "epoch 61 | loss: 5.20524 |  0:02:16s\n",
      "epoch 62 | loss: 5.12323 |  0:02:18s\n",
      "epoch 63 | loss: 5.19614 |  0:02:20s\n",
      "epoch 64 | loss: 5.19561 |  0:02:22s\n",
      "epoch 65 | loss: 5.30265 |  0:02:24s\n",
      "epoch 66 | loss: 5.20848 |  0:02:26s\n",
      "epoch 67 | loss: 5.2079  |  0:02:28s\n",
      "epoch 68 | loss: 5.03636 |  0:02:31s\n",
      "epoch 69 | loss: 5.10764 |  0:02:33s\n",
      "epoch 70 | loss: 5.06929 |  0:02:35s\n",
      "epoch 71 | loss: 5.10165 |  0:02:37s\n",
      "epoch 72 | loss: 5.02091 |  0:02:39s\n",
      "epoch 73 | loss: 5.1824  |  0:02:42s\n",
      "epoch 74 | loss: 5.07025 |  0:02:44s\n",
      "epoch 75 | loss: 4.93569 |  0:02:46s\n",
      "epoch 76 | loss: 4.85454 |  0:02:48s\n",
      "epoch 77 | loss: 4.85527 |  0:02:50s\n",
      "epoch 78 | loss: 4.93135 |  0:02:53s\n",
      "epoch 79 | loss: 4.87245 |  0:02:55s\n",
      "epoch 80 | loss: 4.78552 |  0:02:57s\n",
      "epoch 81 | loss: 4.79195 |  0:02:59s\n",
      "epoch 82 | loss: 5.57808 |  0:03:01s\n",
      "epoch 83 | loss: 5.11532 |  0:03:03s\n",
      "epoch 84 | loss: 4.79392 |  0:03:06s\n",
      "epoch 85 | loss: 4.80541 |  0:03:08s\n",
      "epoch 86 | loss: 4.75602 |  0:03:10s\n",
      "epoch 87 | loss: 4.79078 |  0:03:12s\n",
      "epoch 88 | loss: 4.76294 |  0:03:15s\n",
      "epoch 89 | loss: 4.71386 |  0:03:17s\n",
      "epoch 90 | loss: 4.63199 |  0:03:19s\n",
      "epoch 91 | loss: 4.73468 |  0:03:21s\n",
      "epoch 92 | loss: 4.69428 |  0:03:23s\n",
      "epoch 93 | loss: 4.75104 |  0:03:25s\n",
      "epoch 94 | loss: 4.52026 |  0:03:27s\n",
      "epoch 95 | loss: 4.58228 |  0:03:30s\n",
      "epoch 96 | loss: 4.49096 |  0:03:32s\n",
      "epoch 97 | loss: 4.53916 |  0:03:34s\n",
      "epoch 98 | loss: 4.54177 |  0:03:36s\n",
      "epoch 99 | loss: 4.60324 |  0:03:39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:39:58,593]\u001b[0m Trial 5 finished with value: 2.550078155642487 and parameters: {'n_d': 27, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0734902749801718, 'n_independent': 1, 'n_shared': 5, 'momentum': 0.2671578390488752}. Best is trial 2 with value: 2.516404174234202.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 131.89004|  0:00:02s\n",
      "epoch 1  | loss: 26.00874|  0:00:05s\n",
      "epoch 2  | loss: 20.25521|  0:00:08s\n",
      "epoch 3  | loss: 19.251  |  0:00:11s\n",
      "epoch 4  | loss: 18.01602|  0:00:13s\n",
      "epoch 5  | loss: 17.37874|  0:00:16s\n",
      "epoch 6  | loss: 15.99069|  0:00:19s\n",
      "epoch 7  | loss: 15.50613|  0:00:21s\n",
      "epoch 8  | loss: 14.71874|  0:00:24s\n",
      "epoch 9  | loss: 13.43048|  0:00:27s\n",
      "epoch 10 | loss: 13.27243|  0:00:30s\n",
      "epoch 11 | loss: 12.40949|  0:00:33s\n",
      "epoch 12 | loss: 12.13766|  0:00:36s\n",
      "epoch 13 | loss: 11.63314|  0:00:38s\n",
      "epoch 14 | loss: 11.35365|  0:00:41s\n",
      "epoch 15 | loss: 11.27841|  0:00:44s\n",
      "epoch 16 | loss: 10.74683|  0:00:47s\n",
      "epoch 17 | loss: 10.95095|  0:00:49s\n",
      "epoch 18 | loss: 10.63286|  0:00:52s\n",
      "epoch 19 | loss: 10.27679|  0:00:55s\n",
      "epoch 20 | loss: 10.23791|  0:00:58s\n",
      "epoch 21 | loss: 9.95465 |  0:01:01s\n",
      "epoch 22 | loss: 9.82607 |  0:01:04s\n",
      "epoch 23 | loss: 9.76002 |  0:01:06s\n",
      "epoch 24 | loss: 9.79764 |  0:01:09s\n",
      "epoch 25 | loss: 9.56492 |  0:01:12s\n",
      "epoch 26 | loss: 9.4014  |  0:01:15s\n",
      "epoch 27 | loss: 9.24059 |  0:01:17s\n",
      "epoch 28 | loss: 9.1641  |  0:01:20s\n",
      "epoch 29 | loss: 8.97587 |  0:01:23s\n",
      "epoch 30 | loss: 8.73125 |  0:01:26s\n",
      "epoch 31 | loss: 8.6874  |  0:01:29s\n",
      "epoch 32 | loss: 8.69329 |  0:01:32s\n",
      "epoch 33 | loss: 8.39118 |  0:01:35s\n",
      "epoch 34 | loss: 8.61947 |  0:01:37s\n",
      "epoch 35 | loss: 8.33319 |  0:01:40s\n",
      "epoch 36 | loss: 8.34387 |  0:01:43s\n",
      "epoch 37 | loss: 8.26905 |  0:01:46s\n",
      "epoch 38 | loss: 8.08292 |  0:01:48s\n",
      "epoch 39 | loss: 8.00589 |  0:01:51s\n",
      "epoch 40 | loss: 7.91492 |  0:01:54s\n",
      "epoch 41 | loss: 7.78736 |  0:01:57s\n",
      "epoch 42 | loss: 7.6213  |  0:01:59s\n",
      "epoch 43 | loss: 7.665   |  0:02:03s\n",
      "epoch 44 | loss: 7.62425 |  0:02:05s\n",
      "epoch 45 | loss: 7.41962 |  0:02:08s\n",
      "epoch 46 | loss: 7.3482  |  0:02:11s\n",
      "epoch 47 | loss: 7.20137 |  0:02:14s\n",
      "epoch 48 | loss: 7.0987  |  0:02:16s\n",
      "epoch 49 | loss: 7.01149 |  0:02:19s\n",
      "epoch 50 | loss: 7.23917 |  0:02:22s\n",
      "epoch 51 | loss: 6.99159 |  0:02:25s\n",
      "epoch 52 | loss: 7.04367 |  0:02:27s\n",
      "epoch 53 | loss: 6.89784 |  0:02:30s\n",
      "epoch 54 | loss: 6.82988 |  0:02:34s\n",
      "epoch 55 | loss: 6.77708 |  0:02:37s\n",
      "epoch 56 | loss: 6.81523 |  0:02:39s\n",
      "epoch 57 | loss: 6.61832 |  0:02:42s\n",
      "epoch 58 | loss: 6.55389 |  0:02:45s\n",
      "epoch 59 | loss: 6.67249 |  0:02:48s\n",
      "epoch 60 | loss: 6.39734 |  0:02:50s\n",
      "epoch 61 | loss: 6.42056 |  0:02:53s\n",
      "epoch 62 | loss: 6.33207 |  0:02:56s\n",
      "epoch 63 | loss: 6.31209 |  0:02:59s\n",
      "epoch 64 | loss: 6.26845 |  0:03:01s\n",
      "epoch 65 | loss: 6.31287 |  0:03:05s\n",
      "epoch 66 | loss: 6.39062 |  0:03:07s\n",
      "epoch 67 | loss: 6.25558 |  0:03:10s\n",
      "epoch 68 | loss: 6.15718 |  0:03:13s\n",
      "epoch 69 | loss: 6.1644  |  0:03:16s\n",
      "epoch 70 | loss: 6.11186 |  0:03:18s\n",
      "epoch 71 | loss: 5.98456 |  0:03:21s\n",
      "epoch 72 | loss: 5.89858 |  0:03:24s\n",
      "epoch 73 | loss: 6.11193 |  0:03:27s\n",
      "epoch 74 | loss: 5.89831 |  0:03:29s\n",
      "epoch 75 | loss: 5.86095 |  0:03:32s\n",
      "epoch 76 | loss: 5.99795 |  0:03:34s\n",
      "epoch 77 | loss: 5.72383 |  0:03:38s\n",
      "epoch 78 | loss: 5.77888 |  0:03:41s\n",
      "epoch 79 | loss: 5.90344 |  0:03:44s\n",
      "epoch 80 | loss: 5.91637 |  0:03:46s\n",
      "epoch 81 | loss: 5.7811  |  0:03:49s\n",
      "epoch 82 | loss: 5.73501 |  0:03:52s\n",
      "epoch 83 | loss: 5.65263 |  0:03:54s\n",
      "epoch 84 | loss: 5.78253 |  0:03:57s\n",
      "epoch 85 | loss: 5.5502  |  0:04:00s\n",
      "epoch 86 | loss: 5.59964 |  0:04:03s\n",
      "epoch 87 | loss: 5.60328 |  0:04:05s\n",
      "epoch 88 | loss: 5.59562 |  0:04:09s\n",
      "epoch 89 | loss: 5.49397 |  0:04:11s\n",
      "epoch 90 | loss: 5.4947  |  0:04:14s\n",
      "epoch 91 | loss: 5.60559 |  0:04:17s\n",
      "epoch 92 | loss: 5.68476 |  0:04:20s\n",
      "epoch 93 | loss: 5.47702 |  0:04:23s\n",
      "epoch 94 | loss: 5.55088 |  0:04:25s\n",
      "epoch 95 | loss: 5.35296 |  0:04:28s\n",
      "epoch 96 | loss: 5.41601 |  0:04:31s\n",
      "epoch 97 | loss: 5.35546 |  0:04:34s\n",
      "epoch 98 | loss: 5.38551 |  0:04:36s\n",
      "epoch 99 | loss: 5.29813 |  0:04:39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:44:41,340]\u001b[0m Trial 6 finished with value: 2.494900428891409 and parameters: {'n_d': 27, 'n_a': 35, 'n_steps': 5, 'gamma': 1.1378019867067573, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.17444217220619315}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 150.27421|  0:00:03s\n",
      "epoch 1  | loss: 32.37936|  0:00:06s\n",
      "epoch 2  | loss: 28.267  |  0:00:09s\n",
      "epoch 3  | loss: 26.037  |  0:00:12s\n",
      "epoch 4  | loss: 25.46999|  0:00:16s\n",
      "epoch 5  | loss: 20.31006|  0:00:19s\n",
      "epoch 6  | loss: 18.90197|  0:00:22s\n",
      "epoch 7  | loss: 17.79555|  0:00:25s\n",
      "epoch 8  | loss: 16.67895|  0:00:29s\n",
      "epoch 9  | loss: 14.73067|  0:00:32s\n",
      "epoch 10 | loss: 14.07883|  0:00:35s\n",
      "epoch 11 | loss: 13.26112|  0:00:39s\n",
      "epoch 12 | loss: 12.05669|  0:00:42s\n",
      "epoch 13 | loss: 12.50466|  0:00:45s\n",
      "epoch 14 | loss: 11.16755|  0:00:48s\n",
      "epoch 15 | loss: 10.69339|  0:00:51s\n",
      "epoch 16 | loss: 10.4429 |  0:00:55s\n",
      "epoch 17 | loss: 10.15068|  0:00:58s\n",
      "epoch 18 | loss: 10.0256 |  0:01:02s\n",
      "epoch 19 | loss: 9.74638 |  0:01:05s\n",
      "epoch 20 | loss: 9.7209  |  0:01:08s\n",
      "epoch 21 | loss: 9.42836 |  0:01:12s\n",
      "epoch 22 | loss: 9.0464  |  0:01:15s\n",
      "epoch 23 | loss: 9.11215 |  0:01:18s\n",
      "epoch 24 | loss: 8.76802 |  0:01:22s\n",
      "epoch 25 | loss: 8.69874 |  0:01:25s\n",
      "epoch 26 | loss: 8.42403 |  0:01:28s\n",
      "epoch 27 | loss: 8.38341 |  0:01:32s\n",
      "epoch 28 | loss: 9.11594 |  0:01:35s\n",
      "epoch 29 | loss: 8.5125  |  0:01:38s\n",
      "epoch 30 | loss: 8.46511 |  0:01:41s\n",
      "epoch 31 | loss: 8.25942 |  0:01:45s\n",
      "epoch 32 | loss: 7.8514  |  0:01:48s\n",
      "epoch 33 | loss: 7.76784 |  0:01:51s\n",
      "epoch 34 | loss: 7.72087 |  0:01:54s\n",
      "epoch 35 | loss: 7.5985  |  0:01:57s\n",
      "epoch 36 | loss: 7.54681 |  0:02:01s\n",
      "epoch 37 | loss: 7.372   |  0:02:04s\n",
      "epoch 38 | loss: 7.33928 |  0:02:07s\n",
      "epoch 39 | loss: 7.08374 |  0:02:11s\n",
      "epoch 40 | loss: 7.22845 |  0:02:14s\n",
      "epoch 41 | loss: 6.97074 |  0:02:17s\n",
      "epoch 42 | loss: 6.86934 |  0:02:20s\n",
      "epoch 43 | loss: 6.74692 |  0:02:23s\n",
      "epoch 44 | loss: 6.66439 |  0:02:26s\n",
      "epoch 45 | loss: 6.53982 |  0:02:30s\n",
      "epoch 46 | loss: 6.55163 |  0:02:34s\n",
      "epoch 47 | loss: 6.45232 |  0:02:37s\n",
      "epoch 48 | loss: 6.51739 |  0:02:40s\n",
      "epoch 49 | loss: 6.2991  |  0:02:44s\n",
      "epoch 50 | loss: 6.50077 |  0:02:47s\n",
      "epoch 51 | loss: 6.31502 |  0:02:50s\n",
      "epoch 52 | loss: 6.3649  |  0:02:53s\n",
      "epoch 53 | loss: 6.12033 |  0:02:57s\n",
      "epoch 54 | loss: 6.14399 |  0:03:00s\n",
      "epoch 55 | loss: 6.19982 |  0:03:03s\n",
      "epoch 56 | loss: 6.09854 |  0:03:07s\n",
      "epoch 57 | loss: 6.05891 |  0:03:10s\n",
      "epoch 58 | loss: 6.01062 |  0:03:13s\n",
      "epoch 59 | loss: 6.07865 |  0:03:16s\n",
      "epoch 60 | loss: 5.8627  |  0:03:20s\n",
      "epoch 61 | loss: 5.85756 |  0:03:23s\n",
      "epoch 62 | loss: 5.82618 |  0:03:26s\n",
      "epoch 63 | loss: 5.95574 |  0:03:30s\n",
      "epoch 64 | loss: 5.63379 |  0:03:33s\n",
      "epoch 65 | loss: 5.71313 |  0:03:36s\n",
      "epoch 66 | loss: 5.7062  |  0:03:40s\n",
      "epoch 67 | loss: 5.65427 |  0:03:43s\n",
      "epoch 68 | loss: 5.60491 |  0:03:46s\n",
      "epoch 69 | loss: 5.50905 |  0:03:49s\n",
      "epoch 70 | loss: 5.48745 |  0:03:53s\n",
      "epoch 71 | loss: 5.48585 |  0:03:56s\n",
      "epoch 72 | loss: 5.47898 |  0:03:59s\n",
      "epoch 73 | loss: 5.48774 |  0:04:03s\n",
      "epoch 74 | loss: 5.35599 |  0:04:06s\n",
      "epoch 75 | loss: 5.47418 |  0:04:09s\n",
      "epoch 76 | loss: 5.42984 |  0:04:13s\n",
      "epoch 77 | loss: 5.39711 |  0:04:16s\n",
      "epoch 78 | loss: 5.30466 |  0:04:19s\n",
      "epoch 79 | loss: 5.28933 |  0:04:22s\n",
      "epoch 80 | loss: 5.10626 |  0:04:25s\n",
      "epoch 81 | loss: 5.31543 |  0:04:29s\n",
      "epoch 82 | loss: 5.17841 |  0:04:32s\n",
      "epoch 83 | loss: 5.185   |  0:04:35s\n",
      "epoch 84 | loss: 5.26731 |  0:04:39s\n",
      "epoch 85 | loss: 5.16993 |  0:04:42s\n",
      "epoch 86 | loss: 5.14289 |  0:04:46s\n",
      "epoch 87 | loss: 4.9929  |  0:04:49s\n",
      "epoch 88 | loss: 5.10022 |  0:04:52s\n",
      "epoch 89 | loss: 5.03066 |  0:04:55s\n",
      "epoch 90 | loss: 5.12238 |  0:04:58s\n",
      "epoch 91 | loss: 4.99566 |  0:05:02s\n",
      "epoch 92 | loss: 5.22055 |  0:05:05s\n",
      "epoch 93 | loss: 4.95768 |  0:05:08s\n",
      "epoch 94 | loss: 5.10467 |  0:05:12s\n",
      "epoch 95 | loss: 4.98763 |  0:05:15s\n",
      "epoch 96 | loss: 4.88778 |  0:05:19s\n",
      "epoch 97 | loss: 4.88451 |  0:05:22s\n",
      "epoch 98 | loss: 4.91829 |  0:05:25s\n",
      "epoch 99 | loss: 4.92749 |  0:05:28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:50:12,583]\u001b[0m Trial 7 finished with value: 2.700514506308968 and parameters: {'n_d': 14, 'n_a': 61, 'n_steps': 4, 'gamma': 1.5501195964709498, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.04330712383322643}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 173.88186|  0:00:04s\n",
      "epoch 1  | loss: 37.67533|  0:00:09s\n",
      "epoch 2  | loss: 33.34266|  0:00:15s\n",
      "epoch 3  | loss: 25.70518|  0:00:19s\n",
      "epoch 4  | loss: 24.00683|  0:00:24s\n",
      "epoch 5  | loss: 21.61965|  0:00:29s\n",
      "epoch 6  | loss: 19.6208 |  0:00:34s\n",
      "epoch 7  | loss: 19.85231|  0:00:39s\n",
      "epoch 8  | loss: 19.36683|  0:00:44s\n",
      "epoch 9  | loss: 20.37662|  0:00:49s\n",
      "epoch 10 | loss: 19.6611 |  0:00:54s\n",
      "epoch 11 | loss: 17.91272|  0:00:59s\n",
      "epoch 12 | loss: 17.39602|  0:01:04s\n",
      "epoch 13 | loss: 16.58299|  0:01:08s\n",
      "epoch 14 | loss: 18.29407|  0:01:13s\n",
      "epoch 15 | loss: 18.4778 |  0:01:19s\n",
      "epoch 16 | loss: 16.0748 |  0:01:23s\n",
      "epoch 17 | loss: 14.89122|  0:01:28s\n",
      "epoch 18 | loss: 14.36076|  0:01:33s\n",
      "epoch 19 | loss: 14.14625|  0:01:38s\n",
      "epoch 20 | loss: 13.38892|  0:01:43s\n",
      "epoch 21 | loss: 13.25079|  0:01:48s\n",
      "epoch 22 | loss: 13.12424|  0:01:53s\n",
      "epoch 23 | loss: 13.2975 |  0:01:58s\n",
      "epoch 24 | loss: 13.4762 |  0:02:03s\n",
      "epoch 25 | loss: 13.48194|  0:02:08s\n",
      "epoch 26 | loss: 12.94831|  0:02:13s\n",
      "epoch 27 | loss: 12.84248|  0:02:18s\n",
      "epoch 28 | loss: 12.92118|  0:02:23s\n",
      "epoch 29 | loss: 12.9879 |  0:02:28s\n",
      "epoch 30 | loss: 12.73367|  0:02:33s\n",
      "epoch 31 | loss: 12.64725|  0:02:37s\n",
      "epoch 32 | loss: 12.62109|  0:02:43s\n",
      "epoch 33 | loss: 12.89092|  0:02:47s\n",
      "epoch 34 | loss: 12.85896|  0:02:53s\n",
      "epoch 35 | loss: 13.17637|  0:02:58s\n",
      "epoch 36 | loss: 12.90027|  0:03:02s\n",
      "epoch 37 | loss: 12.67432|  0:03:07s\n",
      "epoch 38 | loss: 12.55549|  0:03:12s\n",
      "epoch 39 | loss: 12.59328|  0:03:17s\n",
      "epoch 40 | loss: 12.5    |  0:03:22s\n",
      "epoch 41 | loss: 12.55472|  0:03:27s\n",
      "epoch 42 | loss: 13.15326|  0:03:32s\n",
      "epoch 43 | loss: 12.21883|  0:03:37s\n",
      "epoch 44 | loss: 11.82125|  0:03:42s\n",
      "epoch 45 | loss: 11.74383|  0:03:47s\n",
      "epoch 46 | loss: 11.29271|  0:03:52s\n",
      "epoch 47 | loss: 11.23542|  0:03:57s\n",
      "epoch 48 | loss: 10.90445|  0:04:02s\n",
      "epoch 49 | loss: 10.98248|  0:04:07s\n",
      "epoch 50 | loss: 10.41601|  0:04:12s\n",
      "epoch 51 | loss: 10.15429|  0:04:17s\n",
      "epoch 52 | loss: 10.13975|  0:04:22s\n",
      "epoch 53 | loss: 10.12294|  0:04:27s\n",
      "epoch 54 | loss: 9.61696 |  0:04:32s\n",
      "epoch 55 | loss: 9.45883 |  0:04:37s\n",
      "epoch 56 | loss: 9.04794 |  0:04:41s\n",
      "epoch 57 | loss: 8.98346 |  0:04:46s\n",
      "epoch 58 | loss: 8.74036 |  0:04:51s\n",
      "epoch 59 | loss: 8.62051 |  0:04:57s\n",
      "epoch 60 | loss: 8.55712 |  0:05:01s\n",
      "epoch 61 | loss: 8.29572 |  0:05:06s\n",
      "epoch 62 | loss: 8.15826 |  0:05:11s\n",
      "epoch 63 | loss: 8.26772 |  0:05:16s\n",
      "epoch 64 | loss: 8.09577 |  0:05:21s\n",
      "epoch 65 | loss: 8.03395 |  0:05:26s\n",
      "epoch 66 | loss: 7.86397 |  0:05:31s\n",
      "epoch 67 | loss: 7.8366  |  0:05:36s\n",
      "epoch 68 | loss: 7.7121  |  0:05:41s\n",
      "epoch 69 | loss: 7.50908 |  0:05:46s\n",
      "epoch 70 | loss: 7.46295 |  0:05:51s\n",
      "epoch 71 | loss: 7.29183 |  0:05:55s\n",
      "epoch 72 | loss: 7.35087 |  0:06:01s\n",
      "epoch 73 | loss: 7.60786 |  0:06:05s\n",
      "epoch 74 | loss: 7.20904 |  0:06:10s\n",
      "epoch 75 | loss: 7.04188 |  0:06:15s\n",
      "epoch 76 | loss: 6.96662 |  0:06:20s\n",
      "epoch 77 | loss: 6.89599 |  0:06:25s\n",
      "epoch 78 | loss: 6.78917 |  0:06:30s\n",
      "epoch 79 | loss: 6.69266 |  0:06:35s\n",
      "epoch 80 | loss: 6.6387  |  0:06:40s\n",
      "epoch 81 | loss: 6.7176  |  0:06:45s\n",
      "epoch 82 | loss: 6.64048 |  0:06:50s\n",
      "epoch 83 | loss: 6.51948 |  0:06:55s\n",
      "epoch 84 | loss: 6.46892 |  0:07:00s\n",
      "epoch 85 | loss: 6.47884 |  0:07:05s\n",
      "epoch 86 | loss: 6.42462 |  0:07:10s\n",
      "epoch 87 | loss: 6.29369 |  0:07:15s\n",
      "epoch 88 | loss: 6.32723 |  0:07:20s\n",
      "epoch 89 | loss: 6.36086 |  0:07:24s\n",
      "epoch 90 | loss: 6.11724 |  0:07:29s\n",
      "epoch 91 | loss: 6.15279 |  0:07:34s\n",
      "epoch 92 | loss: 6.75932 |  0:07:39s\n",
      "epoch 93 | loss: 6.29623 |  0:07:44s\n",
      "epoch 94 | loss: 6.24137 |  0:07:49s\n",
      "epoch 95 | loss: 6.27019 |  0:07:54s\n",
      "epoch 96 | loss: 6.22872 |  0:07:59s\n",
      "epoch 97 | loss: 6.04981 |  0:08:04s\n",
      "epoch 98 | loss: 6.11466 |  0:08:09s\n",
      "epoch 99 | loss: 5.97962 |  0:08:14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 09:58:30,664]\u001b[0m Trial 8 finished with value: 5.210812452198318 and parameters: {'n_d': 11, 'n_a': 55, 'n_steps': 6, 'gamma': 1.0920357690025664, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.30362615524927633}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 111.48321|  0:00:04s\n",
      "epoch 1  | loss: 51.44964|  0:00:08s\n",
      "epoch 2  | loss: 65.02232|  0:00:12s\n",
      "epoch 3  | loss: 39.89786|  0:00:16s\n",
      "epoch 4  | loss: 26.20887|  0:00:20s\n",
      "epoch 5  | loss: 20.93564|  0:00:24s\n",
      "epoch 6  | loss: 17.84593|  0:00:28s\n",
      "epoch 7  | loss: 15.59482|  0:00:33s\n",
      "epoch 8  | loss: 15.00994|  0:00:36s\n",
      "epoch 9  | loss: 13.75733|  0:00:41s\n",
      "epoch 10 | loss: 13.43668|  0:00:45s\n",
      "epoch 11 | loss: 12.19396|  0:00:49s\n",
      "epoch 12 | loss: 11.86965|  0:00:53s\n",
      "epoch 13 | loss: 11.3619 |  0:00:57s\n",
      "epoch 14 | loss: 10.80986|  0:01:02s\n",
      "epoch 15 | loss: 10.74572|  0:01:06s\n",
      "epoch 16 | loss: 10.42567|  0:01:10s\n",
      "epoch 17 | loss: 10.63426|  0:01:14s\n",
      "epoch 18 | loss: 10.25798|  0:01:18s\n",
      "epoch 19 | loss: 10.10551|  0:01:22s\n",
      "epoch 20 | loss: 10.58165|  0:01:26s\n",
      "epoch 21 | loss: 10.24877|  0:01:30s\n",
      "epoch 22 | loss: 10.03517|  0:01:34s\n",
      "epoch 23 | loss: 10.03337|  0:01:38s\n",
      "epoch 24 | loss: 9.894   |  0:01:42s\n",
      "epoch 25 | loss: 9.4937  |  0:01:46s\n",
      "epoch 26 | loss: 9.38385 |  0:01:51s\n",
      "epoch 27 | loss: 9.3413  |  0:01:55s\n",
      "epoch 28 | loss: 9.2606  |  0:01:59s\n",
      "epoch 29 | loss: 9.0239  |  0:02:03s\n",
      "epoch 30 | loss: 8.89755 |  0:02:07s\n",
      "epoch 31 | loss: 8.99936 |  0:02:11s\n",
      "epoch 32 | loss: 9.00659 |  0:02:15s\n",
      "epoch 33 | loss: 8.73252 |  0:02:19s\n",
      "epoch 34 | loss: 8.41822 |  0:02:24s\n",
      "epoch 35 | loss: 8.37304 |  0:02:28s\n",
      "epoch 36 | loss: 8.28363 |  0:02:32s\n",
      "epoch 37 | loss: 8.23358 |  0:02:36s\n",
      "epoch 38 | loss: 8.05812 |  0:02:40s\n",
      "epoch 39 | loss: 7.9433  |  0:02:44s\n",
      "epoch 40 | loss: 8.16826 |  0:02:48s\n",
      "epoch 41 | loss: 7.89576 |  0:02:52s\n",
      "epoch 42 | loss: 8.13756 |  0:02:56s\n",
      "epoch 43 | loss: 7.77131 |  0:03:00s\n",
      "epoch 44 | loss: 7.68621 |  0:03:05s\n",
      "epoch 45 | loss: 7.57855 |  0:03:09s\n",
      "epoch 46 | loss: 7.59925 |  0:03:13s\n",
      "epoch 47 | loss: 7.60407 |  0:03:17s\n",
      "epoch 48 | loss: 7.54524 |  0:03:21s\n",
      "epoch 49 | loss: 7.4452  |  0:03:25s\n",
      "epoch 50 | loss: 7.36268 |  0:03:29s\n",
      "epoch 51 | loss: 7.10234 |  0:03:33s\n",
      "epoch 52 | loss: 7.07939 |  0:03:37s\n",
      "epoch 53 | loss: 6.99838 |  0:03:41s\n",
      "epoch 54 | loss: 6.96357 |  0:03:46s\n",
      "epoch 55 | loss: 7.0129  |  0:03:49s\n",
      "epoch 56 | loss: 6.66489 |  0:03:53s\n",
      "epoch 57 | loss: 6.68488 |  0:03:58s\n",
      "epoch 58 | loss: 6.85144 |  0:04:02s\n",
      "epoch 59 | loss: 6.74958 |  0:04:06s\n",
      "epoch 60 | loss: 6.74048 |  0:04:10s\n",
      "epoch 61 | loss: 6.54549 |  0:04:14s\n",
      "epoch 62 | loss: 6.62774 |  0:04:18s\n",
      "epoch 63 | loss: 6.43144 |  0:04:22s\n",
      "epoch 64 | loss: 6.30305 |  0:04:26s\n",
      "epoch 65 | loss: 6.51983 |  0:04:31s\n",
      "epoch 66 | loss: 6.28326 |  0:04:35s\n",
      "epoch 67 | loss: 6.27565 |  0:04:39s\n",
      "epoch 68 | loss: 6.10689 |  0:04:43s\n",
      "epoch 69 | loss: 6.11042 |  0:04:47s\n",
      "epoch 70 | loss: 6.02568 |  0:04:51s\n",
      "epoch 71 | loss: 6.0652  |  0:04:55s\n",
      "epoch 72 | loss: 5.94123 |  0:04:59s\n",
      "epoch 73 | loss: 5.84978 |  0:05:03s\n",
      "epoch 74 | loss: 5.96036 |  0:05:08s\n",
      "epoch 75 | loss: 6.0479  |  0:05:12s\n",
      "epoch 76 | loss: 6.13793 |  0:05:16s\n",
      "epoch 77 | loss: 5.93154 |  0:05:20s\n",
      "epoch 78 | loss: 5.85242 |  0:05:24s\n",
      "epoch 79 | loss: 5.77145 |  0:05:28s\n",
      "epoch 80 | loss: 5.72865 |  0:05:32s\n",
      "epoch 81 | loss: 5.82461 |  0:05:36s\n",
      "epoch 82 | loss: 5.66092 |  0:05:41s\n",
      "epoch 83 | loss: 5.77992 |  0:05:45s\n",
      "epoch 84 | loss: 5.60681 |  0:05:49s\n",
      "epoch 85 | loss: 5.44958 |  0:05:53s\n",
      "epoch 86 | loss: 5.5287  |  0:05:57s\n",
      "epoch 87 | loss: 5.61546 |  0:06:01s\n",
      "epoch 88 | loss: 5.86442 |  0:06:05s\n",
      "epoch 89 | loss: 5.66533 |  0:06:09s\n",
      "epoch 90 | loss: 5.59545 |  0:06:13s\n",
      "epoch 91 | loss: 5.61436 |  0:06:17s\n",
      "epoch 92 | loss: 5.36167 |  0:06:21s\n",
      "epoch 93 | loss: 5.4054  |  0:06:25s\n",
      "epoch 94 | loss: 5.51356 |  0:06:29s\n",
      "epoch 95 | loss: 5.44745 |  0:06:34s\n",
      "epoch 96 | loss: 5.57761 |  0:06:38s\n",
      "epoch 97 | loss: 5.76095 |  0:06:42s\n",
      "epoch 98 | loss: 5.39612 |  0:06:46s\n",
      "epoch 99 | loss: 5.16264 |  0:06:50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 10:05:24,976]\u001b[0m Trial 9 finished with value: 2.5302215500813827 and parameters: {'n_d': 63, 'n_a': 9, 'n_steps': 8, 'gamma': 1.7167874121238436, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.10543011590009474}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 122.83776|  0:00:03s\n",
      "epoch 1  | loss: 49.67267|  0:00:06s\n",
      "epoch 2  | loss: 47.91929|  0:00:11s\n",
      "epoch 3  | loss: 45.13316|  0:00:14s\n",
      "epoch 4  | loss: 31.56455|  0:00:18s\n",
      "epoch 5  | loss: 21.34093|  0:00:22s\n",
      "epoch 6  | loss: 16.22592|  0:00:25s\n",
      "epoch 7  | loss: 15.22104|  0:00:29s\n",
      "epoch 8  | loss: 14.08538|  0:00:33s\n",
      "epoch 9  | loss: 13.10694|  0:00:36s\n",
      "epoch 10 | loss: 12.48199|  0:00:40s\n",
      "epoch 11 | loss: 12.16485|  0:00:44s\n",
      "epoch 12 | loss: 11.40634|  0:00:48s\n",
      "epoch 13 | loss: 11.00995|  0:00:51s\n",
      "epoch 14 | loss: 10.89858|  0:00:55s\n",
      "epoch 15 | loss: 10.49202|  0:00:58s\n",
      "epoch 16 | loss: 10.33328|  0:01:02s\n",
      "epoch 17 | loss: 10.36226|  0:01:06s\n",
      "epoch 18 | loss: 10.12015|  0:01:09s\n",
      "epoch 19 | loss: 9.89986 |  0:01:13s\n",
      "epoch 20 | loss: 9.59705 |  0:01:17s\n",
      "epoch 21 | loss: 9.45038 |  0:01:21s\n",
      "epoch 22 | loss: 9.55903 |  0:01:24s\n",
      "epoch 23 | loss: 9.08793 |  0:01:28s\n",
      "epoch 24 | loss: 9.13928 |  0:01:32s\n",
      "epoch 25 | loss: 9.22974 |  0:01:35s\n",
      "epoch 26 | loss: 9.22854 |  0:01:39s\n",
      "epoch 27 | loss: 8.88021 |  0:01:42s\n",
      "epoch 28 | loss: 8.74721 |  0:01:46s\n",
      "epoch 29 | loss: 8.63984 |  0:01:50s\n",
      "epoch 30 | loss: 8.76464 |  0:01:54s\n",
      "epoch 31 | loss: 8.42899 |  0:01:57s\n",
      "epoch 32 | loss: 8.52197 |  0:02:01s\n",
      "epoch 33 | loss: 8.14064 |  0:02:05s\n",
      "epoch 34 | loss: 8.64979 |  0:02:08s\n",
      "epoch 35 | loss: 8.06197 |  0:02:12s\n",
      "epoch 36 | loss: 8.10689 |  0:02:16s\n",
      "epoch 37 | loss: 7.97859 |  0:02:20s\n",
      "epoch 38 | loss: 7.76281 |  0:02:24s\n",
      "epoch 39 | loss: 7.69016 |  0:02:27s\n",
      "epoch 40 | loss: 7.86303 |  0:02:31s\n",
      "epoch 41 | loss: 7.88092 |  0:02:35s\n",
      "epoch 42 | loss: 7.46643 |  0:02:38s\n",
      "epoch 43 | loss: 7.64393 |  0:02:42s\n",
      "epoch 44 | loss: 7.60228 |  0:02:46s\n",
      "epoch 45 | loss: 7.41425 |  0:02:50s\n",
      "epoch 46 | loss: 7.38297 |  0:02:53s\n",
      "epoch 47 | loss: 7.26272 |  0:02:57s\n",
      "epoch 48 | loss: 7.13292 |  0:03:01s\n",
      "epoch 49 | loss: 7.25255 |  0:03:04s\n",
      "epoch 50 | loss: 7.07565 |  0:03:08s\n",
      "epoch 51 | loss: 6.90901 |  0:03:12s\n",
      "epoch 52 | loss: 6.86461 |  0:03:15s\n",
      "epoch 53 | loss: 7.08318 |  0:03:19s\n",
      "epoch 54 | loss: 6.9469  |  0:03:23s\n",
      "epoch 55 | loss: 6.70309 |  0:03:27s\n",
      "epoch 56 | loss: 6.75901 |  0:03:30s\n",
      "epoch 57 | loss: 6.60632 |  0:03:34s\n",
      "epoch 58 | loss: 6.65032 |  0:03:37s\n",
      "epoch 59 | loss: 6.58614 |  0:03:41s\n",
      "epoch 60 | loss: 6.55043 |  0:03:45s\n",
      "epoch 61 | loss: 6.40973 |  0:03:48s\n",
      "epoch 62 | loss: 6.40339 |  0:03:53s\n",
      "epoch 63 | loss: 6.57287 |  0:03:56s\n",
      "epoch 64 | loss: 6.43986 |  0:04:00s\n",
      "epoch 65 | loss: 6.39339 |  0:04:04s\n",
      "epoch 66 | loss: 6.30304 |  0:04:07s\n",
      "epoch 67 | loss: 6.53895 |  0:04:11s\n",
      "epoch 68 | loss: 6.15918 |  0:04:15s\n",
      "epoch 69 | loss: 6.24814 |  0:04:18s\n",
      "epoch 70 | loss: 6.25159 |  0:04:22s\n",
      "epoch 71 | loss: 6.30102 |  0:04:26s\n",
      "epoch 72 | loss: 6.09841 |  0:04:29s\n",
      "epoch 73 | loss: 6.07621 |  0:04:33s\n",
      "epoch 74 | loss: 5.97993 |  0:04:37s\n",
      "epoch 75 | loss: 6.01235 |  0:04:41s\n",
      "epoch 76 | loss: 6.11426 |  0:04:44s\n",
      "epoch 77 | loss: 6.11427 |  0:04:48s\n",
      "epoch 78 | loss: 5.95183 |  0:04:51s\n",
      "epoch 79 | loss: 5.96118 |  0:04:56s\n",
      "epoch 80 | loss: 6.19074 |  0:04:59s\n",
      "epoch 81 | loss: 5.81647 |  0:05:03s\n",
      "epoch 82 | loss: 5.72596 |  0:05:07s\n",
      "epoch 83 | loss: 5.69169 |  0:05:10s\n",
      "epoch 84 | loss: 6.00991 |  0:05:14s\n",
      "epoch 85 | loss: 6.03873 |  0:05:18s\n",
      "epoch 86 | loss: 5.95166 |  0:05:21s\n",
      "epoch 87 | loss: 5.75005 |  0:05:25s\n",
      "epoch 88 | loss: 5.73033 |  0:05:29s\n",
      "epoch 89 | loss: 5.69167 |  0:05:33s\n",
      "epoch 90 | loss: 5.69992 |  0:05:36s\n",
      "epoch 91 | loss: 5.46493 |  0:05:40s\n",
      "epoch 92 | loss: 5.64333 |  0:05:43s\n",
      "epoch 93 | loss: 5.44284 |  0:05:47s\n",
      "epoch 94 | loss: 5.57438 |  0:05:51s\n",
      "epoch 95 | loss: 5.69489 |  0:05:54s\n",
      "epoch 96 | loss: 5.34986 |  0:05:58s\n",
      "epoch 97 | loss: 5.21858 |  0:06:02s\n",
      "epoch 98 | loss: 5.43952 |  0:06:06s\n",
      "epoch 99 | loss: 5.37045 |  0:06:09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 10:11:38,203]\u001b[0m Trial 10 finished with value: 2.849228712913129 and parameters: {'n_d': 43, 'n_a': 33, 'n_steps': 6, 'gamma': 1.9727079907791336, 'n_independent': 5, 'n_shared': 1, 'momentum': 0.3939647664609881}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 122.81785|  0:00:03s\n",
      "epoch 1  | loss: 35.50753|  0:00:06s\n",
      "epoch 2  | loss: 34.99597|  0:00:09s\n",
      "epoch 3  | loss: 27.85227|  0:00:12s\n",
      "epoch 4  | loss: 25.49368|  0:00:15s\n",
      "epoch 5  | loss: 21.97637|  0:00:19s\n",
      "epoch 6  | loss: 19.63952|  0:00:22s\n",
      "epoch 7  | loss: 17.98506|  0:00:25s\n",
      "epoch 8  | loss: 15.1906 |  0:00:28s\n",
      "epoch 9  | loss: 14.14244|  0:00:31s\n",
      "epoch 10 | loss: 14.3001 |  0:00:34s\n",
      "epoch 11 | loss: 13.38247|  0:00:37s\n",
      "epoch 12 | loss: 12.89338|  0:00:40s\n",
      "epoch 13 | loss: 12.34294|  0:00:44s\n",
      "epoch 14 | loss: 12.20405|  0:00:47s\n",
      "epoch 15 | loss: 12.11763|  0:00:50s\n",
      "epoch 16 | loss: 11.2265 |  0:00:54s\n",
      "epoch 17 | loss: 11.12313|  0:00:57s\n",
      "epoch 18 | loss: 10.71744|  0:01:00s\n",
      "epoch 19 | loss: 10.45756|  0:01:03s\n",
      "epoch 20 | loss: 10.31383|  0:01:06s\n",
      "epoch 21 | loss: 9.97132 |  0:01:09s\n",
      "epoch 22 | loss: 10.15213|  0:01:13s\n",
      "epoch 23 | loss: 10.03476|  0:01:16s\n",
      "epoch 24 | loss: 9.8288  |  0:01:19s\n",
      "epoch 25 | loss: 10.05989|  0:01:22s\n",
      "epoch 26 | loss: 10.0721 |  0:01:26s\n",
      "epoch 27 | loss: 9.91092 |  0:01:29s\n",
      "epoch 28 | loss: 9.53419 |  0:01:32s\n",
      "epoch 29 | loss: 9.38884 |  0:01:35s\n",
      "epoch 30 | loss: 9.22806 |  0:01:38s\n",
      "epoch 31 | loss: 9.20679 |  0:01:41s\n",
      "epoch 32 | loss: 8.77117 |  0:01:44s\n",
      "epoch 33 | loss: 8.62694 |  0:01:48s\n",
      "epoch 34 | loss: 8.51341 |  0:01:51s\n",
      "epoch 35 | loss: 8.4411  |  0:01:54s\n",
      "epoch 36 | loss: 8.31661 |  0:01:57s\n",
      "epoch 37 | loss: 8.6303  |  0:02:01s\n",
      "epoch 38 | loss: 8.15402 |  0:02:04s\n",
      "epoch 39 | loss: 7.98891 |  0:02:07s\n",
      "epoch 40 | loss: 7.69227 |  0:02:10s\n",
      "epoch 41 | loss: 7.65955 |  0:02:13s\n",
      "epoch 42 | loss: 7.50076 |  0:02:16s\n",
      "epoch 43 | loss: 7.75099 |  0:02:19s\n",
      "epoch 44 | loss: 7.42505 |  0:02:23s\n",
      "epoch 45 | loss: 7.29248 |  0:02:26s\n",
      "epoch 46 | loss: 7.22627 |  0:02:29s\n",
      "epoch 47 | loss: 7.16473 |  0:02:33s\n",
      "epoch 48 | loss: 6.99633 |  0:02:36s\n",
      "epoch 49 | loss: 6.95589 |  0:02:39s\n",
      "epoch 50 | loss: 6.68286 |  0:02:42s\n",
      "epoch 51 | loss: 6.7413  |  0:02:45s\n",
      "epoch 52 | loss: 6.68008 |  0:02:49s\n",
      "epoch 53 | loss: 6.71427 |  0:02:52s\n",
      "epoch 54 | loss: 6.42306 |  0:02:56s\n",
      "epoch 55 | loss: 6.44544 |  0:02:59s\n",
      "epoch 56 | loss: 6.41032 |  0:03:02s\n",
      "epoch 57 | loss: 6.32273 |  0:03:05s\n",
      "epoch 58 | loss: 6.57897 |  0:03:08s\n",
      "epoch 59 | loss: 6.41886 |  0:03:11s\n",
      "epoch 60 | loss: 6.23929 |  0:03:14s\n",
      "epoch 61 | loss: 6.08122 |  0:03:18s\n",
      "epoch 62 | loss: 6.12898 |  0:03:21s\n",
      "epoch 63 | loss: 6.05093 |  0:03:24s\n",
      "epoch 64 | loss: 6.02888 |  0:03:27s\n",
      "epoch 65 | loss: 6.00896 |  0:03:31s\n",
      "epoch 66 | loss: 5.91899 |  0:03:34s\n",
      "epoch 67 | loss: 5.80504 |  0:03:37s\n",
      "epoch 68 | loss: 5.97357 |  0:03:40s\n",
      "epoch 69 | loss: 5.78637 |  0:03:43s\n",
      "epoch 70 | loss: 5.82853 |  0:03:46s\n",
      "epoch 71 | loss: 5.77874 |  0:03:49s\n",
      "epoch 72 | loss: 5.7776  |  0:03:53s\n",
      "epoch 73 | loss: 5.76044 |  0:03:56s\n",
      "epoch 74 | loss: 5.87683 |  0:03:59s\n",
      "epoch 75 | loss: 5.62186 |  0:04:03s\n",
      "epoch 76 | loss: 5.55151 |  0:04:06s\n",
      "epoch 77 | loss: 5.55546 |  0:04:09s\n",
      "epoch 78 | loss: 5.51934 |  0:04:12s\n",
      "epoch 79 | loss: 5.40329 |  0:04:15s\n",
      "epoch 80 | loss: 5.44274 |  0:04:18s\n",
      "epoch 81 | loss: 5.48815 |  0:04:21s\n",
      "epoch 82 | loss: 5.59249 |  0:04:24s\n",
      "epoch 83 | loss: 5.29411 |  0:04:28s\n",
      "epoch 84 | loss: 5.41449 |  0:04:31s\n",
      "epoch 85 | loss: 5.37616 |  0:04:34s\n",
      "epoch 86 | loss: 5.31469 |  0:04:37s\n",
      "epoch 87 | loss: 5.207   |  0:04:41s\n",
      "epoch 88 | loss: 5.44599 |  0:04:44s\n",
      "epoch 89 | loss: 5.4014  |  0:04:47s\n",
      "epoch 90 | loss: 5.17512 |  0:04:51s\n",
      "epoch 91 | loss: 5.27388 |  0:04:54s\n",
      "epoch 92 | loss: 5.12398 |  0:04:58s\n",
      "epoch 93 | loss: 5.34113 |  0:05:02s\n",
      "epoch 94 | loss: 5.16391 |  0:05:05s\n",
      "epoch 95 | loss: 5.15608 |  0:05:09s\n",
      "epoch 96 | loss: 5.23381 |  0:05:12s\n",
      "epoch 97 | loss: 5.2217  |  0:05:16s\n",
      "epoch 98 | loss: 5.21436 |  0:05:19s\n",
      "epoch 99 | loss: 5.05828 |  0:05:23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 10:17:04,347]\u001b[0m Trial 11 finished with value: 2.547158879001383 and parameters: {'n_d': 25, 'n_a': 44, 'n_steps': 5, 'gamma': 1.3956974172571235, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.13507246435285994}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 115.00605|  0:00:02s\n",
      "epoch 1  | loss: 37.63973|  0:00:05s\n",
      "epoch 2  | loss: 36.98643|  0:00:07s\n",
      "epoch 3  | loss: 26.24277|  0:00:10s\n",
      "epoch 4  | loss: 23.28622|  0:00:12s\n",
      "epoch 5  | loss: 33.57708|  0:00:14s\n",
      "epoch 6  | loss: 19.86547|  0:00:17s\n",
      "epoch 7  | loss: 17.39549|  0:00:19s\n",
      "epoch 8  | loss: 16.22637|  0:00:21s\n",
      "epoch 9  | loss: 14.51348|  0:00:24s\n",
      "epoch 10 | loss: 13.23294|  0:00:26s\n",
      "epoch 11 | loss: 12.64704|  0:00:29s\n",
      "epoch 12 | loss: 12.37034|  0:00:31s\n",
      "epoch 13 | loss: 11.97733|  0:00:34s\n",
      "epoch 14 | loss: 11.70109|  0:00:37s\n",
      "epoch 15 | loss: 11.27475|  0:00:39s\n",
      "epoch 16 | loss: 11.33357|  0:00:41s\n",
      "epoch 17 | loss: 10.7506 |  0:00:44s\n",
      "epoch 18 | loss: 10.78922|  0:00:46s\n",
      "epoch 19 | loss: 10.4202 |  0:00:48s\n",
      "epoch 20 | loss: 10.31323|  0:00:51s\n",
      "epoch 21 | loss: 9.79475 |  0:00:53s\n",
      "epoch 22 | loss: 9.92828 |  0:00:55s\n",
      "epoch 23 | loss: 9.74572 |  0:00:58s\n",
      "epoch 24 | loss: 9.80177 |  0:01:00s\n",
      "epoch 25 | loss: 9.50323 |  0:01:03s\n",
      "epoch 26 | loss: 9.27196 |  0:01:05s\n",
      "epoch 27 | loss: 9.22287 |  0:01:08s\n",
      "epoch 28 | loss: 8.89798 |  0:01:10s\n",
      "epoch 29 | loss: 8.93715 |  0:01:13s\n",
      "epoch 30 | loss: 8.71224 |  0:01:15s\n",
      "epoch 31 | loss: 8.68631 |  0:01:17s\n",
      "epoch 32 | loss: 8.36859 |  0:01:20s\n",
      "epoch 33 | loss: 8.3682  |  0:01:22s\n",
      "epoch 34 | loss: 8.18651 |  0:01:25s\n",
      "epoch 35 | loss: 8.02152 |  0:01:27s\n",
      "epoch 36 | loss: 8.58031 |  0:01:29s\n",
      "epoch 37 | loss: 8.23947 |  0:01:32s\n",
      "epoch 38 | loss: 8.44752 |  0:01:34s\n",
      "epoch 39 | loss: 8.32567 |  0:01:37s\n",
      "epoch 40 | loss: 7.99549 |  0:01:39s\n",
      "epoch 41 | loss: 8.0769  |  0:01:42s\n",
      "epoch 42 | loss: 7.78677 |  0:01:44s\n",
      "epoch 43 | loss: 7.76004 |  0:01:47s\n",
      "epoch 44 | loss: 7.72463 |  0:01:49s\n",
      "epoch 45 | loss: 7.71459 |  0:01:51s\n",
      "epoch 46 | loss: 7.76969 |  0:01:54s\n",
      "epoch 47 | loss: 7.48753 |  0:01:56s\n",
      "epoch 48 | loss: 7.6524  |  0:01:59s\n",
      "epoch 49 | loss: 7.26542 |  0:02:01s\n",
      "epoch 50 | loss: 7.20582 |  0:02:03s\n",
      "epoch 51 | loss: 7.0826  |  0:02:06s\n",
      "epoch 52 | loss: 7.1878  |  0:02:08s\n",
      "epoch 53 | loss: 6.94359 |  0:02:11s\n",
      "epoch 54 | loss: 7.02021 |  0:02:14s\n",
      "epoch 55 | loss: 6.96267 |  0:02:16s\n",
      "epoch 56 | loss: 6.92737 |  0:02:19s\n",
      "epoch 57 | loss: 6.83577 |  0:02:21s\n",
      "epoch 58 | loss: 6.7805  |  0:02:23s\n",
      "epoch 59 | loss: 6.59222 |  0:02:26s\n",
      "epoch 60 | loss: 6.70373 |  0:02:28s\n",
      "epoch 61 | loss: 6.58906 |  0:02:31s\n",
      "epoch 62 | loss: 6.78501 |  0:02:33s\n",
      "epoch 63 | loss: 6.48582 |  0:02:35s\n",
      "epoch 64 | loss: 6.42278 |  0:02:38s\n",
      "epoch 65 | loss: 6.36384 |  0:02:40s\n",
      "epoch 66 | loss: 6.5013  |  0:02:43s\n",
      "epoch 67 | loss: 6.38269 |  0:02:45s\n",
      "epoch 68 | loss: 6.40276 |  0:02:48s\n",
      "epoch 69 | loss: 6.2664  |  0:02:50s\n",
      "epoch 70 | loss: 6.34745 |  0:02:53s\n",
      "epoch 71 | loss: 6.14933 |  0:02:55s\n",
      "epoch 72 | loss: 6.0966  |  0:02:57s\n",
      "epoch 73 | loss: 6.16482 |  0:03:00s\n",
      "epoch 74 | loss: 6.28465 |  0:03:02s\n",
      "epoch 75 | loss: 6.09756 |  0:03:05s\n",
      "epoch 76 | loss: 6.01426 |  0:03:07s\n",
      "epoch 77 | loss: 5.95939 |  0:03:10s\n",
      "epoch 78 | loss: 6.01731 |  0:03:12s\n",
      "epoch 79 | loss: 6.04687 |  0:03:15s\n",
      "epoch 80 | loss: 6.26845 |  0:03:17s\n",
      "epoch 81 | loss: 6.37901 |  0:03:19s\n",
      "epoch 82 | loss: 6.21642 |  0:03:22s\n",
      "epoch 83 | loss: 6.31223 |  0:03:24s\n",
      "epoch 84 | loss: 6.02395 |  0:03:27s\n",
      "epoch 85 | loss: 6.24371 |  0:03:29s\n",
      "epoch 86 | loss: 6.14367 |  0:03:31s\n",
      "epoch 87 | loss: 5.87282 |  0:03:34s\n",
      "epoch 88 | loss: 6.01928 |  0:03:36s\n",
      "epoch 89 | loss: 5.98975 |  0:03:39s\n",
      "epoch 90 | loss: 5.69344 |  0:03:41s\n",
      "epoch 91 | loss: 5.61931 |  0:03:44s\n",
      "epoch 92 | loss: 5.72112 |  0:03:46s\n",
      "epoch 93 | loss: 5.72872 |  0:03:49s\n",
      "epoch 94 | loss: 5.69653 |  0:03:51s\n",
      "epoch 95 | loss: 5.76415 |  0:03:54s\n",
      "epoch 96 | loss: 5.59153 |  0:03:56s\n",
      "epoch 97 | loss: 5.52416 |  0:03:58s\n",
      "epoch 98 | loss: 5.6117  |  0:04:01s\n",
      "epoch 99 | loss: 5.4718  |  0:04:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 10:21:10,441]\u001b[0m Trial 12 finished with value: 2.6275597370780512 and parameters: {'n_d': 39, 'n_a': 35, 'n_steps': 5, 'gamma': 1.4518025600713935, 'n_independent': 3, 'n_shared': 1, 'momentum': 0.08243781395273325}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 126.38581|  0:00:04s\n",
      "epoch 1  | loss: 56.89272|  0:00:08s\n",
      "epoch 2  | loss: 49.87949|  0:00:13s\n",
      "epoch 3  | loss: 43.66975|  0:00:18s\n",
      "epoch 4  | loss: 37.63209|  0:00:23s\n",
      "epoch 5  | loss: 34.0862 |  0:00:27s\n",
      "epoch 6  | loss: 32.2073 |  0:00:32s\n",
      "epoch 7  | loss: 25.1851 |  0:00:36s\n",
      "epoch 8  | loss: 19.89346|  0:00:40s\n",
      "epoch 9  | loss: 17.76965|  0:00:45s\n",
      "epoch 10 | loss: 14.58615|  0:00:50s\n",
      "epoch 11 | loss: 13.50336|  0:00:55s\n",
      "epoch 12 | loss: 12.96311|  0:00:59s\n",
      "epoch 13 | loss: 12.32235|  0:01:04s\n",
      "epoch 14 | loss: 11.61338|  0:01:08s\n",
      "epoch 15 | loss: 11.21315|  0:01:13s\n",
      "epoch 16 | loss: 10.90913|  0:01:18s\n",
      "epoch 17 | loss: 10.65224|  0:01:22s\n",
      "epoch 18 | loss: 10.21258|  0:01:27s\n",
      "epoch 19 | loss: 10.34734|  0:01:31s\n",
      "epoch 20 | loss: 9.89223 |  0:01:36s\n",
      "epoch 21 | loss: 9.70824 |  0:01:40s\n",
      "epoch 22 | loss: 9.59518 |  0:01:45s\n",
      "epoch 23 | loss: 9.30406 |  0:01:50s\n",
      "epoch 24 | loss: 9.30365 |  0:01:54s\n",
      "epoch 25 | loss: 9.08422 |  0:01:59s\n",
      "epoch 26 | loss: 8.87007 |  0:02:03s\n",
      "epoch 27 | loss: 8.87127 |  0:02:08s\n",
      "epoch 28 | loss: 8.7394  |  0:02:12s\n",
      "epoch 29 | loss: 8.3693  |  0:02:17s\n",
      "epoch 30 | loss: 8.1725  |  0:02:22s\n",
      "epoch 31 | loss: 8.15149 |  0:02:26s\n",
      "epoch 32 | loss: 8.1271  |  0:02:31s\n",
      "epoch 33 | loss: 7.9814  |  0:02:35s\n",
      "epoch 34 | loss: 7.97828 |  0:02:40s\n",
      "epoch 35 | loss: 7.9433  |  0:02:44s\n",
      "epoch 36 | loss: 8.00464 |  0:02:49s\n",
      "epoch 37 | loss: 7.58196 |  0:02:54s\n",
      "epoch 38 | loss: 7.54197 |  0:02:58s\n",
      "epoch 39 | loss: 7.62373 |  0:03:03s\n",
      "epoch 40 | loss: 7.41049 |  0:03:07s\n",
      "epoch 41 | loss: 7.40703 |  0:03:12s\n",
      "epoch 42 | loss: 7.20783 |  0:03:16s\n",
      "epoch 43 | loss: 7.27202 |  0:03:21s\n",
      "epoch 44 | loss: 7.05682 |  0:03:26s\n",
      "epoch 45 | loss: 6.95331 |  0:03:31s\n",
      "epoch 46 | loss: 6.82258 |  0:03:35s\n",
      "epoch 47 | loss: 7.10593 |  0:03:40s\n",
      "epoch 48 | loss: 6.93971 |  0:03:44s\n",
      "epoch 49 | loss: 6.8616  |  0:03:48s\n",
      "epoch 50 | loss: 6.7326  |  0:03:53s\n",
      "epoch 51 | loss: 6.6412  |  0:03:58s\n",
      "epoch 52 | loss: 6.69319 |  0:04:03s\n",
      "epoch 53 | loss: 6.79194 |  0:04:07s\n",
      "epoch 54 | loss: 6.45061 |  0:04:11s\n",
      "epoch 55 | loss: 6.59733 |  0:04:16s\n",
      "epoch 56 | loss: 6.32751 |  0:04:20s\n",
      "epoch 57 | loss: 6.32106 |  0:04:25s\n",
      "epoch 58 | loss: 6.14199 |  0:04:30s\n",
      "epoch 59 | loss: 6.40754 |  0:04:34s\n",
      "epoch 60 | loss: 6.05574 |  0:04:39s\n",
      "epoch 61 | loss: 6.21052 |  0:04:43s\n",
      "epoch 62 | loss: 6.10253 |  0:04:48s\n",
      "epoch 63 | loss: 6.11721 |  0:04:52s\n",
      "epoch 64 | loss: 5.81199 |  0:04:57s\n",
      "epoch 65 | loss: 5.94088 |  0:05:02s\n",
      "epoch 66 | loss: 5.82903 |  0:05:06s\n",
      "epoch 67 | loss: 5.76437 |  0:05:11s\n",
      "epoch 68 | loss: 6.20248 |  0:05:15s\n",
      "epoch 69 | loss: 6.05013 |  0:05:20s\n",
      "epoch 70 | loss: 5.84226 |  0:05:25s\n",
      "epoch 71 | loss: 5.85051 |  0:05:30s\n",
      "epoch 72 | loss: 5.79889 |  0:05:34s\n",
      "epoch 73 | loss: 5.78699 |  0:05:38s\n",
      "epoch 74 | loss: 6.13511 |  0:05:43s\n",
      "epoch 75 | loss: 5.85889 |  0:05:47s\n",
      "epoch 76 | loss: 5.89316 |  0:05:52s\n",
      "epoch 77 | loss: 6.14081 |  0:05:57s\n",
      "epoch 78 | loss: 5.75524 |  0:06:02s\n",
      "epoch 79 | loss: 5.76512 |  0:06:06s\n",
      "epoch 80 | loss: 5.90722 |  0:06:10s\n",
      "epoch 81 | loss: 5.83657 |  0:06:15s\n",
      "epoch 82 | loss: 5.77782 |  0:06:19s\n",
      "epoch 83 | loss: 5.58921 |  0:06:24s\n",
      "epoch 84 | loss: 5.53322 |  0:06:29s\n",
      "epoch 85 | loss: 5.65238 |  0:06:33s\n",
      "epoch 86 | loss: 5.44633 |  0:06:38s\n",
      "epoch 87 | loss: 5.41648 |  0:06:43s\n",
      "epoch 88 | loss: 5.51032 |  0:06:47s\n",
      "epoch 89 | loss: 5.52308 |  0:06:51s\n",
      "epoch 90 | loss: 5.25758 |  0:06:56s\n",
      "epoch 91 | loss: 5.59814 |  0:07:01s\n",
      "epoch 92 | loss: 5.46792 |  0:07:06s\n",
      "epoch 93 | loss: 5.3818  |  0:07:10s\n",
      "epoch 94 | loss: 5.31927 |  0:07:14s\n",
      "epoch 95 | loss: 5.28132 |  0:07:19s\n",
      "epoch 96 | loss: 5.16063 |  0:07:23s\n",
      "epoch 97 | loss: 5.24027 |  0:07:28s\n",
      "epoch 98 | loss: 5.4304  |  0:07:33s\n",
      "epoch 99 | loss: 5.46162 |  0:07:37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 10:28:52,396]\u001b[0m Trial 13 finished with value: 2.594715222454284 and parameters: {'n_d': 21, 'n_a': 46, 'n_steps': 7, 'gamma': 1.659190105579023, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.1964640124476169}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44479, 103)\n",
      "(44479,)\n",
      "(2341, 103)\n",
      "(2341,)\n",
      "epoch 0  | loss: 156.44244|  0:00:02s\n",
      "epoch 1  | loss: 21.5072 |  0:00:04s\n",
      "epoch 2  | loss: 16.78615|  0:00:06s\n",
      "epoch 3  | loss: 15.9005 |  0:00:09s\n",
      "epoch 4  | loss: 13.82375|  0:00:12s\n",
      "epoch 5  | loss: 13.0402 |  0:00:14s\n",
      "epoch 6  | loss: 11.82054|  0:00:16s\n",
      "epoch 7  | loss: 11.44426|  0:00:19s\n",
      "epoch 8  | loss: 11.27802|  0:00:22s\n",
      "epoch 9  | loss: 10.71741|  0:00:24s\n",
      "epoch 10 | loss: 10.47033|  0:00:27s\n",
      "epoch 11 | loss: 10.06739|  0:00:29s\n",
      "epoch 12 | loss: 10.10126|  0:00:31s\n",
      "epoch 13 | loss: 9.81187 |  0:00:34s\n",
      "epoch 14 | loss: 9.59816 |  0:00:36s\n",
      "epoch 15 | loss: 9.40534 |  0:00:38s\n",
      "epoch 16 | loss: 9.11767 |  0:00:41s\n",
      "epoch 17 | loss: 9.12485 |  0:00:43s\n",
      "epoch 18 | loss: 8.85024 |  0:00:46s\n",
      "epoch 19 | loss: 8.98041 |  0:00:48s\n",
      "epoch 20 | loss: 8.61201 |  0:00:50s\n",
      "epoch 21 | loss: 8.55322 |  0:00:54s\n",
      "epoch 22 | loss: 8.3333  |  0:00:56s\n",
      "epoch 23 | loss: 8.43184 |  0:00:58s\n",
      "epoch 24 | loss: 8.24809 |  0:01:01s\n",
      "epoch 25 | loss: 8.1242  |  0:01:03s\n",
      "epoch 26 | loss: 7.9995  |  0:01:06s\n",
      "epoch 27 | loss: 7.91166 |  0:01:08s\n",
      "epoch 28 | loss: 7.76427 |  0:01:10s\n",
      "epoch 29 | loss: 7.78461 |  0:01:13s\n",
      "epoch 30 | loss: 7.6657  |  0:01:15s\n",
      "epoch 31 | loss: 7.578   |  0:01:17s\n",
      "epoch 32 | loss: 7.58038 |  0:01:20s\n",
      "epoch 33 | loss: 7.46554 |  0:01:22s\n",
      "epoch 34 | loss: 7.40926 |  0:01:25s\n",
      "epoch 35 | loss: 7.13235 |  0:01:28s\n",
      "epoch 36 | loss: 7.29309 |  0:01:30s\n",
      "epoch 37 | loss: 6.99614 |  0:01:33s\n",
      "epoch 38 | loss: 7.01563 |  0:01:35s\n",
      "epoch 39 | loss: 7.0393  |  0:01:37s\n",
      "epoch 40 | loss: 7.01621 |  0:01:40s\n",
      "epoch 41 | loss: 6.65532 |  0:01:42s\n",
      "epoch 42 | loss: 6.85932 |  0:01:44s\n",
      "epoch 43 | loss: 6.79481 |  0:01:47s\n",
      "epoch 44 | loss: 6.50129 |  0:01:49s\n",
      "epoch 45 | loss: 6.57892 |  0:01:52s\n",
      "epoch 46 | loss: 6.57112 |  0:01:54s\n",
      "epoch 47 | loss: 6.44506 |  0:01:57s\n",
      "epoch 48 | loss: 6.41726 |  0:01:59s\n",
      "epoch 49 | loss: 6.37361 |  0:02:02s\n",
      "epoch 50 | loss: 6.25834 |  0:02:04s\n",
      "epoch 51 | loss: 6.14239 |  0:02:06s\n",
      "epoch 52 | loss: 7.98904 |  0:02:09s\n",
      "epoch 53 | loss: 6.8988  |  0:02:11s\n",
      "epoch 54 | loss: 6.26355 |  0:02:14s\n",
      "epoch 55 | loss: 6.17602 |  0:02:16s\n",
      "epoch 56 | loss: 6.2517  |  0:02:19s\n",
      "epoch 57 | loss: 5.95177 |  0:02:21s\n",
      "epoch 58 | loss: 6.06811 |  0:02:23s\n",
      "epoch 59 | loss: 5.91406 |  0:02:26s\n",
      "epoch 60 | loss: 5.94043 |  0:02:29s\n",
      "epoch 61 | loss: 5.80032 |  0:02:31s\n",
      "epoch 62 | loss: 5.68642 |  0:02:33s\n",
      "epoch 63 | loss: 5.78059 |  0:02:36s\n",
      "epoch 64 | loss: 5.72365 |  0:02:38s\n",
      "epoch 65 | loss: 5.95477 |  0:02:41s\n",
      "epoch 66 | loss: 6.25142 |  0:02:43s\n",
      "epoch 67 | loss: 5.82101 |  0:02:46s\n",
      "epoch 68 | loss: 5.65707 |  0:02:48s\n",
      "epoch 69 | loss: 5.68639 |  0:02:50s\n",
      "epoch 70 | loss: 5.58095 |  0:02:53s\n",
      "epoch 71 | loss: 5.46366 |  0:02:55s\n",
      "epoch 72 | loss: 5.51051 |  0:02:58s\n",
      "epoch 73 | loss: 5.65438 |  0:03:00s\n",
      "epoch 74 | loss: 5.30584 |  0:03:03s\n",
      "epoch 75 | loss: 5.39567 |  0:03:05s\n",
      "epoch 76 | loss: 5.41263 |  0:03:08s\n",
      "epoch 77 | loss: 5.40314 |  0:03:10s\n",
      "epoch 78 | loss: 5.46668 |  0:03:12s\n",
      "epoch 79 | loss: 5.28107 |  0:03:15s\n",
      "epoch 80 | loss: 5.34271 |  0:03:17s\n",
      "epoch 81 | loss: 5.40921 |  0:03:20s\n",
      "epoch 82 | loss: 5.39629 |  0:03:22s\n",
      "epoch 83 | loss: 5.33719 |  0:03:24s\n",
      "epoch 84 | loss: 5.1133  |  0:03:27s\n",
      "epoch 85 | loss: 5.19267 |  0:03:30s\n",
      "epoch 86 | loss: 5.14506 |  0:03:32s\n",
      "epoch 87 | loss: 5.14264 |  0:03:34s\n",
      "epoch 88 | loss: 5.08863 |  0:03:37s\n",
      "epoch 89 | loss: 5.25713 |  0:03:39s\n",
      "epoch 90 | loss: 5.08857 |  0:03:42s\n",
      "epoch 91 | loss: 5.17643 |  0:03:44s\n",
      "epoch 92 | loss: 5.16452 |  0:03:46s\n",
      "epoch 93 | loss: 4.99621 |  0:03:49s\n",
      "epoch 94 | loss: 5.05201 |  0:03:51s\n",
      "epoch 95 | loss: 4.97343 |  0:03:54s\n",
      "epoch 96 | loss: 5.02861 |  0:03:56s\n",
      "epoch 97 | loss: 5.00408 |  0:03:58s\n",
      "epoch 98 | loss: 5.02038 |  0:04:02s\n",
      "epoch 99 | loss: 4.88732 |  0:04:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-01 10:32:58,745]\u001b[0m Trial 14 finished with value: 2.540720906211383 and parameters: {'n_d': 33, 'n_a': 27, 'n_steps': 3, 'gamma': 1.3095077048132047, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.19418973570187065}. Best is trial 6 with value: 2.494900428891409.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_d': 16, 'n_a': 17, 'n_steps': 8, 'gamma': 1.1792901351310356, 'n_independent': 1, 'n_shared': 3, 'momentum': 0.13446266890938988}\n",
      "0 : [2.869403157472206]\n",
      "{'n_d': 32, 'n_a': 19, 'n_steps': 9, 'gamma': 1.1522013032621965, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.011915537822230756}\n",
      "1 : [5.320756861847874]\n",
      "{'n_d': 16, 'n_a': 46, 'n_steps': 5, 'gamma': 1.5243842429702306, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.07105431259368751}\n",
      "2 : [2.516404174234202]\n",
      "{'n_d': 54, 'n_a': 12, 'n_steps': 4, 'gamma': 1.2376124451529584, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.20715115026468467}\n",
      "3 : [2.7415864774295335]\n",
      "{'n_d': 61, 'n_a': 59, 'n_steps': 10, 'gamma': 1.036939251682052, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.30606686654228815}\n",
      "4 : [7.551546365862773]\n",
      "{'n_d': 27, 'n_a': 64, 'n_steps': 3, 'gamma': 1.0734902749801718, 'n_independent': 1, 'n_shared': 5, 'momentum': 0.2671578390488752}\n",
      "5 : [2.550078155642487]\n",
      "{'n_d': 27, 'n_a': 35, 'n_steps': 5, 'gamma': 1.1378019867067573, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.17444217220619315}\n",
      "6 : [2.494900428891409]\n",
      "{'n_d': 14, 'n_a': 61, 'n_steps': 4, 'gamma': 1.5501195964709498, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.04330712383322643}\n",
      "7 : [2.700514506308968]\n",
      "{'n_d': 11, 'n_a': 55, 'n_steps': 6, 'gamma': 1.0920357690025664, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.30362615524927633}\n",
      "8 : [5.210812452198318]\n",
      "{'n_d': 63, 'n_a': 9, 'n_steps': 8, 'gamma': 1.7167874121238436, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.10543011590009474}\n",
      "9 : [2.5302215500813827]\n",
      "{'n_d': 43, 'n_a': 33, 'n_steps': 6, 'gamma': 1.9727079907791336, 'n_independent': 5, 'n_shared': 1, 'momentum': 0.3939647664609881}\n",
      "10 : [2.849228712913129]\n",
      "{'n_d': 25, 'n_a': 44, 'n_steps': 5, 'gamma': 1.3956974172571235, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.13507246435285994}\n",
      "11 : [2.547158879001383]\n",
      "{'n_d': 39, 'n_a': 35, 'n_steps': 5, 'gamma': 1.4518025600713935, 'n_independent': 3, 'n_shared': 1, 'momentum': 0.08243781395273325}\n",
      "12 : [2.6275597370780512]\n",
      "{'n_d': 21, 'n_a': 46, 'n_steps': 7, 'gamma': 1.659190105579023, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.1964640124476169}\n",
      "13 : [2.594715222454284]\n",
      "{'n_d': 33, 'n_a': 27, 'n_steps': 3, 'gamma': 1.3095077048132047, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.19418973570187065}\n",
      "14 : [2.540720906211383]\n",
      " Best params for fold : [0/20]\n",
      "{'n_d': 27, 'n_a': 35, 'n_steps': 5, 'gamma': 1.1378019867067573, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.17444217220619315}\n",
      "epoch 0  | loss: 131.89004|  0:00:02s\n",
      "epoch 1  | loss: 26.00874|  0:00:05s\n",
      "epoch 2  | loss: 20.25521|  0:00:08s\n",
      "epoch 3  | loss: 19.251  |  0:00:11s\n",
      "epoch 4  | loss: 18.01602|  0:00:13s\n",
      "epoch 5  | loss: 17.37874|  0:00:16s\n",
      "epoch 6  | loss: 15.99069|  0:00:19s\n",
      "epoch 7  | loss: 15.50613|  0:00:21s\n",
      "epoch 8  | loss: 14.71874|  0:00:24s\n",
      "epoch 9  | loss: 13.43048|  0:00:28s\n",
      "epoch 10 | loss: 13.27243|  0:00:30s\n",
      "epoch 11 | loss: 12.40949|  0:00:33s\n",
      "epoch 12 | loss: 12.13766|  0:00:36s\n",
      "epoch 13 | loss: 11.63314|  0:00:39s\n",
      "epoch 14 | loss: 11.35365|  0:00:41s\n",
      "epoch 15 | loss: 11.27841|  0:00:44s\n",
      "epoch 16 | loss: 10.74683|  0:00:47s\n",
      "epoch 17 | loss: 10.95095|  0:00:50s\n",
      "epoch 18 | loss: 10.63286|  0:00:52s\n",
      "epoch 19 | loss: 10.27679|  0:00:55s\n",
      "epoch 20 | loss: 10.23791|  0:00:58s\n",
      "epoch 21 | loss: 9.95465 |  0:01:01s\n",
      "epoch 22 | loss: 9.82607 |  0:01:04s\n",
      "epoch 23 | loss: 9.76002 |  0:01:06s\n",
      "epoch 24 | loss: 9.79764 |  0:01:09s\n",
      "epoch 25 | loss: 9.56492 |  0:01:12s\n",
      "epoch 26 | loss: 9.4014  |  0:01:15s\n",
      "epoch 27 | loss: 9.24059 |  0:01:18s\n",
      "epoch 28 | loss: 9.1641  |  0:01:20s\n",
      "epoch 29 | loss: 8.97587 |  0:01:23s\n",
      "epoch 30 | loss: 8.73125 |  0:01:26s\n",
      "epoch 31 | loss: 8.6874  |  0:01:29s\n",
      "epoch 32 | loss: 8.69329 |  0:01:32s\n",
      "epoch 33 | loss: 8.39118 |  0:01:35s\n",
      "epoch 34 | loss: 8.61947 |  0:01:38s\n",
      "epoch 35 | loss: 8.33319 |  0:01:40s\n",
      "epoch 36 | loss: 8.34387 |  0:01:43s\n",
      "epoch 37 | loss: 8.26905 |  0:01:46s\n",
      "epoch 38 | loss: 8.08292 |  0:01:49s\n",
      "epoch 39 | loss: 8.00589 |  0:01:51s\n",
      "epoch 40 | loss: 7.91492 |  0:01:54s\n",
      "epoch 41 | loss: 7.78736 |  0:01:57s\n",
      "epoch 42 | loss: 7.6213  |  0:02:00s\n",
      "epoch 43 | loss: 7.665   |  0:02:03s\n",
      "epoch 44 | loss: 7.62425 |  0:02:06s\n",
      "epoch 45 | loss: 7.41962 |  0:02:09s\n",
      "epoch 46 | loss: 7.3482  |  0:02:11s\n",
      "epoch 47 | loss: 7.20137 |  0:02:14s\n",
      "epoch 48 | loss: 7.0987  |  0:02:17s\n",
      "epoch 49 | loss: 7.01149 |  0:02:20s\n",
      "epoch 50 | loss: 7.23917 |  0:02:22s\n",
      "epoch 51 | loss: 6.99159 |  0:02:25s\n",
      "epoch 52 | loss: 7.04367 |  0:02:28s\n",
      "epoch 53 | loss: 6.89784 |  0:02:31s\n",
      "epoch 54 | loss: 6.82988 |  0:02:34s\n",
      "epoch 55 | loss: 6.77708 |  0:02:37s\n",
      "epoch 56 | loss: 6.81523 |  0:02:40s\n",
      "epoch 57 | loss: 6.61832 |  0:02:42s\n",
      "epoch 58 | loss: 6.55389 |  0:02:45s\n",
      "epoch 59 | loss: 6.67249 |  0:02:48s\n",
      "epoch 60 | loss: 6.39734 |  0:02:51s\n",
      "epoch 61 | loss: 6.42056 |  0:02:53s\n",
      "epoch 62 | loss: 6.33207 |  0:02:56s\n",
      "epoch 63 | loss: 6.31209 |  0:02:59s\n",
      "epoch 64 | loss: 6.26845 |  0:03:02s\n",
      "epoch 65 | loss: 6.31287 |  0:03:05s\n",
      "epoch 66 | loss: 6.39062 |  0:03:07s\n",
      "epoch 67 | loss: 6.25558 |  0:03:10s\n",
      "epoch 68 | loss: 6.15718 |  0:03:13s\n",
      "epoch 69 | loss: 6.1644  |  0:03:16s\n",
      "epoch 70 | loss: 6.11186 |  0:03:19s\n",
      "epoch 71 | loss: 5.98456 |  0:03:21s\n",
      "epoch 72 | loss: 5.89858 |  0:03:24s\n",
      "epoch 73 | loss: 6.11193 |  0:03:27s\n",
      "epoch 74 | loss: 5.89831 |  0:03:30s\n",
      "epoch 75 | loss: 5.86095 |  0:03:32s\n",
      "epoch 76 | loss: 5.99795 |  0:03:36s\n",
      "epoch 77 | loss: 5.72383 |  0:03:39s\n",
      "epoch 78 | loss: 5.77888 |  0:03:41s\n",
      "epoch 79 | loss: 5.90344 |  0:03:44s\n",
      "epoch 80 | loss: 5.91637 |  0:03:47s\n",
      "epoch 81 | loss: 5.7811  |  0:03:50s\n",
      "epoch 82 | loss: 5.73501 |  0:03:52s\n",
      "epoch 83 | loss: 5.65263 |  0:03:55s\n",
      "epoch 84 | loss: 5.78253 |  0:03:58s\n",
      "epoch 85 | loss: 5.5502  |  0:04:01s\n",
      "epoch 86 | loss: 5.59964 |  0:04:03s\n",
      "epoch 87 | loss: 5.60328 |  0:04:06s\n",
      "epoch 88 | loss: 5.59562 |  0:04:09s\n",
      "epoch 89 | loss: 5.49397 |  0:04:12s\n",
      "epoch 90 | loss: 5.4947  |  0:04:15s\n",
      "epoch 91 | loss: 5.60559 |  0:04:17s\n",
      "epoch 92 | loss: 5.68476 |  0:04:20s\n",
      "epoch 93 | loss: 5.47702 |  0:04:23s\n",
      "epoch 94 | loss: 5.55088 |  0:04:26s\n",
      "epoch 95 | loss: 5.35296 |  0:04:28s\n",
      "epoch 96 | loss: 5.41601 |  0:04:31s\n",
      "epoch 97 | loss: 5.35546 |  0:04:34s\n",
      "epoch 98 | loss: 5.38551 |  0:04:37s\n",
      "epoch 99 | loss: 5.29813 |  0:04:40s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "0: [2.869403157472206]\n",
      "1: [5.320756861847874]\n",
      "2: [2.516404174234202]\n",
      "3: [2.7415864774295335]\n",
      "4: [7.551546365862773]\n",
      "5: [2.550078155642487]\n",
      "6: [2.494900428891409]\n",
      "7: [2.700514506308968]\n",
      "8: [5.210812452198318]\n",
      "9: [2.5302215500813827]\n",
      "10: [2.849228712913129]\n",
      "11: [2.547158879001383]\n",
      "12: [2.6275597370780512]\n",
      "13: [2.594715222454284]\n",
      "14: [2.540720906211383]\n",
      "[++] Ended the training process for fold 0\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 131.89004|  0:00:02s\n",
      "epoch 1  | loss: 26.00874|  0:00:05s\n",
      "epoch 2  | loss: 20.25521|  0:00:08s\n",
      "epoch 3  | loss: 19.251  |  0:00:10s\n",
      "epoch 4  | loss: 18.01602|  0:00:13s\n",
      "epoch 5  | loss: 17.37874|  0:00:16s\n",
      "epoch 6  | loss: 15.99069|  0:00:19s\n",
      "epoch 7  | loss: 15.50613|  0:00:22s\n",
      "epoch 8  | loss: 14.71874|  0:00:24s\n",
      "epoch 9  | loss: 13.43048|  0:00:28s\n",
      "epoch 10 | loss: 13.27243|  0:00:30s\n",
      "epoch 11 | loss: 12.40949|  0:00:33s\n",
      "epoch 12 | loss: 12.13766|  0:00:36s\n",
      "epoch 13 | loss: 11.63314|  0:00:38s\n",
      "epoch 14 | loss: 11.35365|  0:00:41s\n",
      "epoch 15 | loss: 11.27841|  0:00:44s\n",
      "epoch 16 | loss: 10.74683|  0:00:47s\n",
      "epoch 17 | loss: 10.95095|  0:00:49s\n",
      "epoch 18 | loss: 10.63286|  0:00:53s\n",
      "epoch 19 | loss: 10.27679|  0:00:55s\n",
      "epoch 20 | loss: 10.23791|  0:00:58s\n",
      "epoch 21 | loss: 9.95465 |  0:01:01s\n",
      "epoch 22 | loss: 9.82607 |  0:01:04s\n",
      "epoch 23 | loss: 9.76002 |  0:01:07s\n",
      "epoch 24 | loss: 9.79764 |  0:01:09s\n",
      "epoch 25 | loss: 9.56492 |  0:01:12s\n",
      "epoch 26 | loss: 9.4014  |  0:01:15s\n",
      "epoch 27 | loss: 9.24059 |  0:01:18s\n",
      "epoch 28 | loss: 9.1641  |  0:01:21s\n",
      "epoch 29 | loss: 8.97587 |  0:01:23s\n",
      "epoch 30 | loss: 8.73125 |  0:01:26s\n",
      "epoch 31 | loss: 8.6874  |  0:01:29s\n",
      "epoch 32 | loss: 8.69329 |  0:01:32s\n",
      "epoch 33 | loss: 8.39118 |  0:01:35s\n",
      "epoch 34 | loss: 8.61947 |  0:01:38s\n",
      "epoch 35 | loss: 8.33319 |  0:01:40s\n",
      "epoch 36 | loss: 8.34387 |  0:01:43s\n",
      "epoch 37 | loss: 8.26905 |  0:01:46s\n",
      "epoch 38 | loss: 8.08292 |  0:01:49s\n",
      "epoch 39 | loss: 8.00589 |  0:01:51s\n",
      "epoch 40 | loss: 7.91492 |  0:01:54s\n",
      "epoch 41 | loss: 7.78736 |  0:01:57s\n",
      "epoch 42 | loss: 7.6213  |  0:02:00s\n",
      "epoch 43 | loss: 7.665   |  0:02:03s\n",
      "epoch 44 | loss: 7.62425 |  0:02:05s\n",
      "epoch 45 | loss: 7.41962 |  0:02:08s\n",
      "epoch 46 | loss: 7.3482  |  0:02:11s\n",
      "epoch 47 | loss: 7.20137 |  0:02:14s\n",
      "epoch 48 | loss: 7.0987  |  0:02:17s\n",
      "epoch 49 | loss: 7.01149 |  0:02:20s\n",
      "epoch 50 | loss: 7.23917 |  0:02:23s\n",
      "epoch 51 | loss: 6.99159 |  0:02:25s\n",
      "epoch 52 | loss: 7.04367 |  0:02:28s\n",
      "epoch 53 | loss: 6.89784 |  0:02:31s\n",
      "epoch 54 | loss: 6.82988 |  0:02:34s\n",
      "epoch 55 | loss: 6.77708 |  0:02:37s\n",
      "epoch 56 | loss: 6.81523 |  0:02:40s\n",
      "epoch 57 | loss: 6.61832 |  0:02:42s\n",
      "epoch 58 | loss: 6.55389 |  0:02:45s\n",
      "epoch 59 | loss: 6.67249 |  0:02:48s\n",
      "epoch 60 | loss: 6.39734 |  0:02:51s\n",
      "epoch 61 | loss: 6.42056 |  0:02:54s\n",
      "epoch 62 | loss: 6.33207 |  0:02:56s\n",
      "epoch 63 | loss: 6.31209 |  0:02:59s\n",
      "epoch 64 | loss: 6.26845 |  0:03:02s\n",
      "epoch 65 | loss: 6.31287 |  0:03:05s\n",
      "epoch 66 | loss: 6.39062 |  0:03:08s\n",
      "epoch 67 | loss: 6.25558 |  0:03:11s\n",
      "epoch 68 | loss: 6.15718 |  0:03:13s\n",
      "epoch 69 | loss: 6.1644  |  0:03:16s\n",
      "epoch 70 | loss: 6.11186 |  0:03:19s\n",
      "epoch 71 | loss: 5.98456 |  0:03:22s\n",
      "epoch 72 | loss: 5.89858 |  0:03:24s\n",
      "epoch 73 | loss: 6.11193 |  0:03:27s\n",
      "epoch 74 | loss: 5.89831 |  0:03:30s\n",
      "epoch 75 | loss: 5.86095 |  0:03:33s\n",
      "epoch 76 | loss: 5.99795 |  0:03:36s\n",
      "epoch 77 | loss: 5.72383 |  0:03:39s\n",
      "epoch 78 | loss: 5.77888 |  0:03:42s\n",
      "epoch 79 | loss: 5.90344 |  0:03:44s\n",
      "epoch 80 | loss: 5.91637 |  0:03:47s\n",
      "epoch 81 | loss: 5.7811  |  0:03:50s\n",
      "epoch 82 | loss: 5.73501 |  0:03:53s\n",
      "epoch 83 | loss: 5.65263 |  0:03:56s\n",
      "epoch 84 | loss: 5.78253 |  0:03:58s\n",
      "epoch 85 | loss: 5.5502  |  0:04:01s\n",
      "epoch 86 | loss: 5.59964 |  0:04:04s\n",
      "epoch 87 | loss: 5.60328 |  0:04:07s\n",
      "epoch 88 | loss: 5.59562 |  0:04:10s\n",
      "epoch 89 | loss: 5.49397 |  0:04:13s\n",
      "epoch 90 | loss: 5.4947  |  0:04:15s\n",
      "epoch 91 | loss: 5.60559 |  0:04:18s\n",
      "epoch 92 | loss: 5.68476 |  0:04:21s\n",
      "epoch 93 | loss: 5.47702 |  0:04:24s\n",
      "epoch 94 | loss: 5.55088 |  0:04:27s\n",
      "epoch 95 | loss: 5.35296 |  0:04:29s\n",
      "epoch 96 | loss: 5.41601 |  0:04:32s\n",
      "epoch 97 | loss: 5.35546 |  0:04:35s\n",
      "epoch 98 | loss: 5.38551 |  0:04:38s\n",
      "epoch 99 | loss: 5.29813 |  0:04:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 0\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 146.32964|  0:00:02s\n",
      "epoch 1  | loss: 26.96936|  0:00:05s\n",
      "epoch 2  | loss: 22.0051 |  0:00:08s\n",
      "epoch 3  | loss: 21.21387|  0:00:11s\n",
      "epoch 4  | loss: 22.13049|  0:00:13s\n",
      "epoch 5  | loss: 17.16285|  0:00:16s\n",
      "epoch 6  | loss: 14.3308 |  0:00:19s\n",
      "epoch 7  | loss: 13.34686|  0:00:22s\n",
      "epoch 8  | loss: 12.53132|  0:00:25s\n",
      "epoch 9  | loss: 12.0585 |  0:00:28s\n",
      "epoch 10 | loss: 11.89268|  0:00:30s\n",
      "epoch 11 | loss: 11.21472|  0:00:33s\n",
      "epoch 12 | loss: 11.06479|  0:00:36s\n",
      "epoch 13 | loss: 10.61702|  0:00:39s\n",
      "epoch 14 | loss: 10.42591|  0:00:42s\n",
      "epoch 15 | loss: 10.15432|  0:00:45s\n",
      "epoch 16 | loss: 10.19973|  0:00:47s\n",
      "epoch 17 | loss: 9.96438 |  0:00:50s\n",
      "epoch 18 | loss: 9.65519 |  0:00:53s\n",
      "epoch 19 | loss: 9.56811 |  0:00:56s\n",
      "epoch 20 | loss: 9.832   |  0:00:59s\n",
      "epoch 21 | loss: 10.09158|  0:01:02s\n",
      "epoch 22 | loss: 9.82844 |  0:01:04s\n",
      "epoch 23 | loss: 9.33911 |  0:01:07s\n",
      "epoch 24 | loss: 9.18989 |  0:01:10s\n",
      "epoch 25 | loss: 9.00014 |  0:01:13s\n",
      "epoch 26 | loss: 8.77957 |  0:01:16s\n",
      "epoch 27 | loss: 8.79548 |  0:01:18s\n",
      "epoch 28 | loss: 8.86806 |  0:01:21s\n",
      "epoch 29 | loss: 9.21049 |  0:01:24s\n",
      "epoch 30 | loss: 8.96778 |  0:01:27s\n",
      "epoch 31 | loss: 8.70025 |  0:01:30s\n",
      "epoch 32 | loss: 8.81115 |  0:01:33s\n",
      "epoch 33 | loss: 9.51694 |  0:01:36s\n",
      "epoch 34 | loss: 8.87337 |  0:01:39s\n",
      "epoch 35 | loss: 8.71542 |  0:01:42s\n",
      "epoch 36 | loss: 8.65298 |  0:01:44s\n",
      "epoch 37 | loss: 8.5705  |  0:01:47s\n",
      "epoch 38 | loss: 8.42801 |  0:01:50s\n",
      "epoch 39 | loss: 8.6166  |  0:01:53s\n",
      "epoch 40 | loss: 8.40661 |  0:01:56s\n",
      "epoch 41 | loss: 8.18902 |  0:01:58s\n",
      "epoch 42 | loss: 8.43746 |  0:02:02s\n",
      "epoch 43 | loss: 8.41777 |  0:02:04s\n",
      "epoch 44 | loss: 7.91112 |  0:02:07s\n",
      "epoch 45 | loss: 7.81068 |  0:02:10s\n",
      "epoch 46 | loss: 7.62905 |  0:02:13s\n",
      "epoch 47 | loss: 7.78488 |  0:02:15s\n",
      "epoch 48 | loss: 7.6378  |  0:02:18s\n",
      "epoch 49 | loss: 7.54583 |  0:02:21s\n",
      "epoch 50 | loss: 7.49129 |  0:02:24s\n",
      "epoch 51 | loss: 7.38898 |  0:02:26s\n",
      "epoch 52 | loss: 7.26102 |  0:02:29s\n",
      "epoch 53 | loss: 7.2595  |  0:02:32s\n",
      "epoch 54 | loss: 7.22412 |  0:02:35s\n",
      "epoch 55 | loss: 7.13704 |  0:02:38s\n",
      "epoch 56 | loss: 7.24794 |  0:02:41s\n",
      "epoch 57 | loss: 7.12683 |  0:02:44s\n",
      "epoch 58 | loss: 7.0455  |  0:02:46s\n",
      "epoch 59 | loss: 7.13029 |  0:02:49s\n",
      "epoch 60 | loss: 7.0905  |  0:02:52s\n",
      "epoch 61 | loss: 6.95522 |  0:02:55s\n",
      "epoch 62 | loss: 6.97402 |  0:02:58s\n",
      "epoch 63 | loss: 7.15085 |  0:03:00s\n",
      "epoch 64 | loss: 6.93697 |  0:03:03s\n",
      "epoch 65 | loss: 6.99609 |  0:03:06s\n",
      "epoch 66 | loss: 7.01959 |  0:03:09s\n",
      "epoch 67 | loss: 7.18472 |  0:03:12s\n",
      "epoch 68 | loss: 6.89177 |  0:03:15s\n",
      "epoch 69 | loss: 6.85442 |  0:03:18s\n",
      "epoch 70 | loss: 6.82328 |  0:03:20s\n",
      "epoch 71 | loss: 6.86407 |  0:03:23s\n",
      "epoch 72 | loss: 6.72957 |  0:03:26s\n",
      "epoch 73 | loss: 6.63087 |  0:03:29s\n",
      "epoch 74 | loss: 6.46108 |  0:03:31s\n",
      "epoch 75 | loss: 7.12587 |  0:03:34s\n",
      "epoch 76 | loss: 6.97328 |  0:03:37s\n",
      "epoch 77 | loss: 6.5518  |  0:03:40s\n",
      "epoch 78 | loss: 6.60857 |  0:03:43s\n",
      "epoch 79 | loss: 6.84314 |  0:03:46s\n",
      "epoch 80 | loss: 6.70897 |  0:03:48s\n",
      "epoch 81 | loss: 6.72393 |  0:03:51s\n",
      "epoch 82 | loss: 6.70505 |  0:03:54s\n",
      "epoch 83 | loss: 8.07353 |  0:03:57s\n",
      "epoch 84 | loss: 7.29367 |  0:03:59s\n",
      "epoch 85 | loss: 6.96689 |  0:04:02s\n",
      "epoch 86 | loss: 6.70743 |  0:04:06s\n",
      "epoch 87 | loss: 7.08674 |  0:04:08s\n",
      "epoch 88 | loss: 6.98246 |  0:04:11s\n",
      "epoch 89 | loss: 6.66345 |  0:04:14s\n",
      "epoch 90 | loss: 6.48101 |  0:04:17s\n",
      "epoch 91 | loss: 6.40959 |  0:04:20s\n",
      "epoch 92 | loss: 6.41852 |  0:04:22s\n",
      "epoch 93 | loss: 6.49409 |  0:04:25s\n",
      "epoch 94 | loss: 6.40487 |  0:04:28s\n",
      "epoch 95 | loss: 6.2603  |  0:04:31s\n",
      "epoch 96 | loss: 6.2723  |  0:04:33s\n",
      "epoch 97 | loss: 6.16108 |  0:04:36s\n",
      "epoch 98 | loss: 6.25646 |  0:04:39s\n",
      "epoch 99 | loss: 6.0787  |  0:04:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 1\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 146.0224|  0:00:02s\n",
      "epoch 1  | loss: 27.51834|  0:00:05s\n",
      "epoch 2  | loss: 22.61691|  0:00:08s\n",
      "epoch 3  | loss: 22.02782|  0:00:10s\n",
      "epoch 4  | loss: 19.98768|  0:00:13s\n",
      "epoch 5  | loss: 18.18984|  0:00:16s\n",
      "epoch 6  | loss: 16.22321|  0:00:18s\n",
      "epoch 7  | loss: 15.09559|  0:00:22s\n",
      "epoch 8  | loss: 15.26646|  0:00:25s\n",
      "epoch 9  | loss: 19.30244|  0:00:27s\n",
      "epoch 10 | loss: 14.64761|  0:00:30s\n",
      "epoch 11 | loss: 13.8891 |  0:00:33s\n",
      "epoch 12 | loss: 12.84616|  0:00:36s\n",
      "epoch 13 | loss: 12.52403|  0:00:39s\n",
      "epoch 14 | loss: 12.31496|  0:00:41s\n",
      "epoch 15 | loss: 11.93533|  0:00:44s\n",
      "epoch 16 | loss: 11.6711 |  0:00:47s\n",
      "epoch 17 | loss: 11.26022|  0:00:50s\n",
      "epoch 18 | loss: 11.04026|  0:00:53s\n",
      "epoch 19 | loss: 11.75648|  0:00:56s\n",
      "epoch 20 | loss: 11.90967|  0:00:59s\n",
      "epoch 21 | loss: 10.75299|  0:01:02s\n",
      "epoch 22 | loss: 10.74293|  0:01:04s\n",
      "epoch 23 | loss: 10.37828|  0:01:07s\n",
      "epoch 24 | loss: 10.14108|  0:01:10s\n",
      "epoch 25 | loss: 10.06438|  0:01:13s\n",
      "epoch 26 | loss: 9.51976 |  0:01:15s\n",
      "epoch 27 | loss: 9.53651 |  0:01:18s\n",
      "epoch 28 | loss: 9.57145 |  0:01:21s\n",
      "epoch 29 | loss: 9.20454 |  0:01:23s\n",
      "epoch 30 | loss: 9.15307 |  0:01:27s\n",
      "epoch 31 | loss: 9.17493 |  0:01:30s\n",
      "epoch 32 | loss: 9.13492 |  0:01:32s\n",
      "epoch 33 | loss: 8.79208 |  0:01:35s\n",
      "epoch 34 | loss: 8.60237 |  0:01:38s\n",
      "epoch 35 | loss: 8.59532 |  0:01:41s\n",
      "epoch 36 | loss: 8.26575 |  0:01:43s\n",
      "epoch 37 | loss: 8.17009 |  0:01:46s\n",
      "epoch 38 | loss: 8.28699 |  0:01:49s\n",
      "epoch 39 | loss: 8.10061 |  0:01:52s\n",
      "epoch 40 | loss: 8.023   |  0:01:55s\n",
      "epoch 41 | loss: 7.92297 |  0:01:58s\n",
      "epoch 42 | loss: 7.95604 |  0:02:01s\n",
      "epoch 43 | loss: 7.86341 |  0:02:04s\n",
      "epoch 44 | loss: 7.90233 |  0:02:06s\n",
      "epoch 45 | loss: 7.71103 |  0:02:09s\n",
      "epoch 46 | loss: 7.56886 |  0:02:12s\n",
      "epoch 47 | loss: 7.39466 |  0:02:15s\n",
      "epoch 48 | loss: 7.49013 |  0:02:17s\n",
      "epoch 49 | loss: 7.39918 |  0:02:20s\n",
      "epoch 50 | loss: 7.13586 |  0:02:23s\n",
      "epoch 51 | loss: 7.28835 |  0:02:26s\n",
      "epoch 52 | loss: 7.20439 |  0:02:29s\n",
      "epoch 53 | loss: 7.21933 |  0:02:32s\n",
      "epoch 54 | loss: 6.96496 |  0:02:35s\n",
      "epoch 55 | loss: 6.92854 |  0:02:37s\n",
      "epoch 56 | loss: 7.10941 |  0:02:40s\n",
      "epoch 57 | loss: 6.71977 |  0:02:43s\n",
      "epoch 58 | loss: 6.63868 |  0:02:46s\n",
      "epoch 59 | loss: 6.84288 |  0:02:49s\n",
      "epoch 60 | loss: 6.58071 |  0:02:51s\n",
      "epoch 61 | loss: 7.15527 |  0:02:54s\n",
      "epoch 62 | loss: 6.61997 |  0:02:57s\n",
      "epoch 63 | loss: 6.51172 |  0:03:00s\n",
      "epoch 64 | loss: 6.40312 |  0:03:03s\n",
      "epoch 65 | loss: 6.31311 |  0:03:06s\n",
      "epoch 66 | loss: 6.48889 |  0:03:09s\n",
      "epoch 67 | loss: 6.59505 |  0:03:11s\n",
      "epoch 68 | loss: 6.54183 |  0:03:14s\n",
      "epoch 69 | loss: 6.2436  |  0:03:17s\n",
      "epoch 70 | loss: 6.29487 |  0:03:20s\n",
      "epoch 71 | loss: 6.18496 |  0:03:22s\n",
      "epoch 72 | loss: 6.05585 |  0:03:25s\n",
      "epoch 73 | loss: 6.29535 |  0:03:28s\n",
      "epoch 74 | loss: 6.12932 |  0:03:31s\n",
      "epoch 75 | loss: 6.17561 |  0:03:34s\n",
      "epoch 76 | loss: 5.99803 |  0:03:36s\n",
      "epoch 77 | loss: 5.92413 |  0:03:39s\n",
      "epoch 78 | loss: 5.88372 |  0:03:42s\n",
      "epoch 79 | loss: 5.97055 |  0:03:45s\n",
      "epoch 80 | loss: 5.90886 |  0:03:47s\n",
      "epoch 81 | loss: 5.76873 |  0:03:50s\n",
      "epoch 82 | loss: 5.81318 |  0:03:53s\n",
      "epoch 83 | loss: 5.74117 |  0:03:55s\n",
      "epoch 84 | loss: 5.72373 |  0:03:58s\n",
      "epoch 85 | loss: 5.83769 |  0:04:01s\n",
      "epoch 86 | loss: 5.62113 |  0:04:04s\n",
      "epoch 87 | loss: 5.7     |  0:04:07s\n",
      "epoch 88 | loss: 5.64563 |  0:04:10s\n",
      "epoch 89 | loss: 5.73057 |  0:04:13s\n",
      "epoch 90 | loss: 5.61437 |  0:04:15s\n",
      "epoch 91 | loss: 5.66882 |  0:04:18s\n",
      "epoch 92 | loss: 5.63408 |  0:04:21s\n",
      "epoch 93 | loss: 5.63742 |  0:04:24s\n",
      "epoch 94 | loss: 5.4809  |  0:04:26s\n",
      "epoch 95 | loss: 5.34267 |  0:04:29s\n",
      "epoch 96 | loss: 5.47147 |  0:04:32s\n",
      "epoch 97 | loss: 5.41337 |  0:04:35s\n",
      "epoch 98 | loss: 5.25729 |  0:04:38s\n",
      "epoch 99 | loss: 5.44845 |  0:04:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 2\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 144.11325|  0:00:02s\n",
      "epoch 1  | loss: 28.11895|  0:00:05s\n",
      "epoch 2  | loss: 20.72493|  0:00:08s\n",
      "epoch 3  | loss: 17.91697|  0:00:11s\n",
      "epoch 4  | loss: 15.91924|  0:00:14s\n",
      "epoch 5  | loss: 15.32571|  0:00:16s\n",
      "epoch 6  | loss: 14.86942|  0:00:19s\n",
      "epoch 7  | loss: 14.29169|  0:00:22s\n",
      "epoch 8  | loss: 12.87878|  0:00:25s\n",
      "epoch 9  | loss: 12.40275|  0:00:28s\n",
      "epoch 10 | loss: 12.24346|  0:00:30s\n",
      "epoch 11 | loss: 12.04527|  0:00:33s\n",
      "epoch 12 | loss: 11.34868|  0:00:36s\n",
      "epoch 13 | loss: 11.57713|  0:00:39s\n",
      "epoch 14 | loss: 11.5301 |  0:00:41s\n",
      "epoch 15 | loss: 10.85324|  0:00:44s\n",
      "epoch 16 | loss: 10.50631|  0:00:47s\n",
      "epoch 17 | loss: 10.23598|  0:00:50s\n",
      "epoch 18 | loss: 10.26528|  0:00:53s\n",
      "epoch 19 | loss: 9.78194 |  0:00:56s\n",
      "epoch 20 | loss: 9.7127  |  0:00:59s\n",
      "epoch 21 | loss: 9.66872 |  0:01:01s\n",
      "epoch 22 | loss: 9.43804 |  0:01:04s\n",
      "epoch 23 | loss: 9.26368 |  0:01:07s\n",
      "epoch 24 | loss: 9.24257 |  0:01:10s\n",
      "epoch 25 | loss: 9.13397 |  0:01:12s\n",
      "epoch 26 | loss: 8.94529 |  0:01:15s\n",
      "epoch 27 | loss: 8.78891 |  0:01:18s\n",
      "epoch 28 | loss: 8.71526 |  0:01:21s\n",
      "epoch 29 | loss: 8.78884 |  0:01:23s\n",
      "epoch 30 | loss: 8.68659 |  0:01:27s\n",
      "epoch 31 | loss: 8.42994 |  0:01:30s\n",
      "epoch 32 | loss: 8.46907 |  0:01:32s\n",
      "epoch 33 | loss: 8.28223 |  0:01:35s\n",
      "epoch 34 | loss: 8.26944 |  0:01:38s\n",
      "epoch 35 | loss: 8.24095 |  0:01:40s\n",
      "epoch 36 | loss: 8.01389 |  0:01:43s\n",
      "epoch 37 | loss: 8.01548 |  0:01:46s\n",
      "epoch 38 | loss: 7.98028 |  0:01:49s\n",
      "epoch 39 | loss: 9.01662 |  0:01:52s\n",
      "epoch 40 | loss: 8.59674 |  0:01:54s\n",
      "epoch 41 | loss: 19.4289 |  0:01:58s\n",
      "epoch 42 | loss: 11.01914|  0:02:00s\n",
      "epoch 43 | loss: 9.95104 |  0:02:03s\n",
      "epoch 44 | loss: 9.45624 |  0:02:06s\n",
      "epoch 45 | loss: 9.65339 |  0:02:08s\n",
      "epoch 46 | loss: 8.67102 |  0:02:11s\n",
      "epoch 47 | loss: 8.43164 |  0:02:14s\n",
      "epoch 48 | loss: 8.35386 |  0:02:17s\n",
      "epoch 49 | loss: 8.1446  |  0:02:19s\n",
      "epoch 50 | loss: 7.92282 |  0:02:22s\n",
      "epoch 51 | loss: 8.04034 |  0:02:25s\n",
      "epoch 52 | loss: 7.76241 |  0:02:28s\n",
      "epoch 53 | loss: 7.60901 |  0:02:31s\n",
      "epoch 54 | loss: 7.58048 |  0:02:34s\n",
      "epoch 55 | loss: 7.64535 |  0:02:37s\n",
      "epoch 56 | loss: 7.41544 |  0:02:39s\n",
      "epoch 57 | loss: 7.21647 |  0:02:42s\n",
      "epoch 58 | loss: 7.05013 |  0:02:45s\n",
      "epoch 59 | loss: 7.18552 |  0:02:48s\n",
      "epoch 60 | loss: 6.9814  |  0:02:50s\n",
      "epoch 61 | loss: 6.91267 |  0:02:53s\n",
      "epoch 62 | loss: 6.71272 |  0:02:56s\n",
      "epoch 63 | loss: 6.80663 |  0:02:59s\n",
      "epoch 64 | loss: 6.74805 |  0:03:02s\n",
      "epoch 65 | loss: 6.58211 |  0:03:05s\n",
      "epoch 66 | loss: 6.46669 |  0:03:07s\n",
      "epoch 67 | loss: 6.50815 |  0:03:10s\n",
      "epoch 68 | loss: 6.60623 |  0:03:13s\n",
      "epoch 69 | loss: 6.58532 |  0:03:16s\n",
      "epoch 70 | loss: 6.51221 |  0:03:18s\n",
      "epoch 71 | loss: 6.28427 |  0:03:21s\n",
      "epoch 72 | loss: 6.31026 |  0:03:24s\n",
      "epoch 73 | loss: 6.16217 |  0:03:27s\n",
      "epoch 74 | loss: 6.19575 |  0:03:29s\n",
      "epoch 75 | loss: 6.20516 |  0:03:33s\n",
      "epoch 76 | loss: 6.14463 |  0:03:36s\n",
      "epoch 77 | loss: 6.06807 |  0:03:38s\n",
      "epoch 78 | loss: 6.06558 |  0:03:41s\n",
      "epoch 79 | loss: 6.06216 |  0:03:44s\n",
      "epoch 80 | loss: 5.87661 |  0:03:47s\n",
      "epoch 81 | loss: 5.92429 |  0:03:49s\n",
      "epoch 82 | loss: 5.96869 |  0:03:52s\n",
      "epoch 83 | loss: 6.00077 |  0:03:55s\n",
      "epoch 84 | loss: 5.88498 |  0:03:58s\n",
      "epoch 85 | loss: 5.79811 |  0:04:01s\n",
      "epoch 86 | loss: 5.80633 |  0:04:04s\n",
      "epoch 87 | loss: 5.88808 |  0:04:07s\n",
      "epoch 88 | loss: 5.62773 |  0:04:09s\n",
      "epoch 89 | loss: 5.69958 |  0:04:12s\n",
      "epoch 90 | loss: 5.62116 |  0:04:15s\n",
      "epoch 91 | loss: 5.58133 |  0:04:18s\n",
      "epoch 92 | loss: 5.59878 |  0:04:20s\n",
      "epoch 93 | loss: 5.57585 |  0:04:23s\n",
      "epoch 94 | loss: 5.64599 |  0:04:26s\n",
      "epoch 95 | loss: 5.52409 |  0:04:29s\n",
      "epoch 96 | loss: 5.56617 |  0:04:32s\n",
      "epoch 97 | loss: 5.59309 |  0:04:35s\n",
      "epoch 98 | loss: 5.36078 |  0:04:38s\n",
      "epoch 99 | loss: 5.30865 |  0:04:40s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 3\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 146.07523|  0:00:02s\n",
      "epoch 1  | loss: 32.59133|  0:00:05s\n",
      "epoch 2  | loss: 26.29874|  0:00:08s\n",
      "epoch 3  | loss: 23.66618|  0:00:10s\n",
      "epoch 4  | loss: 21.3338 |  0:00:13s\n",
      "epoch 5  | loss: 20.09547|  0:00:16s\n",
      "epoch 6  | loss: 19.8853 |  0:00:19s\n",
      "epoch 7  | loss: 21.49662|  0:00:22s\n",
      "epoch 8  | loss: 20.19052|  0:00:24s\n",
      "epoch 9  | loss: 19.07833|  0:00:27s\n",
      "epoch 10 | loss: 17.97219|  0:00:30s\n",
      "epoch 11 | loss: 17.11831|  0:00:33s\n",
      "epoch 12 | loss: 17.72723|  0:00:35s\n",
      "epoch 13 | loss: 29.06218|  0:00:38s\n",
      "epoch 14 | loss: 16.14973|  0:00:41s\n",
      "epoch 15 | loss: 13.75506|  0:00:44s\n",
      "epoch 16 | loss: 12.80892|  0:00:46s\n",
      "epoch 17 | loss: 12.36895|  0:00:49s\n",
      "epoch 18 | loss: 12.11037|  0:00:52s\n",
      "epoch 19 | loss: 11.62851|  0:00:55s\n",
      "epoch 20 | loss: 11.48599|  0:00:58s\n",
      "epoch 21 | loss: 10.98039|  0:01:01s\n",
      "epoch 22 | loss: 10.6299 |  0:01:04s\n",
      "epoch 23 | loss: 10.58637|  0:01:06s\n",
      "epoch 24 | loss: 10.1889 |  0:01:09s\n",
      "epoch 25 | loss: 9.97838 |  0:01:12s\n",
      "epoch 26 | loss: 10.18935|  0:01:15s\n",
      "epoch 27 | loss: 9.84548 |  0:01:18s\n",
      "epoch 28 | loss: 9.95393 |  0:01:21s\n",
      "epoch 29 | loss: 9.37698 |  0:01:23s\n",
      "epoch 30 | loss: 9.37344 |  0:01:26s\n",
      "epoch 31 | loss: 9.21661 |  0:01:29s\n",
      "epoch 32 | loss: 9.25918 |  0:01:32s\n",
      "epoch 33 | loss: 9.0476  |  0:01:35s\n",
      "epoch 34 | loss: 9.38107 |  0:01:37s\n",
      "epoch 35 | loss: 8.70097 |  0:01:40s\n",
      "epoch 36 | loss: 8.5331  |  0:01:43s\n",
      "epoch 37 | loss: 8.44362 |  0:01:46s\n",
      "epoch 38 | loss: 8.33496 |  0:01:48s\n",
      "epoch 39 | loss: 8.30014 |  0:01:51s\n",
      "epoch 40 | loss: 8.21508 |  0:01:54s\n",
      "epoch 41 | loss: 7.9238  |  0:01:57s\n",
      "epoch 42 | loss: 7.78083 |  0:02:00s\n",
      "epoch 43 | loss: 7.77793 |  0:02:03s\n",
      "epoch 44 | loss: 7.64661 |  0:02:06s\n",
      "epoch 45 | loss: 7.7287  |  0:02:08s\n",
      "epoch 46 | loss: 7.46434 |  0:02:11s\n",
      "epoch 47 | loss: 7.5196  |  0:02:14s\n",
      "epoch 48 | loss: 7.42554 |  0:02:17s\n",
      "epoch 49 | loss: 7.15922 |  0:02:19s\n",
      "epoch 50 | loss: 7.10358 |  0:02:22s\n",
      "epoch 51 | loss: 7.12257 |  0:02:25s\n",
      "epoch 52 | loss: 7.01511 |  0:02:28s\n",
      "epoch 53 | loss: 6.95721 |  0:02:31s\n",
      "epoch 54 | loss: 6.77406 |  0:02:33s\n",
      "epoch 55 | loss: 7.08156 |  0:02:36s\n",
      "epoch 56 | loss: 7.45805 |  0:02:39s\n",
      "epoch 57 | loss: 7.5159  |  0:02:42s\n",
      "epoch 58 | loss: 7.44283 |  0:02:45s\n",
      "epoch 59 | loss: 7.38606 |  0:02:47s\n",
      "epoch 60 | loss: 7.55758 |  0:02:51s\n",
      "epoch 61 | loss: 7.24841 |  0:02:53s\n",
      "epoch 62 | loss: 7.40819 |  0:02:56s\n",
      "epoch 63 | loss: 7.31176 |  0:02:59s\n",
      "epoch 64 | loss: 7.22037 |  0:03:02s\n",
      "epoch 65 | loss: 7.05938 |  0:03:05s\n",
      "epoch 66 | loss: 7.18209 |  0:03:07s\n",
      "epoch 67 | loss: 7.51987 |  0:03:10s\n",
      "epoch 68 | loss: 7.13639 |  0:03:13s\n",
      "epoch 69 | loss: 7.20382 |  0:03:16s\n",
      "epoch 70 | loss: 6.86909 |  0:03:18s\n",
      "epoch 71 | loss: 7.18116 |  0:03:22s\n",
      "epoch 72 | loss: 6.86266 |  0:03:24s\n",
      "epoch 73 | loss: 6.80157 |  0:03:27s\n",
      "epoch 74 | loss: 6.65165 |  0:03:30s\n",
      "epoch 75 | loss: 6.66417 |  0:03:33s\n",
      "epoch 76 | loss: 6.48887 |  0:03:36s\n",
      "epoch 77 | loss: 6.92711 |  0:03:38s\n",
      "epoch 78 | loss: 6.55119 |  0:03:41s\n",
      "epoch 79 | loss: 6.5447  |  0:03:44s\n",
      "epoch 80 | loss: 6.533   |  0:03:47s\n",
      "epoch 81 | loss: 6.35541 |  0:03:49s\n",
      "epoch 82 | loss: 6.32027 |  0:03:52s\n",
      "epoch 83 | loss: 6.31786 |  0:03:55s\n",
      "epoch 84 | loss: 6.17682 |  0:03:58s\n",
      "epoch 85 | loss: 6.01709 |  0:04:00s\n",
      "epoch 86 | loss: 5.95284 |  0:04:04s\n",
      "epoch 87 | loss: 5.95632 |  0:04:07s\n",
      "epoch 88 | loss: 5.97712 |  0:04:09s\n",
      "epoch 89 | loss: 5.76018 |  0:04:12s\n",
      "epoch 90 | loss: 5.9971  |  0:04:15s\n",
      "epoch 91 | loss: 6.21359 |  0:04:18s\n",
      "epoch 92 | loss: 6.00993 |  0:04:20s\n",
      "epoch 93 | loss: 6.14474 |  0:04:23s\n",
      "epoch 94 | loss: 5.85755 |  0:04:26s\n",
      "epoch 95 | loss: 5.91584 |  0:04:29s\n",
      "epoch 96 | loss: 5.95264 |  0:04:32s\n",
      "epoch 97 | loss: 5.91177 |  0:04:35s\n",
      "epoch 98 | loss: 5.86494 |  0:04:38s\n",
      "epoch 99 | loss: 5.72191 |  0:04:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 4\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 146.6271|  0:00:02s\n",
      "epoch 1  | loss: 30.77543|  0:00:05s\n",
      "epoch 2  | loss: 21.87766|  0:00:08s\n",
      "epoch 3  | loss: 19.13821|  0:00:11s\n",
      "epoch 4  | loss: 16.62367|  0:00:13s\n",
      "epoch 5  | loss: 15.00702|  0:00:16s\n",
      "epoch 6  | loss: 14.32301|  0:00:19s\n",
      "epoch 7  | loss: 13.25127|  0:00:22s\n",
      "epoch 8  | loss: 12.31086|  0:00:25s\n",
      "epoch 9  | loss: 12.73958|  0:00:28s\n",
      "epoch 10 | loss: 11.89858|  0:00:30s\n",
      "epoch 11 | loss: 11.18751|  0:00:33s\n",
      "epoch 12 | loss: 10.56992|  0:00:36s\n",
      "epoch 13 | loss: 10.61298|  0:00:39s\n",
      "epoch 14 | loss: 10.17008|  0:00:41s\n",
      "epoch 15 | loss: 9.87352 |  0:00:44s\n",
      "epoch 16 | loss: 9.92676 |  0:00:47s\n",
      "epoch 17 | loss: 9.60071 |  0:00:50s\n",
      "epoch 18 | loss: 9.34921 |  0:00:53s\n",
      "epoch 19 | loss: 9.22937 |  0:00:56s\n",
      "epoch 20 | loss: 9.26085 |  0:00:58s\n",
      "epoch 21 | loss: 9.09689 |  0:01:01s\n",
      "epoch 22 | loss: 8.75638 |  0:01:04s\n",
      "epoch 23 | loss: 8.67706 |  0:01:06s\n",
      "epoch 24 | loss: 8.53365 |  0:01:09s\n",
      "epoch 25 | loss: 8.48344 |  0:01:12s\n",
      "epoch 26 | loss: 8.57473 |  0:01:15s\n",
      "epoch 27 | loss: 8.39615 |  0:01:18s\n",
      "epoch 28 | loss: 8.09879 |  0:01:21s\n",
      "epoch 29 | loss: 7.97872 |  0:01:24s\n",
      "epoch 30 | loss: 8.01297 |  0:01:27s\n",
      "epoch 31 | loss: 7.91007 |  0:01:29s\n",
      "epoch 32 | loss: 7.82057 |  0:01:32s\n",
      "epoch 33 | loss: 7.62099 |  0:01:35s\n",
      "epoch 34 | loss: 7.59668 |  0:01:38s\n",
      "epoch 35 | loss: 7.57213 |  0:01:40s\n",
      "epoch 36 | loss: 7.55346 |  0:01:43s\n",
      "epoch 37 | loss: 7.39897 |  0:01:46s\n",
      "epoch 38 | loss: 7.41573 |  0:01:49s\n",
      "epoch 39 | loss: 7.17939 |  0:01:51s\n",
      "epoch 40 | loss: 7.2962  |  0:01:54s\n",
      "epoch 41 | loss: 6.98782 |  0:01:57s\n",
      "epoch 42 | loss: 6.99254 |  0:02:00s\n",
      "epoch 43 | loss: 6.93808 |  0:02:03s\n",
      "epoch 44 | loss: 6.99892 |  0:02:05s\n",
      "epoch 45 | loss: 6.93845 |  0:02:08s\n",
      "epoch 46 | loss: 7.35352 |  0:02:11s\n",
      "epoch 47 | loss: 6.93759 |  0:02:14s\n",
      "epoch 48 | loss: 6.74904 |  0:02:17s\n",
      "epoch 49 | loss: 6.60632 |  0:02:20s\n",
      "epoch 50 | loss: 6.74167 |  0:02:22s\n",
      "epoch 51 | loss: 6.60241 |  0:02:25s\n",
      "epoch 52 | loss: 6.33682 |  0:02:28s\n",
      "epoch 53 | loss: 6.44349 |  0:02:31s\n",
      "epoch 54 | loss: 6.38329 |  0:02:34s\n",
      "epoch 55 | loss: 6.45172 |  0:02:36s\n",
      "epoch 56 | loss: 6.35429 |  0:02:39s\n",
      "epoch 57 | loss: 6.27221 |  0:02:42s\n",
      "epoch 58 | loss: 6.15971 |  0:02:45s\n",
      "epoch 59 | loss: 6.57491 |  0:02:48s\n",
      "epoch 60 | loss: 6.62043 |  0:02:51s\n",
      "epoch 61 | loss: 6.65623 |  0:02:53s\n",
      "epoch 62 | loss: 6.21157 |  0:02:56s\n",
      "epoch 63 | loss: 6.12778 |  0:02:59s\n",
      "epoch 64 | loss: 5.93036 |  0:03:02s\n",
      "epoch 65 | loss: 5.89982 |  0:03:04s\n",
      "epoch 66 | loss: 5.99314 |  0:03:07s\n",
      "epoch 67 | loss: 6.26195 |  0:03:10s\n",
      "epoch 68 | loss: 5.78956 |  0:03:13s\n",
      "epoch 69 | loss: 5.73763 |  0:03:16s\n",
      "epoch 70 | loss: 5.79698 |  0:03:18s\n",
      "epoch 71 | loss: 5.58831 |  0:03:21s\n",
      "epoch 72 | loss: 5.67258 |  0:03:24s\n",
      "epoch 73 | loss: 5.58928 |  0:03:27s\n",
      "epoch 74 | loss: 5.61216 |  0:03:30s\n",
      "epoch 75 | loss: 5.61303 |  0:03:32s\n",
      "epoch 76 | loss: 5.42775 |  0:03:35s\n",
      "epoch 77 | loss: 5.52761 |  0:03:38s\n",
      "epoch 78 | loss: 5.39543 |  0:03:41s\n",
      "epoch 79 | loss: 5.45097 |  0:03:43s\n",
      "epoch 80 | loss: 5.3779  |  0:03:46s\n",
      "epoch 81 | loss: 5.3556  |  0:03:49s\n",
      "epoch 82 | loss: 5.28338 |  0:03:52s\n",
      "epoch 83 | loss: 5.24516 |  0:03:54s\n",
      "epoch 84 | loss: 5.29708 |  0:03:57s\n",
      "epoch 85 | loss: 5.20489 |  0:04:00s\n",
      "epoch 86 | loss: 5.10559 |  0:04:03s\n",
      "epoch 87 | loss: 5.23491 |  0:04:06s\n",
      "epoch 88 | loss: 5.11236 |  0:04:09s\n",
      "epoch 89 | loss: 5.13303 |  0:04:12s\n",
      "epoch 90 | loss: 5.47329 |  0:04:14s\n",
      "epoch 91 | loss: 5.09425 |  0:04:17s\n",
      "epoch 92 | loss: 5.13167 |  0:04:20s\n",
      "epoch 93 | loss: 4.92753 |  0:04:23s\n",
      "epoch 94 | loss: 5.13559 |  0:04:25s\n",
      "epoch 95 | loss: 5.19191 |  0:04:28s\n",
      "epoch 96 | loss: 5.11042 |  0:04:31s\n",
      "epoch 97 | loss: 5.04969 |  0:04:34s\n",
      "epoch 98 | loss: 4.9398  |  0:04:37s\n",
      "epoch 99 | loss: 4.91768 |  0:04:40s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 5\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 145.29473|  0:00:02s\n",
      "epoch 1  | loss: 25.60404|  0:00:05s\n",
      "epoch 2  | loss: 22.85997|  0:00:08s\n",
      "epoch 3  | loss: 24.26186|  0:00:11s\n",
      "epoch 4  | loss: 18.70113|  0:00:13s\n",
      "epoch 5  | loss: 17.69702|  0:00:16s\n",
      "epoch 6  | loss: 15.65206|  0:00:19s\n",
      "epoch 7  | loss: 14.44045|  0:00:22s\n",
      "epoch 8  | loss: 14.44687|  0:00:25s\n",
      "epoch 9  | loss: 13.57473|  0:00:28s\n",
      "epoch 10 | loss: 13.47223|  0:00:31s\n",
      "epoch 11 | loss: 12.83209|  0:00:34s\n",
      "epoch 12 | loss: 20.64597|  0:00:36s\n",
      "epoch 13 | loss: 18.73284|  0:00:39s\n",
      "epoch 14 | loss: 16.44706|  0:00:42s\n",
      "epoch 15 | loss: 15.08629|  0:00:45s\n",
      "epoch 16 | loss: 14.95435|  0:00:47s\n",
      "epoch 17 | loss: 14.46607|  0:00:50s\n",
      "epoch 18 | loss: 13.7183 |  0:00:54s\n",
      "epoch 19 | loss: 12.84889|  0:00:56s\n",
      "epoch 20 | loss: 12.78965|  0:00:59s\n",
      "epoch 21 | loss: 11.77379|  0:01:02s\n",
      "epoch 22 | loss: 11.55548|  0:01:05s\n",
      "epoch 23 | loss: 10.89075|  0:01:07s\n",
      "epoch 24 | loss: 10.59316|  0:01:10s\n",
      "epoch 25 | loss: 10.24307|  0:01:13s\n",
      "epoch 26 | loss: 10.51637|  0:01:15s\n",
      "epoch 27 | loss: 10.12715|  0:01:18s\n",
      "epoch 28 | loss: 9.96908 |  0:01:21s\n",
      "epoch 29 | loss: 9.67415 |  0:01:24s\n",
      "epoch 30 | loss: 9.78446 |  0:01:27s\n",
      "epoch 31 | loss: 9.46721 |  0:01:30s\n",
      "epoch 32 | loss: 9.30569 |  0:01:32s\n",
      "epoch 33 | loss: 9.12638 |  0:01:35s\n",
      "epoch 34 | loss: 9.12305 |  0:01:38s\n",
      "epoch 35 | loss: 8.85231 |  0:01:41s\n",
      "epoch 36 | loss: 9.24277 |  0:01:43s\n",
      "epoch 37 | loss: 8.67661 |  0:01:46s\n",
      "epoch 38 | loss: 8.63235 |  0:01:49s\n",
      "epoch 39 | loss: 8.47564 |  0:01:52s\n",
      "epoch 40 | loss: 8.35122 |  0:01:55s\n",
      "epoch 41 | loss: 8.28733 |  0:01:57s\n",
      "epoch 42 | loss: 8.28418 |  0:02:00s\n",
      "epoch 43 | loss: 8.03729 |  0:02:03s\n",
      "epoch 44 | loss: 7.91512 |  0:02:06s\n",
      "epoch 45 | loss: 7.90968 |  0:02:08s\n",
      "epoch 46 | loss: 7.59441 |  0:02:11s\n",
      "epoch 47 | loss: 7.73521 |  0:02:14s\n",
      "epoch 48 | loss: 7.70886 |  0:02:17s\n",
      "epoch 49 | loss: 7.68431 |  0:02:19s\n",
      "epoch 50 | loss: 7.8818  |  0:02:22s\n",
      "epoch 51 | loss: 8.01808 |  0:02:25s\n",
      "epoch 52 | loss: 8.01895 |  0:02:28s\n",
      "epoch 53 | loss: 7.76353 |  0:02:31s\n",
      "epoch 54 | loss: 7.48274 |  0:02:34s\n",
      "epoch 55 | loss: 7.52134 |  0:02:36s\n",
      "epoch 56 | loss: 7.50034 |  0:02:39s\n",
      "epoch 57 | loss: 7.53377 |  0:02:42s\n",
      "epoch 58 | loss: 7.59766 |  0:02:45s\n",
      "epoch 59 | loss: 7.76963 |  0:02:48s\n",
      "epoch 60 | loss: 7.6769  |  0:02:50s\n",
      "epoch 61 | loss: 7.74694 |  0:02:53s\n",
      "epoch 62 | loss: 7.50376 |  0:02:56s\n",
      "epoch 63 | loss: 7.34139 |  0:02:59s\n",
      "epoch 64 | loss: 7.1961  |  0:03:02s\n",
      "epoch 65 | loss: 6.95297 |  0:03:05s\n",
      "epoch 66 | loss: 6.92852 |  0:03:08s\n",
      "epoch 67 | loss: 6.65644 |  0:03:11s\n",
      "epoch 68 | loss: 6.73643 |  0:03:13s\n",
      "epoch 69 | loss: 6.83069 |  0:03:16s\n",
      "epoch 70 | loss: 6.58129 |  0:03:19s\n",
      "epoch 71 | loss: 6.6043  |  0:03:22s\n",
      "epoch 72 | loss: 6.42881 |  0:03:24s\n",
      "epoch 73 | loss: 6.50008 |  0:03:27s\n",
      "epoch 74 | loss: 6.39417 |  0:03:31s\n",
      "epoch 75 | loss: 6.90738 |  0:03:33s\n",
      "epoch 76 | loss: 6.67102 |  0:03:36s\n",
      "epoch 77 | loss: 6.74063 |  0:03:39s\n",
      "epoch 78 | loss: 6.561   |  0:03:42s\n",
      "epoch 79 | loss: 6.52312 |  0:03:45s\n",
      "epoch 80 | loss: 6.42506 |  0:03:47s\n",
      "epoch 81 | loss: 6.31175 |  0:03:50s\n",
      "epoch 82 | loss: 6.50083 |  0:03:53s\n",
      "epoch 83 | loss: 6.48779 |  0:03:56s\n",
      "epoch 84 | loss: 6.2728  |  0:03:59s\n",
      "epoch 85 | loss: 6.34332 |  0:04:02s\n",
      "epoch 86 | loss: 6.15637 |  0:04:05s\n",
      "epoch 87 | loss: 6.20422 |  0:04:07s\n",
      "epoch 88 | loss: 5.94156 |  0:04:10s\n",
      "epoch 89 | loss: 6.05571 |  0:04:13s\n",
      "epoch 90 | loss: 5.88455 |  0:04:16s\n",
      "epoch 91 | loss: 5.83397 |  0:04:19s\n",
      "epoch 92 | loss: 5.89925 |  0:04:21s\n",
      "epoch 93 | loss: 5.93306 |  0:04:24s\n",
      "epoch 94 | loss: 5.84166 |  0:04:27s\n",
      "epoch 95 | loss: 5.64359 |  0:04:30s\n",
      "epoch 96 | loss: 5.69157 |  0:04:33s\n",
      "epoch 97 | loss: 5.62042 |  0:04:36s\n",
      "epoch 98 | loss: 5.63372 |  0:04:39s\n",
      "epoch 99 | loss: 5.65184 |  0:04:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 6\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 146.90536|  0:00:02s\n",
      "epoch 1  | loss: 30.23839|  0:00:05s\n",
      "epoch 2  | loss: 23.3545 |  0:00:08s\n",
      "epoch 3  | loss: 22.89373|  0:00:11s\n",
      "epoch 4  | loss: 24.98406|  0:00:13s\n",
      "epoch 5  | loss: 21.13579|  0:00:16s\n",
      "epoch 6  | loss: 18.64803|  0:00:19s\n",
      "epoch 7  | loss: 18.27268|  0:00:22s\n",
      "epoch 8  | loss: 16.44538|  0:00:25s\n",
      "epoch 9  | loss: 15.48461|  0:00:28s\n",
      "epoch 10 | loss: 14.57673|  0:00:31s\n",
      "epoch 11 | loss: 13.48028|  0:00:33s\n",
      "epoch 12 | loss: 12.97952|  0:00:36s\n",
      "epoch 13 | loss: 12.89517|  0:00:39s\n",
      "epoch 14 | loss: 12.18145|  0:00:42s\n",
      "epoch 15 | loss: 12.18485|  0:00:45s\n",
      "epoch 16 | loss: 11.96026|  0:00:47s\n",
      "epoch 17 | loss: 11.83552|  0:00:51s\n",
      "epoch 18 | loss: 11.45943|  0:00:54s\n",
      "epoch 19 | loss: 11.04695|  0:00:56s\n",
      "epoch 20 | loss: 10.88057|  0:00:59s\n",
      "epoch 21 | loss: 10.83677|  0:01:02s\n",
      "epoch 22 | loss: 10.5296 |  0:01:05s\n",
      "epoch 23 | loss: 10.25304|  0:01:07s\n",
      "epoch 24 | loss: 10.3421 |  0:01:10s\n",
      "epoch 25 | loss: 10.3685 |  0:01:13s\n",
      "epoch 26 | loss: 10.26852|  0:01:16s\n",
      "epoch 27 | loss: 9.70386 |  0:01:18s\n",
      "epoch 28 | loss: 9.87329 |  0:01:21s\n",
      "epoch 29 | loss: 9.83238 |  0:01:24s\n",
      "epoch 30 | loss: 9.94726 |  0:01:27s\n",
      "epoch 31 | loss: 9.89063 |  0:01:30s\n",
      "epoch 32 | loss: 10.12897|  0:01:33s\n",
      "epoch 33 | loss: 9.55827 |  0:01:35s\n",
      "epoch 34 | loss: 9.58377 |  0:01:38s\n",
      "epoch 35 | loss: 16.52658|  0:01:41s\n",
      "epoch 36 | loss: 11.20968|  0:01:44s\n",
      "epoch 37 | loss: 9.90267 |  0:01:46s\n",
      "epoch 38 | loss: 9.41855 |  0:01:49s\n",
      "epoch 39 | loss: 9.04398 |  0:01:52s\n",
      "epoch 40 | loss: 9.00462 |  0:01:55s\n",
      "epoch 41 | loss: 8.82023 |  0:01:58s\n",
      "epoch 42 | loss: 8.79555 |  0:02:01s\n",
      "epoch 43 | loss: 8.61873 |  0:02:04s\n",
      "epoch 44 | loss: 8.46088 |  0:02:06s\n",
      "epoch 45 | loss: 8.31231 |  0:02:09s\n",
      "epoch 46 | loss: 8.23944 |  0:02:12s\n",
      "epoch 47 | loss: 8.22222 |  0:02:15s\n",
      "epoch 48 | loss: 7.92848 |  0:02:17s\n",
      "epoch 49 | loss: 7.76498 |  0:02:20s\n",
      "epoch 50 | loss: 7.60592 |  0:02:23s\n",
      "epoch 51 | loss: 7.73288 |  0:02:26s\n",
      "epoch 52 | loss: 7.55576 |  0:02:29s\n",
      "epoch 53 | loss: 7.49091 |  0:02:32s\n",
      "epoch 54 | loss: 7.25219 |  0:02:35s\n",
      "epoch 55 | loss: 7.2239  |  0:02:38s\n",
      "epoch 56 | loss: 7.37235 |  0:02:40s\n",
      "epoch 57 | loss: 7.27566 |  0:02:43s\n",
      "epoch 58 | loss: 7.01984 |  0:02:46s\n",
      "epoch 59 | loss: 7.08807 |  0:02:49s\n",
      "epoch 60 | loss: 6.92714 |  0:02:51s\n",
      "epoch 61 | loss: 7.0107  |  0:02:54s\n",
      "epoch 62 | loss: 6.78053 |  0:02:58s\n",
      "epoch 63 | loss: 6.69288 |  0:03:00s\n",
      "epoch 64 | loss: 6.71784 |  0:03:03s\n",
      "epoch 65 | loss: 6.63721 |  0:03:06s\n",
      "epoch 66 | loss: 6.7321  |  0:03:09s\n",
      "epoch 67 | loss: 6.70512 |  0:03:12s\n",
      "epoch 68 | loss: 6.46507 |  0:03:14s\n",
      "epoch 69 | loss: 6.49826 |  0:03:17s\n",
      "epoch 70 | loss: 6.52073 |  0:03:20s\n",
      "epoch 71 | loss: 6.48547 |  0:03:23s\n",
      "epoch 72 | loss: 6.34815 |  0:03:26s\n",
      "epoch 73 | loss: 6.44119 |  0:03:29s\n",
      "epoch 74 | loss: 6.35146 |  0:03:32s\n",
      "epoch 75 | loss: 6.24028 |  0:03:35s\n",
      "epoch 76 | loss: 6.16945 |  0:03:37s\n",
      "epoch 77 | loss: 6.17347 |  0:03:40s\n",
      "epoch 78 | loss: 6.08412 |  0:03:43s\n",
      "epoch 79 | loss: 5.92617 |  0:03:46s\n",
      "epoch 80 | loss: 6.13751 |  0:03:49s\n",
      "epoch 81 | loss: 6.04131 |  0:03:52s\n",
      "epoch 82 | loss: 5.92298 |  0:03:54s\n",
      "epoch 83 | loss: 5.85487 |  0:03:57s\n",
      "epoch 84 | loss: 5.85637 |  0:04:00s\n",
      "epoch 85 | loss: 5.82072 |  0:04:03s\n",
      "epoch 86 | loss: 5.83072 |  0:04:06s\n",
      "epoch 87 | loss: 5.80638 |  0:04:09s\n",
      "epoch 88 | loss: 5.78221 |  0:04:12s\n",
      "epoch 89 | loss: 5.78491 |  0:04:14s\n",
      "epoch 90 | loss: 5.82129 |  0:04:17s\n",
      "epoch 91 | loss: 5.61145 |  0:04:20s\n",
      "epoch 92 | loss: 5.64728 |  0:04:23s\n",
      "epoch 93 | loss: 5.5315  |  0:04:25s\n",
      "epoch 94 | loss: 5.60172 |  0:04:28s\n",
      "epoch 95 | loss: 5.5626  |  0:04:31s\n",
      "epoch 96 | loss: 5.47009 |  0:04:34s\n",
      "epoch 97 | loss: 5.46568 |  0:04:37s\n",
      "epoch 98 | loss: 5.53213 |  0:04:40s\n",
      "epoch 99 | loss: 5.54139 |  0:04:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 7\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 151.72597|  0:00:02s\n",
      "epoch 1  | loss: 27.60796|  0:00:05s\n",
      "epoch 2  | loss: 23.67622|  0:00:08s\n",
      "epoch 3  | loss: 23.63752|  0:00:10s\n",
      "epoch 4  | loss: 19.10315|  0:00:13s\n",
      "epoch 5  | loss: 16.99696|  0:00:16s\n",
      "epoch 6  | loss: 19.24651|  0:00:19s\n",
      "epoch 7  | loss: 19.91378|  0:00:22s\n",
      "epoch 8  | loss: 15.70221|  0:00:25s\n",
      "epoch 9  | loss: 15.97347|  0:00:28s\n",
      "epoch 10 | loss: 14.63042|  0:00:30s\n",
      "epoch 11 | loss: 14.28757|  0:00:33s\n",
      "epoch 12 | loss: 14.53909|  0:00:36s\n",
      "epoch 13 | loss: 13.18218|  0:00:39s\n",
      "epoch 14 | loss: 12.44036|  0:00:41s\n",
      "epoch 15 | loss: 12.68577|  0:00:44s\n",
      "epoch 16 | loss: 15.84904|  0:00:47s\n",
      "epoch 17 | loss: 12.12555|  0:00:50s\n",
      "epoch 18 | loss: 11.75695|  0:00:53s\n",
      "epoch 19 | loss: 11.5004 |  0:00:56s\n",
      "epoch 20 | loss: 11.44322|  0:00:59s\n",
      "epoch 21 | loss: 11.33388|  0:01:02s\n",
      "epoch 22 | loss: 10.79064|  0:01:04s\n",
      "epoch 23 | loss: 10.39323|  0:01:07s\n",
      "epoch 24 | loss: 10.88245|  0:01:10s\n",
      "epoch 25 | loss: 10.27793|  0:01:12s\n",
      "epoch 26 | loss: 10.42748|  0:01:15s\n",
      "epoch 27 | loss: 10.15519|  0:01:18s\n",
      "epoch 28 | loss: 10.07842|  0:01:22s\n",
      "epoch 29 | loss: 9.71119 |  0:01:24s\n",
      "epoch 30 | loss: 9.61826 |  0:01:27s\n",
      "epoch 31 | loss: 9.70001 |  0:01:30s\n",
      "epoch 32 | loss: 9.67673 |  0:01:33s\n",
      "epoch 33 | loss: 9.31062 |  0:01:35s\n",
      "epoch 34 | loss: 9.65778 |  0:01:38s\n",
      "epoch 35 | loss: 9.60907 |  0:01:41s\n",
      "epoch 36 | loss: 9.26014 |  0:01:44s\n",
      "epoch 37 | loss: 8.97822 |  0:01:46s\n",
      "epoch 38 | loss: 8.98744 |  0:01:49s\n",
      "epoch 39 | loss: 8.849   |  0:01:53s\n",
      "epoch 40 | loss: 8.76501 |  0:01:55s\n",
      "epoch 41 | loss: 8.6998  |  0:01:58s\n",
      "epoch 42 | loss: 8.70873 |  0:02:01s\n",
      "epoch 43 | loss: 8.63586 |  0:02:04s\n",
      "epoch 44 | loss: 8.48181 |  0:02:07s\n",
      "epoch 45 | loss: 8.54956 |  0:02:09s\n",
      "epoch 46 | loss: 8.26294 |  0:02:12s\n",
      "epoch 47 | loss: 8.29029 |  0:02:15s\n",
      "epoch 48 | loss: 8.17122 |  0:02:18s\n",
      "epoch 49 | loss: 8.1172  |  0:02:20s\n",
      "epoch 50 | loss: 7.93311 |  0:02:24s\n",
      "epoch 51 | loss: 7.8102  |  0:02:27s\n",
      "epoch 52 | loss: 7.99905 |  0:02:29s\n",
      "epoch 53 | loss: 7.8532  |  0:02:32s\n",
      "epoch 54 | loss: 7.86019 |  0:02:35s\n",
      "epoch 55 | loss: 7.60304 |  0:02:37s\n",
      "epoch 56 | loss: 7.78993 |  0:02:40s\n",
      "epoch 57 | loss: 7.53413 |  0:02:43s\n",
      "epoch 58 | loss: 7.47619 |  0:02:46s\n",
      "epoch 59 | loss: 7.41639 |  0:02:49s\n",
      "epoch 60 | loss: 7.36897 |  0:02:51s\n",
      "epoch 61 | loss: 7.43051 |  0:02:55s\n",
      "epoch 62 | loss: 7.25147 |  0:02:58s\n",
      "epoch 63 | loss: 7.53204 |  0:03:00s\n",
      "epoch 64 | loss: 7.37661 |  0:03:03s\n",
      "epoch 65 | loss: 7.15499 |  0:03:06s\n",
      "epoch 66 | loss: 7.25438 |  0:03:08s\n",
      "epoch 67 | loss: 7.15791 |  0:03:11s\n",
      "epoch 68 | loss: 8.04228 |  0:03:14s\n",
      "epoch 69 | loss: 9.25754 |  0:03:17s\n",
      "epoch 70 | loss: 7.29274 |  0:03:20s\n",
      "epoch 71 | loss: 7.01487 |  0:03:22s\n",
      "epoch 72 | loss: 6.80367 |  0:03:25s\n",
      "epoch 73 | loss: 7.09668 |  0:03:28s\n",
      "epoch 74 | loss: 6.90615 |  0:03:31s\n",
      "epoch 75 | loss: 6.89447 |  0:03:34s\n",
      "epoch 76 | loss: 6.7727  |  0:03:37s\n",
      "epoch 77 | loss: 6.60161 |  0:03:40s\n",
      "epoch 78 | loss: 6.49851 |  0:03:42s\n",
      "epoch 79 | loss: 6.57866 |  0:03:45s\n",
      "epoch 80 | loss: 6.63501 |  0:03:48s\n",
      "epoch 81 | loss: 6.59666 |  0:03:51s\n",
      "epoch 82 | loss: 6.42288 |  0:03:54s\n",
      "epoch 83 | loss: 6.51515 |  0:03:56s\n",
      "epoch 84 | loss: 6.43086 |  0:03:59s\n",
      "epoch 85 | loss: 6.34717 |  0:04:02s\n",
      "epoch 86 | loss: 6.33992 |  0:04:05s\n",
      "epoch 87 | loss: 6.221   |  0:04:08s\n",
      "epoch 88 | loss: 6.46514 |  0:04:11s\n",
      "epoch 89 | loss: 6.29757 |  0:04:13s\n",
      "epoch 90 | loss: 6.21042 |  0:04:16s\n",
      "epoch 91 | loss: 6.08085 |  0:04:19s\n",
      "epoch 92 | loss: 6.0116  |  0:04:22s\n",
      "epoch 93 | loss: 6.18733 |  0:04:25s\n",
      "epoch 94 | loss: 6.2097  |  0:04:28s\n",
      "epoch 95 | loss: 6.0409  |  0:04:31s\n",
      "epoch 96 | loss: 6.12209 |  0:04:34s\n",
      "epoch 97 | loss: 5.9061  |  0:04:36s\n",
      "epoch 98 | loss: 6.00782 |  0:04:39s\n",
      "epoch 99 | loss: 6.15477 |  0:04:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 8\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 150.87554|  0:00:02s\n",
      "epoch 1  | loss: 29.48768|  0:00:05s\n",
      "epoch 2  | loss: 25.45618|  0:00:08s\n",
      "epoch 3  | loss: 23.61327|  0:00:11s\n",
      "epoch 4  | loss: 19.39871|  0:00:14s\n",
      "epoch 5  | loss: 17.4686 |  0:00:17s\n",
      "epoch 6  | loss: 16.49167|  0:00:20s\n",
      "epoch 7  | loss: 14.86702|  0:00:23s\n",
      "epoch 8  | loss: 13.63145|  0:00:25s\n",
      "epoch 9  | loss: 12.81851|  0:00:28s\n",
      "epoch 10 | loss: 12.20549|  0:00:31s\n",
      "epoch 11 | loss: 11.43271|  0:00:33s\n",
      "epoch 12 | loss: 11.16028|  0:00:36s\n",
      "epoch 13 | loss: 11.06248|  0:00:39s\n",
      "epoch 14 | loss: 10.77662|  0:00:42s\n",
      "epoch 15 | loss: 10.596  |  0:00:45s\n",
      "epoch 16 | loss: 10.24328|  0:00:48s\n",
      "epoch 17 | loss: 10.03843|  0:00:51s\n",
      "epoch 18 | loss: 9.80399 |  0:00:54s\n",
      "epoch 19 | loss: 9.49474 |  0:00:56s\n",
      "epoch 20 | loss: 9.34399 |  0:00:59s\n",
      "epoch 21 | loss: 9.22139 |  0:01:02s\n",
      "epoch 22 | loss: 9.14557 |  0:01:04s\n",
      "epoch 23 | loss: 8.96317 |  0:01:07s\n",
      "epoch 24 | loss: 9.1142  |  0:01:10s\n",
      "epoch 25 | loss: 8.6857  |  0:01:13s\n",
      "epoch 26 | loss: 8.41754 |  0:01:16s\n",
      "epoch 27 | loss: 8.3136  |  0:01:19s\n",
      "epoch 28 | loss: 8.24817 |  0:01:22s\n",
      "epoch 29 | loss: 8.17846 |  0:01:24s\n",
      "epoch 30 | loss: 8.02592 |  0:01:27s\n",
      "epoch 31 | loss: 8.00933 |  0:01:30s\n",
      "epoch 32 | loss: 8.15599 |  0:01:33s\n",
      "epoch 33 | loss: 7.71277 |  0:01:36s\n",
      "epoch 34 | loss: 7.59407 |  0:01:39s\n",
      "epoch 35 | loss: 7.77476 |  0:01:41s\n",
      "epoch 36 | loss: 7.49187 |  0:01:44s\n",
      "epoch 37 | loss: 7.55657 |  0:01:47s\n",
      "epoch 38 | loss: 7.36234 |  0:01:50s\n",
      "epoch 39 | loss: 7.30787 |  0:01:53s\n",
      "epoch 40 | loss: 7.21422 |  0:01:56s\n",
      "epoch 41 | loss: 7.06303 |  0:01:58s\n",
      "epoch 42 | loss: 7.04672 |  0:02:01s\n",
      "epoch 43 | loss: 7.03919 |  0:02:04s\n",
      "epoch 44 | loss: 6.8161  |  0:02:07s\n",
      "epoch 45 | loss: 6.76948 |  0:02:09s\n",
      "epoch 46 | loss: 6.72605 |  0:02:12s\n",
      "epoch 47 | loss: 6.65591 |  0:02:15s\n",
      "epoch 48 | loss: 6.4429  |  0:02:18s\n",
      "epoch 49 | loss: 6.71384 |  0:02:21s\n",
      "epoch 50 | loss: 6.49546 |  0:02:24s\n",
      "epoch 51 | loss: 6.36623 |  0:02:27s\n",
      "epoch 52 | loss: 6.39453 |  0:02:29s\n",
      "epoch 53 | loss: 6.24122 |  0:02:32s\n",
      "epoch 54 | loss: 6.25295 |  0:02:35s\n",
      "epoch 55 | loss: 6.27999 |  0:02:38s\n",
      "epoch 56 | loss: 6.21102 |  0:02:41s\n",
      "epoch 57 | loss: 6.21067 |  0:02:43s\n",
      "epoch 58 | loss: 6.87466 |  0:02:46s\n",
      "epoch 59 | loss: 6.1216  |  0:02:49s\n",
      "epoch 60 | loss: 6.13082 |  0:02:52s\n",
      "epoch 61 | loss: 6.05486 |  0:02:55s\n",
      "epoch 62 | loss: 5.99561 |  0:02:58s\n",
      "epoch 63 | loss: 5.9773  |  0:03:00s\n",
      "epoch 64 | loss: 5.87115 |  0:03:03s\n",
      "epoch 65 | loss: 5.84515 |  0:03:06s\n",
      "epoch 66 | loss: 5.74672 |  0:03:09s\n",
      "epoch 67 | loss: 5.90662 |  0:03:11s\n",
      "epoch 68 | loss: 5.62485 |  0:03:14s\n",
      "epoch 69 | loss: 6.1499  |  0:03:17s\n",
      "epoch 70 | loss: 5.99867 |  0:03:20s\n",
      "epoch 71 | loss: 5.63435 |  0:03:23s\n",
      "epoch 72 | loss: 5.64402 |  0:03:26s\n",
      "epoch 73 | loss: 5.60075 |  0:03:29s\n",
      "epoch 74 | loss: 5.58832 |  0:03:31s\n",
      "epoch 75 | loss: 5.86375 |  0:03:34s\n",
      "epoch 76 | loss: 5.53046 |  0:03:37s\n",
      "epoch 77 | loss: 5.52856 |  0:03:40s\n",
      "epoch 78 | loss: 5.49307 |  0:03:43s\n",
      "epoch 79 | loss: 5.53036 |  0:03:46s\n",
      "epoch 80 | loss: 5.56525 |  0:03:48s\n",
      "epoch 81 | loss: 5.39518 |  0:03:51s\n",
      "epoch 82 | loss: 5.32921 |  0:03:54s\n",
      "epoch 83 | loss: 5.3123  |  0:03:57s\n",
      "epoch 84 | loss: 5.32265 |  0:04:00s\n",
      "epoch 85 | loss: 5.3243  |  0:04:03s\n",
      "epoch 86 | loss: 5.3076  |  0:04:06s\n",
      "epoch 87 | loss: 5.2726  |  0:04:08s\n",
      "epoch 88 | loss: 5.1402  |  0:04:11s\n",
      "epoch 89 | loss: 5.2392  |  0:04:14s\n",
      "epoch 90 | loss: 5.17889 |  0:04:17s\n",
      "epoch 91 | loss: 5.12265 |  0:04:20s\n",
      "epoch 92 | loss: 5.11593 |  0:04:22s\n",
      "epoch 93 | loss: 5.18673 |  0:04:25s\n",
      "epoch 94 | loss: 5.03128 |  0:04:28s\n",
      "epoch 95 | loss: 5.03004 |  0:04:31s\n",
      "epoch 96 | loss: 5.20776 |  0:04:34s\n",
      "epoch 97 | loss: 5.04236 |  0:04:37s\n",
      "epoch 98 | loss: 5.16852 |  0:04:39s\n",
      "epoch 99 | loss: 5.08637 |  0:04:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 9\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 147.02452|  0:00:02s\n",
      "epoch 1  | loss: 26.56422|  0:00:05s\n",
      "epoch 2  | loss: 21.66078|  0:00:08s\n",
      "epoch 3  | loss: 20.21433|  0:00:11s\n",
      "epoch 4  | loss: 19.66188|  0:00:14s\n",
      "epoch 5  | loss: 19.5084 |  0:00:16s\n",
      "epoch 6  | loss: 17.86144|  0:00:19s\n",
      "epoch 7  | loss: 15.91997|  0:00:22s\n",
      "epoch 8  | loss: 15.57125|  0:00:25s\n",
      "epoch 9  | loss: 14.64721|  0:00:28s\n",
      "epoch 10 | loss: 13.27483|  0:00:30s\n",
      "epoch 11 | loss: 12.71928|  0:00:33s\n",
      "epoch 12 | loss: 12.44109|  0:00:36s\n",
      "epoch 13 | loss: 12.20581|  0:00:39s\n",
      "epoch 14 | loss: 11.81136|  0:00:42s\n",
      "epoch 15 | loss: 11.66226|  0:00:45s\n",
      "epoch 16 | loss: 11.2705 |  0:00:48s\n",
      "epoch 17 | loss: 11.60586|  0:00:51s\n",
      "epoch 18 | loss: 10.88628|  0:00:53s\n",
      "epoch 19 | loss: 10.3766 |  0:00:56s\n",
      "epoch 20 | loss: 10.18986|  0:00:59s\n",
      "epoch 21 | loss: 10.27975|  0:01:02s\n",
      "epoch 22 | loss: 10.09838|  0:01:05s\n",
      "epoch 23 | loss: 9.99889 |  0:01:08s\n",
      "epoch 24 | loss: 9.59462 |  0:01:10s\n",
      "epoch 25 | loss: 9.42509 |  0:01:13s\n",
      "epoch 26 | loss: 9.38801 |  0:01:16s\n",
      "epoch 27 | loss: 9.28695 |  0:01:19s\n",
      "epoch 28 | loss: 9.00771 |  0:01:22s\n",
      "epoch 29 | loss: 9.05634 |  0:01:25s\n",
      "epoch 30 | loss: 8.98146 |  0:01:28s\n",
      "epoch 31 | loss: 8.77526 |  0:01:31s\n",
      "epoch 32 | loss: 8.63771 |  0:01:34s\n",
      "epoch 33 | loss: 8.41889 |  0:01:36s\n",
      "epoch 34 | loss: 14.43039|  0:01:39s\n",
      "epoch 35 | loss: 8.42399 |  0:01:42s\n",
      "epoch 36 | loss: 8.17502 |  0:01:45s\n",
      "epoch 37 | loss: 8.21384 |  0:01:48s\n",
      "epoch 38 | loss: 8.01431 |  0:01:51s\n",
      "epoch 39 | loss: 8.11314 |  0:01:53s\n",
      "epoch 40 | loss: 8.183   |  0:01:56s\n",
      "epoch 41 | loss: 8.22391 |  0:01:59s\n",
      "epoch 42 | loss: 7.96701 |  0:02:02s\n",
      "epoch 43 | loss: 7.82503 |  0:02:05s\n",
      "epoch 44 | loss: 7.70501 |  0:02:07s\n",
      "epoch 45 | loss: 7.75886 |  0:02:10s\n",
      "epoch 46 | loss: 7.64797 |  0:02:13s\n",
      "epoch 47 | loss: 7.62677 |  0:02:16s\n",
      "epoch 48 | loss: 7.62393 |  0:02:19s\n",
      "epoch 49 | loss: 7.80714 |  0:02:21s\n",
      "epoch 50 | loss: 7.37619 |  0:02:24s\n",
      "epoch 51 | loss: 7.87861 |  0:02:27s\n",
      "epoch 52 | loss: 7.50191 |  0:02:30s\n",
      "epoch 53 | loss: 7.26476 |  0:02:33s\n",
      "epoch 54 | loss: 7.10554 |  0:02:36s\n",
      "epoch 55 | loss: 7.00692 |  0:02:38s\n",
      "epoch 56 | loss: 7.04471 |  0:02:41s\n",
      "epoch 57 | loss: 6.94401 |  0:02:44s\n",
      "epoch 58 | loss: 6.82105 |  0:02:47s\n",
      "epoch 59 | loss: 6.91761 |  0:02:50s\n",
      "epoch 60 | loss: 7.23613 |  0:02:52s\n",
      "epoch 61 | loss: 6.75539 |  0:02:55s\n",
      "epoch 62 | loss: 6.85587 |  0:02:58s\n",
      "epoch 63 | loss: 6.75787 |  0:03:01s\n",
      "epoch 64 | loss: 6.71852 |  0:03:03s\n",
      "epoch 65 | loss: 7.18132 |  0:03:06s\n",
      "epoch 66 | loss: 6.90215 |  0:03:09s\n",
      "epoch 67 | loss: 6.69716 |  0:03:12s\n",
      "epoch 68 | loss: 6.98118 |  0:03:14s\n",
      "epoch 69 | loss: 6.61027 |  0:03:17s\n",
      "epoch 70 | loss: 6.50833 |  0:03:20s\n",
      "epoch 71 | loss: 6.46652 |  0:03:23s\n",
      "epoch 72 | loss: 6.79181 |  0:03:26s\n",
      "epoch 73 | loss: 7.12424 |  0:03:29s\n",
      "epoch 74 | loss: 6.93994 |  0:03:31s\n",
      "epoch 75 | loss: 6.49938 |  0:03:34s\n",
      "epoch 76 | loss: 6.31353 |  0:03:37s\n",
      "epoch 77 | loss: 6.35011 |  0:03:40s\n",
      "epoch 78 | loss: 6.2433  |  0:03:43s\n",
      "epoch 79 | loss: 6.14286 |  0:03:45s\n",
      "epoch 80 | loss: 6.16187 |  0:03:48s\n",
      "epoch 81 | loss: 6.0327  |  0:03:51s\n",
      "epoch 82 | loss: 6.00995 |  0:03:54s\n",
      "epoch 83 | loss: 5.91149 |  0:03:57s\n",
      "epoch 84 | loss: 5.95071 |  0:04:00s\n",
      "epoch 85 | loss: 5.83795 |  0:04:02s\n",
      "epoch 86 | loss: 5.82775 |  0:04:05s\n",
      "epoch 87 | loss: 5.83727 |  0:04:08s\n",
      "epoch 88 | loss: 5.74069 |  0:04:11s\n",
      "epoch 89 | loss: 5.81303 |  0:04:14s\n",
      "epoch 90 | loss: 5.5812  |  0:04:16s\n",
      "epoch 91 | loss: 5.64983 |  0:04:19s\n",
      "epoch 92 | loss: 5.60828 |  0:04:22s\n",
      "epoch 93 | loss: 5.72724 |  0:04:25s\n",
      "epoch 94 | loss: 5.48762 |  0:04:28s\n",
      "epoch 95 | loss: 5.46423 |  0:04:31s\n",
      "epoch 96 | loss: 5.46188 |  0:04:34s\n",
      "epoch 97 | loss: 5.42431 |  0:04:36s\n",
      "epoch 98 | loss: 5.46959 |  0:04:39s\n",
      "epoch 99 | loss: 5.48462 |  0:04:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 10\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 144.5776|  0:00:02s\n",
      "epoch 1  | loss: 29.67559|  0:00:05s\n",
      "epoch 2  | loss: 25.44301|  0:00:08s\n",
      "epoch 3  | loss: 22.58435|  0:00:11s\n",
      "epoch 4  | loss: 24.12669|  0:00:14s\n",
      "epoch 5  | loss: 22.43326|  0:00:17s\n",
      "epoch 6  | loss: 19.45927|  0:00:19s\n",
      "epoch 7  | loss: 18.04448|  0:00:22s\n",
      "epoch 8  | loss: 16.87052|  0:00:25s\n",
      "epoch 9  | loss: 16.12893|  0:00:28s\n",
      "epoch 10 | loss: 15.22559|  0:00:30s\n",
      "epoch 11 | loss: 14.45555|  0:00:33s\n",
      "epoch 12 | loss: 13.92423|  0:00:36s\n",
      "epoch 13 | loss: 13.58224|  0:00:39s\n",
      "epoch 14 | loss: 12.75406|  0:00:42s\n",
      "epoch 15 | loss: 12.29276|  0:00:45s\n",
      "epoch 16 | loss: 12.03891|  0:00:47s\n",
      "epoch 17 | loss: 12.03466|  0:00:50s\n",
      "epoch 18 | loss: 11.09996|  0:00:53s\n",
      "epoch 19 | loss: 10.88785|  0:00:56s\n",
      "epoch 20 | loss: 11.05324|  0:00:59s\n",
      "epoch 21 | loss: 10.40719|  0:01:01s\n",
      "epoch 22 | loss: 10.62673|  0:01:04s\n",
      "epoch 23 | loss: 10.23913|  0:01:07s\n",
      "epoch 24 | loss: 10.19654|  0:01:09s\n",
      "epoch 25 | loss: 10.00062|  0:01:12s\n",
      "epoch 26 | loss: 9.53437 |  0:01:15s\n",
      "epoch 27 | loss: 9.83586 |  0:01:18s\n",
      "epoch 28 | loss: 9.59418 |  0:01:21s\n",
      "epoch 29 | loss: 9.25307 |  0:01:24s\n",
      "epoch 30 | loss: 9.05458 |  0:01:27s\n",
      "epoch 31 | loss: 8.96091 |  0:01:30s\n",
      "epoch 32 | loss: 9.18382 |  0:01:32s\n",
      "epoch 33 | loss: 9.21376 |  0:01:35s\n",
      "epoch 34 | loss: 8.97963 |  0:01:38s\n",
      "epoch 35 | loss: 8.93533 |  0:01:40s\n",
      "epoch 36 | loss: 8.6573  |  0:01:43s\n",
      "epoch 37 | loss: 8.46408 |  0:01:46s\n",
      "epoch 38 | loss: 8.55449 |  0:01:49s\n",
      "epoch 39 | loss: 8.28086 |  0:01:52s\n",
      "epoch 40 | loss: 8.09394 |  0:01:55s\n",
      "epoch 41 | loss: 8.28823 |  0:01:58s\n",
      "epoch 42 | loss: 7.89748 |  0:02:00s\n",
      "epoch 43 | loss: 7.96469 |  0:02:03s\n",
      "epoch 44 | loss: 7.69029 |  0:02:06s\n",
      "epoch 45 | loss: 8.10699 |  0:02:09s\n",
      "epoch 46 | loss: 8.22343 |  0:02:12s\n",
      "epoch 47 | loss: 8.04641 |  0:02:14s\n",
      "epoch 48 | loss: 8.08645 |  0:02:18s\n",
      "epoch 49 | loss: 7.74365 |  0:02:20s\n",
      "epoch 50 | loss: 7.84666 |  0:02:24s\n",
      "epoch 51 | loss: 7.85955 |  0:02:26s\n",
      "epoch 52 | loss: 7.70646 |  0:02:29s\n",
      "epoch 53 | loss: 7.97146 |  0:02:32s\n",
      "epoch 54 | loss: 7.69868 |  0:02:35s\n",
      "epoch 55 | loss: 7.43809 |  0:02:37s\n",
      "epoch 56 | loss: 7.28014 |  0:02:40s\n",
      "epoch 57 | loss: 7.31703 |  0:02:43s\n",
      "epoch 58 | loss: 7.26108 |  0:02:46s\n",
      "epoch 59 | loss: 7.3874  |  0:02:49s\n",
      "epoch 60 | loss: 7.19471 |  0:02:51s\n",
      "epoch 61 | loss: 7.03048 |  0:02:54s\n",
      "epoch 62 | loss: 7.10776 |  0:02:57s\n",
      "epoch 63 | loss: 6.91947 |  0:03:00s\n",
      "epoch 64 | loss: 6.97342 |  0:03:02s\n",
      "epoch 65 | loss: 6.78956 |  0:03:05s\n",
      "epoch 66 | loss: 6.65451 |  0:03:08s\n",
      "epoch 67 | loss: 6.8962  |  0:03:11s\n",
      "epoch 68 | loss: 6.71707 |  0:03:14s\n",
      "epoch 69 | loss: 6.72907 |  0:03:16s\n",
      "epoch 70 | loss: 6.87684 |  0:03:20s\n",
      "epoch 71 | loss: 6.817   |  0:03:22s\n",
      "epoch 72 | loss: 6.75091 |  0:03:25s\n",
      "epoch 73 | loss: 6.75311 |  0:03:28s\n",
      "epoch 74 | loss: 6.67815 |  0:03:31s\n",
      "epoch 75 | loss: 6.56183 |  0:03:34s\n",
      "epoch 76 | loss: 6.52524 |  0:03:36s\n",
      "epoch 77 | loss: 6.33792 |  0:03:39s\n",
      "epoch 78 | loss: 6.57438 |  0:03:42s\n",
      "epoch 79 | loss: 6.48522 |  0:03:45s\n",
      "epoch 80 | loss: 6.43064 |  0:03:47s\n",
      "epoch 81 | loss: 6.50154 |  0:03:50s\n",
      "epoch 82 | loss: 6.475   |  0:03:54s\n",
      "epoch 83 | loss: 6.34009 |  0:03:56s\n",
      "epoch 84 | loss: 6.25621 |  0:03:59s\n",
      "epoch 85 | loss: 6.1914  |  0:04:02s\n",
      "epoch 86 | loss: 6.26669 |  0:04:05s\n",
      "epoch 87 | loss: 6.10013 |  0:04:07s\n",
      "epoch 88 | loss: 6.02792 |  0:04:10s\n",
      "epoch 89 | loss: 6.01862 |  0:04:13s\n",
      "epoch 90 | loss: 6.25868 |  0:04:16s\n",
      "epoch 91 | loss: 5.96716 |  0:04:19s\n",
      "epoch 92 | loss: 5.90234 |  0:04:21s\n",
      "epoch 93 | loss: 6.32849 |  0:04:25s\n",
      "epoch 94 | loss: 6.18435 |  0:04:28s\n",
      "epoch 95 | loss: 6.04459 |  0:04:30s\n",
      "epoch 96 | loss: 6.07943 |  0:04:33s\n",
      "epoch 97 | loss: 6.22833 |  0:04:36s\n",
      "epoch 98 | loss: 6.04623 |  0:04:39s\n",
      "epoch 99 | loss: 5.93323 |  0:04:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 11\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 145.49772|  0:00:02s\n",
      "epoch 1  | loss: 30.1889 |  0:00:05s\n",
      "epoch 2  | loss: 23.28537|  0:00:08s\n",
      "epoch 3  | loss: 22.05297|  0:00:11s\n",
      "epoch 4  | loss: 21.43136|  0:00:14s\n",
      "epoch 5  | loss: 19.13726|  0:00:17s\n",
      "epoch 6  | loss: 18.5685 |  0:00:20s\n",
      "epoch 7  | loss: 17.72673|  0:00:22s\n",
      "epoch 8  | loss: 15.8545 |  0:00:25s\n",
      "epoch 9  | loss: 16.01017|  0:00:28s\n",
      "epoch 10 | loss: 14.63818|  0:00:31s\n",
      "epoch 11 | loss: 14.8762 |  0:00:34s\n",
      "epoch 12 | loss: 13.73017|  0:00:37s\n",
      "epoch 13 | loss: 12.82407|  0:00:39s\n",
      "epoch 14 | loss: 15.54276|  0:00:42s\n",
      "epoch 15 | loss: 12.40996|  0:00:45s\n",
      "epoch 16 | loss: 11.59204|  0:00:48s\n",
      "epoch 17 | loss: 11.55884|  0:00:51s\n",
      "epoch 18 | loss: 11.3302 |  0:00:53s\n",
      "epoch 19 | loss: 11.034  |  0:00:56s\n",
      "epoch 20 | loss: 10.99924|  0:00:59s\n",
      "epoch 21 | loss: 10.94037|  0:01:02s\n",
      "epoch 22 | loss: 10.61443|  0:01:04s\n",
      "epoch 23 | loss: 10.27677|  0:01:07s\n",
      "epoch 24 | loss: 9.91116 |  0:01:10s\n",
      "epoch 25 | loss: 9.98891 |  0:01:13s\n",
      "epoch 26 | loss: 9.52043 |  0:01:16s\n",
      "epoch 27 | loss: 9.37375 |  0:01:19s\n",
      "epoch 28 | loss: 9.12598 |  0:01:22s\n",
      "epoch 29 | loss: 9.08862 |  0:01:24s\n",
      "epoch 30 | loss: 9.27774 |  0:01:27s\n",
      "epoch 31 | loss: 9.2485  |  0:01:30s\n",
      "epoch 32 | loss: 8.90264 |  0:01:33s\n",
      "epoch 33 | loss: 8.93746 |  0:01:35s\n",
      "epoch 34 | loss: 8.88661 |  0:01:38s\n",
      "epoch 35 | loss: 8.59369 |  0:01:41s\n",
      "epoch 36 | loss: 8.51602 |  0:01:44s\n",
      "epoch 37 | loss: 8.38554 |  0:01:47s\n",
      "epoch 38 | loss: 8.19345 |  0:01:50s\n",
      "epoch 39 | loss: 8.18472 |  0:01:53s\n",
      "epoch 40 | loss: 8.04035 |  0:01:55s\n",
      "epoch 41 | loss: 7.97457 |  0:01:58s\n",
      "epoch 42 | loss: 7.83079 |  0:02:01s\n",
      "epoch 43 | loss: 7.80302 |  0:02:04s\n",
      "epoch 44 | loss: 7.59819 |  0:02:06s\n",
      "epoch 45 | loss: 7.62796 |  0:02:09s\n",
      "epoch 46 | loss: 7.56205 |  0:02:12s\n",
      "epoch 47 | loss: 7.53522 |  0:02:15s\n",
      "epoch 48 | loss: 7.43091 |  0:02:18s\n",
      "epoch 49 | loss: 7.35773 |  0:02:21s\n",
      "epoch 50 | loss: 7.09292 |  0:02:23s\n",
      "epoch 51 | loss: 7.17643 |  0:02:26s\n",
      "epoch 52 | loss: 7.73331 |  0:02:29s\n",
      "epoch 53 | loss: 7.03479 |  0:02:32s\n",
      "epoch 54 | loss: 7.05501 |  0:02:34s\n",
      "epoch 55 | loss: 7.18811 |  0:02:37s\n",
      "epoch 56 | loss: 6.93313 |  0:02:40s\n",
      "epoch 57 | loss: 6.97337 |  0:02:43s\n",
      "epoch 58 | loss: 6.96688 |  0:02:46s\n",
      "epoch 59 | loss: 6.94639 |  0:02:49s\n",
      "epoch 60 | loss: 7.04552 |  0:02:52s\n",
      "epoch 61 | loss: 6.70463 |  0:02:55s\n",
      "epoch 62 | loss: 6.74729 |  0:02:57s\n",
      "epoch 63 | loss: 6.68803 |  0:03:00s\n",
      "epoch 64 | loss: 7.05552 |  0:03:03s\n",
      "epoch 65 | loss: 6.75069 |  0:03:06s\n",
      "epoch 66 | loss: 6.774   |  0:03:08s\n",
      "epoch 67 | loss: 6.44311 |  0:03:11s\n",
      "epoch 68 | loss: 6.62781 |  0:03:14s\n",
      "epoch 69 | loss: 6.60596 |  0:03:17s\n",
      "epoch 70 | loss: 6.61612 |  0:03:20s\n",
      "epoch 71 | loss: 6.84966 |  0:03:23s\n",
      "epoch 72 | loss: 6.51386 |  0:03:26s\n",
      "epoch 73 | loss: 6.50728 |  0:03:28s\n",
      "epoch 74 | loss: 6.50117 |  0:03:31s\n",
      "epoch 75 | loss: 6.427   |  0:03:34s\n",
      "epoch 76 | loss: 6.2178  |  0:03:37s\n",
      "epoch 77 | loss: 6.2603  |  0:03:39s\n",
      "epoch 78 | loss: 6.41582 |  0:03:42s\n",
      "epoch 79 | loss: 6.30221 |  0:03:45s\n",
      "epoch 80 | loss: 6.30878 |  0:03:48s\n",
      "epoch 81 | loss: 6.15428 |  0:03:51s\n",
      "epoch 82 | loss: 6.31777 |  0:03:54s\n",
      "epoch 83 | loss: 6.29808 |  0:03:56s\n",
      "epoch 84 | loss: 6.21959 |  0:03:59s\n",
      "epoch 85 | loss: 6.29218 |  0:04:02s\n",
      "epoch 86 | loss: 6.41838 |  0:04:04s\n",
      "epoch 87 | loss: 6.29803 |  0:04:07s\n",
      "epoch 88 | loss: 6.51152 |  0:04:10s\n",
      "epoch 89 | loss: 6.45999 |  0:04:13s\n",
      "epoch 90 | loss: 6.51001 |  0:04:16s\n",
      "epoch 91 | loss: 6.20527 |  0:04:18s\n",
      "epoch 92 | loss: 5.98664 |  0:04:21s\n",
      "epoch 93 | loss: 6.00586 |  0:04:24s\n",
      "epoch 94 | loss: 5.74758 |  0:04:27s\n",
      "epoch 95 | loss: 5.7141  |  0:04:30s\n",
      "epoch 96 | loss: 5.87299 |  0:04:32s\n",
      "epoch 97 | loss: 5.86694 |  0:04:35s\n",
      "epoch 98 | loss: 5.74823 |  0:04:38s\n",
      "epoch 99 | loss: 5.87501 |  0:04:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 12\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 145.18718|  0:00:02s\n",
      "epoch 1  | loss: 29.96157|  0:00:05s\n",
      "epoch 2  | loss: 24.14555|  0:00:08s\n",
      "epoch 3  | loss: 21.19184|  0:00:11s\n",
      "epoch 4  | loss: 21.38329|  0:00:14s\n",
      "epoch 5  | loss: 19.96732|  0:00:17s\n",
      "epoch 6  | loss: 20.46552|  0:00:20s\n",
      "epoch 7  | loss: 17.434  |  0:00:23s\n",
      "epoch 8  | loss: 16.55915|  0:00:25s\n",
      "epoch 9  | loss: 15.90212|  0:00:28s\n",
      "epoch 10 | loss: 16.16446|  0:00:31s\n",
      "epoch 11 | loss: 15.2865 |  0:00:34s\n",
      "epoch 12 | loss: 14.40261|  0:00:37s\n",
      "epoch 13 | loss: 13.73141|  0:00:40s\n",
      "epoch 14 | loss: 14.01923|  0:00:42s\n",
      "epoch 15 | loss: 13.57038|  0:00:45s\n",
      "epoch 16 | loss: 18.20024|  0:00:48s\n",
      "epoch 17 | loss: 34.80247|  0:00:51s\n",
      "epoch 18 | loss: 16.24526|  0:00:53s\n",
      "epoch 19 | loss: 13.85886|  0:00:56s\n",
      "epoch 20 | loss: 12.692  |  0:00:59s\n",
      "epoch 21 | loss: 12.49571|  0:01:02s\n",
      "epoch 22 | loss: 11.82766|  0:01:04s\n",
      "epoch 23 | loss: 12.24585|  0:01:07s\n",
      "epoch 24 | loss: 12.0739 |  0:01:10s\n",
      "epoch 25 | loss: 12.81876|  0:01:13s\n",
      "epoch 26 | loss: 11.68067|  0:01:16s\n",
      "epoch 27 | loss: 11.6188 |  0:01:19s\n",
      "epoch 28 | loss: 11.24583|  0:01:22s\n",
      "epoch 29 | loss: 10.84633|  0:01:24s\n",
      "epoch 30 | loss: 11.06576|  0:01:27s\n",
      "epoch 31 | loss: 10.6116 |  0:01:30s\n",
      "epoch 32 | loss: 10.75668|  0:01:32s\n",
      "epoch 33 | loss: 10.62635|  0:01:35s\n",
      "epoch 34 | loss: 10.20731|  0:01:38s\n",
      "epoch 35 | loss: 10.13427|  0:01:41s\n",
      "epoch 36 | loss: 10.2669 |  0:01:44s\n",
      "epoch 37 | loss: 10.25959|  0:01:47s\n",
      "epoch 38 | loss: 9.77106 |  0:01:49s\n",
      "epoch 39 | loss: 9.95331 |  0:01:52s\n",
      "epoch 40 | loss: 9.57989 |  0:01:55s\n",
      "epoch 41 | loss: 9.5389  |  0:01:58s\n",
      "epoch 42 | loss: 9.61489 |  0:02:00s\n",
      "epoch 43 | loss: 9.47984 |  0:02:03s\n",
      "epoch 44 | loss: 9.44966 |  0:02:06s\n",
      "epoch 45 | loss: 9.37425 |  0:02:09s\n",
      "epoch 46 | loss: 9.46503 |  0:02:11s\n",
      "epoch 47 | loss: 9.24045 |  0:02:15s\n",
      "epoch 48 | loss: 9.01315 |  0:02:18s\n",
      "epoch 49 | loss: 8.92619 |  0:02:20s\n",
      "epoch 50 | loss: 8.78047 |  0:02:23s\n",
      "epoch 51 | loss: 8.80256 |  0:02:26s\n",
      "epoch 52 | loss: 8.8371  |  0:02:29s\n",
      "epoch 53 | loss: 8.61682 |  0:02:31s\n",
      "epoch 54 | loss: 8.62503 |  0:02:34s\n",
      "epoch 55 | loss: 9.14436 |  0:02:37s\n",
      "epoch 56 | loss: 8.92732 |  0:02:40s\n",
      "epoch 57 | loss: 9.06778 |  0:02:43s\n",
      "epoch 58 | loss: 8.52257 |  0:02:46s\n",
      "epoch 59 | loss: 8.22311 |  0:02:49s\n",
      "epoch 60 | loss: 8.37935 |  0:02:51s\n",
      "epoch 61 | loss: 8.19905 |  0:02:54s\n",
      "epoch 62 | loss: 8.2176  |  0:02:57s\n",
      "epoch 63 | loss: 8.07072 |  0:03:00s\n",
      "epoch 64 | loss: 7.8925  |  0:03:03s\n",
      "epoch 65 | loss: 7.82053 |  0:03:05s\n",
      "epoch 66 | loss: 7.75428 |  0:03:08s\n",
      "epoch 67 | loss: 7.80568 |  0:03:11s\n",
      "epoch 68 | loss: 7.62603 |  0:03:14s\n",
      "epoch 69 | loss: 7.74061 |  0:03:17s\n",
      "epoch 70 | loss: 7.43182 |  0:03:20s\n",
      "epoch 71 | loss: 7.66479 |  0:03:23s\n",
      "epoch 72 | loss: 7.31694 |  0:03:25s\n",
      "epoch 73 | loss: 7.36082 |  0:03:28s\n",
      "epoch 74 | loss: 7.30576 |  0:03:31s\n",
      "epoch 75 | loss: 7.10192 |  0:03:34s\n",
      "epoch 76 | loss: 7.21498 |  0:03:36s\n",
      "epoch 77 | loss: 7.29308 |  0:03:39s\n",
      "epoch 78 | loss: 7.09961 |  0:03:42s\n",
      "epoch 79 | loss: 6.98514 |  0:03:45s\n",
      "epoch 80 | loss: 6.9259  |  0:03:47s\n",
      "epoch 81 | loss: 7.1061  |  0:03:50s\n",
      "epoch 82 | loss: 6.90572 |  0:03:53s\n",
      "epoch 83 | loss: 7.00636 |  0:03:56s\n",
      "epoch 84 | loss: 7.06742 |  0:03:58s\n",
      "epoch 85 | loss: 6.97256 |  0:04:01s\n",
      "epoch 86 | loss: 6.67329 |  0:04:04s\n",
      "epoch 87 | loss: 6.8066  |  0:04:07s\n",
      "epoch 88 | loss: 6.54491 |  0:04:09s\n",
      "epoch 89 | loss: 6.56347 |  0:04:12s\n",
      "epoch 90 | loss: 6.49301 |  0:04:15s\n",
      "epoch 91 | loss: 6.49998 |  0:04:18s\n",
      "epoch 92 | loss: 6.38553 |  0:04:21s\n",
      "epoch 93 | loss: 6.45467 |  0:04:24s\n",
      "epoch 94 | loss: 6.37214 |  0:04:26s\n",
      "epoch 95 | loss: 6.52542 |  0:04:29s\n",
      "epoch 96 | loss: 6.45781 |  0:04:32s\n",
      "epoch 97 | loss: 6.4066  |  0:04:34s\n",
      "epoch 98 | loss: 6.31792 |  0:04:37s\n",
      "epoch 99 | loss: 6.26315 |  0:04:40s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 13\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 148.33722|  0:00:02s\n",
      "epoch 1  | loss: 29.8776 |  0:00:05s\n",
      "epoch 2  | loss: 22.14457|  0:00:08s\n",
      "epoch 3  | loss: 20.90298|  0:00:11s\n",
      "epoch 4  | loss: 19.20052|  0:00:14s\n",
      "epoch 5  | loss: 18.0152 |  0:00:17s\n",
      "epoch 6  | loss: 17.84553|  0:00:20s\n",
      "epoch 7  | loss: 21.59359|  0:00:22s\n",
      "epoch 8  | loss: 17.78375|  0:00:25s\n",
      "epoch 9  | loss: 24.3994 |  0:00:28s\n",
      "epoch 10 | loss: 24.78864|  0:00:31s\n",
      "epoch 11 | loss: 16.3063 |  0:00:33s\n",
      "epoch 12 | loss: 14.96244|  0:00:36s\n",
      "epoch 13 | loss: 13.72406|  0:00:39s\n",
      "epoch 14 | loss: 13.40007|  0:00:42s\n",
      "epoch 15 | loss: 12.52471|  0:00:45s\n",
      "epoch 16 | loss: 13.0944 |  0:00:47s\n",
      "epoch 17 | loss: 11.53772|  0:00:50s\n",
      "epoch 18 | loss: 11.33474|  0:00:53s\n",
      "epoch 19 | loss: 11.08701|  0:00:56s\n",
      "epoch 20 | loss: 11.6654 |  0:00:58s\n",
      "epoch 21 | loss: 11.17774|  0:01:01s\n",
      "epoch 22 | loss: 10.76572|  0:01:04s\n",
      "epoch 23 | loss: 11.13546|  0:01:06s\n",
      "epoch 24 | loss: 10.88648|  0:01:09s\n",
      "epoch 25 | loss: 10.75914|  0:01:12s\n",
      "epoch 26 | loss: 10.52153|  0:01:15s\n",
      "epoch 27 | loss: 10.22437|  0:01:18s\n",
      "epoch 28 | loss: 10.15378|  0:01:20s\n",
      "epoch 29 | loss: 10.05884|  0:01:23s\n",
      "epoch 30 | loss: 9.70832 |  0:01:26s\n",
      "epoch 31 | loss: 9.649   |  0:01:29s\n",
      "epoch 32 | loss: 9.38923 |  0:01:32s\n",
      "epoch 33 | loss: 9.41031 |  0:01:35s\n",
      "epoch 34 | loss: 9.25274 |  0:01:37s\n",
      "epoch 35 | loss: 9.36325 |  0:01:40s\n",
      "epoch 36 | loss: 9.09403 |  0:01:43s\n",
      "epoch 37 | loss: 9.3607  |  0:01:46s\n",
      "epoch 38 | loss: 9.92762 |  0:01:49s\n",
      "epoch 39 | loss: 9.51754 |  0:01:51s\n",
      "epoch 40 | loss: 9.17978 |  0:01:54s\n",
      "epoch 41 | loss: 9.24477 |  0:01:57s\n",
      "epoch 42 | loss: 8.90859 |  0:02:00s\n",
      "epoch 43 | loss: 9.51336 |  0:02:03s\n",
      "epoch 44 | loss: 9.43698 |  0:02:05s\n",
      "epoch 45 | loss: 9.0145  |  0:02:08s\n",
      "epoch 46 | loss: 8.73187 |  0:02:11s\n",
      "epoch 47 | loss: 8.85692 |  0:02:14s\n",
      "epoch 48 | loss: 8.39765 |  0:02:17s\n",
      "epoch 49 | loss: 8.1786  |  0:02:20s\n",
      "epoch 50 | loss: 8.11308 |  0:02:23s\n",
      "epoch 51 | loss: 8.21588 |  0:02:25s\n",
      "epoch 52 | loss: 8.28273 |  0:02:28s\n",
      "epoch 53 | loss: 7.92277 |  0:02:31s\n",
      "epoch 54 | loss: 7.93424 |  0:02:34s\n",
      "epoch 55 | loss: 7.66249 |  0:02:36s\n",
      "epoch 56 | loss: 7.60904 |  0:02:39s\n",
      "epoch 57 | loss: 7.58261 |  0:02:42s\n",
      "epoch 58 | loss: 7.48416 |  0:02:45s\n",
      "epoch 59 | loss: 7.48286 |  0:02:48s\n",
      "epoch 60 | loss: 7.46217 |  0:02:50s\n",
      "epoch 61 | loss: 7.33259 |  0:02:53s\n",
      "epoch 62 | loss: 7.42517 |  0:02:56s\n",
      "epoch 63 | loss: 7.14359 |  0:02:59s\n",
      "epoch 64 | loss: 7.06417 |  0:03:01s\n",
      "epoch 65 | loss: 7.03523 |  0:03:04s\n",
      "epoch 66 | loss: 6.92398 |  0:03:07s\n",
      "epoch 67 | loss: 6.87643 |  0:03:10s\n",
      "epoch 68 | loss: 6.93405 |  0:03:13s\n",
      "epoch 69 | loss: 6.74974 |  0:03:16s\n",
      "epoch 70 | loss: 6.6301  |  0:03:19s\n",
      "epoch 71 | loss: 6.69642 |  0:03:21s\n",
      "epoch 72 | loss: 6.62602 |  0:03:24s\n",
      "epoch 73 | loss: 6.57031 |  0:03:27s\n",
      "epoch 74 | loss: 6.59255 |  0:03:30s\n",
      "epoch 75 | loss: 6.39844 |  0:03:33s\n",
      "epoch 76 | loss: 6.45502 |  0:03:36s\n",
      "epoch 77 | loss: 6.29355 |  0:03:38s\n",
      "epoch 78 | loss: 6.25566 |  0:03:41s\n",
      "epoch 79 | loss: 6.26821 |  0:03:44s\n",
      "epoch 80 | loss: 6.15736 |  0:03:47s\n",
      "epoch 81 | loss: 6.13935 |  0:03:50s\n",
      "epoch 82 | loss: 5.9981  |  0:03:52s\n",
      "epoch 83 | loss: 6.15078 |  0:03:55s\n",
      "epoch 84 | loss: 6.07604 |  0:03:58s\n",
      "epoch 85 | loss: 6.10109 |  0:04:01s\n",
      "epoch 86 | loss: 5.96509 |  0:04:03s\n",
      "epoch 87 | loss: 6.01565 |  0:04:07s\n",
      "epoch 88 | loss: 5.97693 |  0:04:09s\n",
      "epoch 89 | loss: 5.86798 |  0:04:12s\n",
      "epoch 90 | loss: 5.73051 |  0:04:15s\n",
      "epoch 91 | loss: 5.82627 |  0:04:17s\n",
      "epoch 92 | loss: 5.8159  |  0:04:21s\n",
      "epoch 93 | loss: 5.78247 |  0:04:24s\n",
      "epoch 94 | loss: 5.72386 |  0:04:26s\n",
      "epoch 95 | loss: 5.68052 |  0:04:29s\n",
      "epoch 96 | loss: 5.71888 |  0:04:32s\n",
      "epoch 97 | loss: 5.80576 |  0:04:35s\n",
      "epoch 98 | loss: 5.55306 |  0:04:38s\n",
      "epoch 99 | loss: 5.61247 |  0:04:40s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 14\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 146.9907|  0:00:02s\n",
      "epoch 1  | loss: 29.66886|  0:00:05s\n",
      "epoch 2  | loss: 26.98091|  0:00:08s\n",
      "epoch 3  | loss: 26.11897|  0:00:11s\n",
      "epoch 4  | loss: 33.73483|  0:00:13s\n",
      "epoch 5  | loss: 27.67311|  0:00:16s\n",
      "epoch 6  | loss: 23.36644|  0:00:19s\n",
      "epoch 7  | loss: 22.54176|  0:00:22s\n",
      "epoch 8  | loss: 19.52911|  0:00:25s\n",
      "epoch 9  | loss: 18.80266|  0:00:27s\n",
      "epoch 10 | loss: 19.48485|  0:00:30s\n",
      "epoch 11 | loss: 33.93211|  0:00:33s\n",
      "epoch 12 | loss: 18.45128|  0:00:36s\n",
      "epoch 13 | loss: 17.80453|  0:00:39s\n",
      "epoch 14 | loss: 17.37896|  0:00:43s\n",
      "epoch 15 | loss: 18.45228|  0:00:46s\n",
      "epoch 16 | loss: 17.23836|  0:00:48s\n",
      "epoch 17 | loss: 17.94934|  0:00:51s\n",
      "epoch 18 | loss: 16.55931|  0:00:54s\n",
      "epoch 19 | loss: 16.29524|  0:00:57s\n",
      "epoch 20 | loss: 16.3154 |  0:00:59s\n",
      "epoch 21 | loss: 17.09133|  0:01:02s\n",
      "epoch 22 | loss: 17.52981|  0:01:05s\n",
      "epoch 23 | loss: 17.51105|  0:01:07s\n",
      "epoch 24 | loss: 17.15196|  0:01:10s\n",
      "epoch 25 | loss: 17.12294|  0:01:13s\n",
      "epoch 26 | loss: 18.12997|  0:01:16s\n",
      "epoch 27 | loss: 15.80429|  0:01:19s\n",
      "epoch 28 | loss: 15.33537|  0:01:21s\n",
      "epoch 29 | loss: 15.13289|  0:01:24s\n",
      "epoch 30 | loss: 14.81543|  0:01:27s\n",
      "epoch 31 | loss: 14.67764|  0:01:30s\n",
      "epoch 32 | loss: 16.27334|  0:01:32s\n",
      "epoch 33 | loss: 15.80842|  0:01:35s\n",
      "epoch 34 | loss: 15.93012|  0:01:38s\n",
      "epoch 35 | loss: 14.63657|  0:01:41s\n",
      "epoch 36 | loss: 15.25061|  0:01:44s\n",
      "epoch 37 | loss: 16.14354|  0:01:47s\n",
      "epoch 38 | loss: 16.47159|  0:01:50s\n",
      "epoch 39 | loss: 15.94971|  0:01:52s\n",
      "epoch 40 | loss: 14.34671|  0:01:55s\n",
      "epoch 41 | loss: 13.81104|  0:01:58s\n",
      "epoch 42 | loss: 13.52539|  0:02:01s\n",
      "epoch 43 | loss: 13.61607|  0:02:04s\n",
      "epoch 44 | loss: 13.97499|  0:02:06s\n",
      "epoch 45 | loss: 13.70846|  0:02:09s\n",
      "epoch 46 | loss: 15.91246|  0:02:12s\n",
      "epoch 47 | loss: 18.04568|  0:02:15s\n",
      "epoch 48 | loss: 14.82185|  0:02:18s\n",
      "epoch 49 | loss: 14.69856|  0:02:21s\n",
      "epoch 50 | loss: 14.70125|  0:02:24s\n",
      "epoch 51 | loss: 14.27675|  0:02:27s\n",
      "epoch 52 | loss: 13.65363|  0:02:29s\n",
      "epoch 53 | loss: 13.39627|  0:02:32s\n",
      "epoch 54 | loss: 13.2191 |  0:02:35s\n",
      "epoch 55 | loss: 13.11039|  0:02:38s\n",
      "epoch 56 | loss: 13.08153|  0:02:40s\n",
      "epoch 57 | loss: 13.0471 |  0:02:43s\n",
      "epoch 58 | loss: 12.95355|  0:02:47s\n",
      "epoch 59 | loss: 12.53615|  0:02:49s\n",
      "epoch 60 | loss: 12.67032|  0:02:52s\n",
      "epoch 61 | loss: 12.04809|  0:02:55s\n",
      "epoch 62 | loss: 11.61872|  0:02:58s\n",
      "epoch 63 | loss: 10.82295|  0:03:01s\n",
      "epoch 64 | loss: 10.62909|  0:03:03s\n",
      "epoch 65 | loss: 10.27805|  0:03:06s\n",
      "epoch 66 | loss: 10.44214|  0:03:09s\n",
      "epoch 67 | loss: 10.74593|  0:03:12s\n",
      "epoch 68 | loss: 11.35636|  0:03:14s\n",
      "epoch 69 | loss: 10.17277|  0:03:18s\n",
      "epoch 70 | loss: 9.9634  |  0:03:21s\n",
      "epoch 71 | loss: 9.58354 |  0:03:23s\n",
      "epoch 72 | loss: 9.43348 |  0:03:26s\n",
      "epoch 73 | loss: 9.26456 |  0:03:29s\n",
      "epoch 74 | loss: 9.03616 |  0:03:32s\n",
      "epoch 75 | loss: 8.91024 |  0:03:35s\n",
      "epoch 76 | loss: 8.79373 |  0:03:37s\n",
      "epoch 77 | loss: 8.83975 |  0:03:40s\n",
      "epoch 78 | loss: 8.56705 |  0:03:43s\n",
      "epoch 79 | loss: 8.35885 |  0:03:46s\n",
      "epoch 80 | loss: 8.34697 |  0:03:49s\n",
      "epoch 81 | loss: 8.39258 |  0:03:52s\n",
      "epoch 82 | loss: 8.20455 |  0:03:55s\n",
      "epoch 83 | loss: 8.10348 |  0:03:57s\n",
      "epoch 84 | loss: 7.89784 |  0:04:00s\n",
      "epoch 85 | loss: 7.85394 |  0:04:03s\n",
      "epoch 86 | loss: 7.75006 |  0:04:06s\n",
      "epoch 87 | loss: 7.6269  |  0:04:09s\n",
      "epoch 88 | loss: 7.57471 |  0:04:11s\n",
      "epoch 89 | loss: 7.47122 |  0:04:14s\n",
      "epoch 90 | loss: 7.67578 |  0:04:17s\n",
      "epoch 91 | loss: 7.63093 |  0:04:20s\n",
      "epoch 92 | loss: 7.52673 |  0:04:23s\n",
      "epoch 93 | loss: 7.36119 |  0:04:26s\n",
      "epoch 94 | loss: 7.41938 |  0:04:28s\n",
      "epoch 95 | loss: 7.11891 |  0:04:31s\n",
      "epoch 96 | loss: 7.2236  |  0:04:34s\n",
      "epoch 97 | loss: 7.21354 |  0:04:37s\n",
      "epoch 98 | loss: 6.9814  |  0:04:40s\n",
      "epoch 99 | loss: 7.08225 |  0:04:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 15\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 151.21572|  0:00:02s\n",
      "epoch 1  | loss: 28.76091|  0:00:05s\n",
      "epoch 2  | loss: 23.30173|  0:00:08s\n",
      "epoch 3  | loss: 21.71411|  0:00:11s\n",
      "epoch 4  | loss: 18.35167|  0:00:14s\n",
      "epoch 5  | loss: 16.74518|  0:00:17s\n",
      "epoch 6  | loss: 17.82505|  0:00:19s\n",
      "epoch 7  | loss: 15.45871|  0:00:22s\n",
      "epoch 8  | loss: 14.30762|  0:00:25s\n",
      "epoch 9  | loss: 13.70376|  0:00:28s\n",
      "epoch 10 | loss: 13.51366|  0:00:30s\n",
      "epoch 11 | loss: 12.8726 |  0:00:33s\n",
      "epoch 12 | loss: 13.12095|  0:00:36s\n",
      "epoch 13 | loss: 11.99912|  0:00:39s\n",
      "epoch 14 | loss: 11.72234|  0:00:42s\n",
      "epoch 15 | loss: 11.75222|  0:00:45s\n",
      "epoch 16 | loss: 11.42866|  0:00:48s\n",
      "epoch 17 | loss: 11.25497|  0:00:50s\n",
      "epoch 18 | loss: 10.96619|  0:00:53s\n",
      "epoch 19 | loss: 10.71202|  0:00:56s\n",
      "epoch 20 | loss: 10.89676|  0:00:59s\n",
      "epoch 21 | loss: 10.20553|  0:01:01s\n",
      "epoch 22 | loss: 9.99981 |  0:01:04s\n",
      "epoch 23 | loss: 9.8567  |  0:01:07s\n",
      "epoch 24 | loss: 9.80614 |  0:01:10s\n",
      "epoch 25 | loss: 10.06849|  0:01:13s\n",
      "epoch 26 | loss: 10.56367|  0:01:16s\n",
      "epoch 27 | loss: 9.93453 |  0:01:19s\n",
      "epoch 28 | loss: 9.48973 |  0:01:21s\n",
      "epoch 29 | loss: 9.48423 |  0:01:24s\n",
      "epoch 30 | loss: 9.11561 |  0:01:27s\n",
      "epoch 31 | loss: 8.85813 |  0:01:30s\n",
      "epoch 32 | loss: 8.72693 |  0:01:32s\n",
      "epoch 33 | loss: 8.57325 |  0:01:35s\n",
      "epoch 34 | loss: 8.45926 |  0:01:38s\n",
      "epoch 35 | loss: 8.88846 |  0:01:41s\n",
      "epoch 36 | loss: 8.56435 |  0:01:44s\n",
      "epoch 37 | loss: 8.60453 |  0:01:47s\n",
      "epoch 38 | loss: 9.98723 |  0:01:50s\n",
      "epoch 39 | loss: 9.65021 |  0:01:52s\n",
      "epoch 40 | loss: 10.85787|  0:01:55s\n",
      "epoch 41 | loss: 9.53793 |  0:01:58s\n",
      "epoch 42 | loss: 8.95343 |  0:02:01s\n",
      "epoch 43 | loss: 8.72615 |  0:02:03s\n",
      "epoch 44 | loss: 8.81529 |  0:02:06s\n",
      "epoch 45 | loss: 8.6218  |  0:02:09s\n",
      "epoch 46 | loss: 8.3575  |  0:02:12s\n",
      "epoch 47 | loss: 8.62329 |  0:02:15s\n",
      "epoch 48 | loss: 8.55603 |  0:02:18s\n",
      "epoch 49 | loss: 7.79758 |  0:02:20s\n",
      "epoch 50 | loss: 7.80498 |  0:02:23s\n",
      "epoch 51 | loss: 7.65461 |  0:02:26s\n",
      "epoch 52 | loss: 7.64298 |  0:02:28s\n",
      "epoch 53 | loss: 7.513   |  0:02:31s\n",
      "epoch 54 | loss: 7.57013 |  0:02:34s\n",
      "epoch 55 | loss: 7.58926 |  0:02:37s\n",
      "epoch 56 | loss: 7.47523 |  0:02:39s\n",
      "epoch 57 | loss: 7.4225  |  0:02:43s\n",
      "epoch 58 | loss: 7.15301 |  0:02:46s\n",
      "epoch 59 | loss: 7.00778 |  0:02:48s\n",
      "epoch 60 | loss: 6.87505 |  0:02:51s\n",
      "epoch 61 | loss: 6.92903 |  0:02:54s\n",
      "epoch 62 | loss: 6.81451 |  0:02:57s\n",
      "epoch 63 | loss: 6.77033 |  0:03:00s\n",
      "epoch 64 | loss: 6.8545  |  0:03:02s\n",
      "epoch 65 | loss: 6.871   |  0:03:05s\n",
      "epoch 66 | loss: 6.68518 |  0:03:08s\n",
      "epoch 67 | loss: 6.70318 |  0:03:11s\n",
      "epoch 68 | loss: 6.57248 |  0:03:14s\n",
      "epoch 69 | loss: 6.68588 |  0:03:17s\n",
      "epoch 70 | loss: 6.72873 |  0:03:20s\n",
      "epoch 71 | loss: 6.4376  |  0:03:23s\n",
      "epoch 72 | loss: 6.50975 |  0:03:25s\n",
      "epoch 73 | loss: 6.20865 |  0:03:28s\n",
      "epoch 74 | loss: 6.41976 |  0:03:31s\n",
      "epoch 75 | loss: 6.25902 |  0:03:34s\n",
      "epoch 76 | loss: 6.17673 |  0:03:36s\n",
      "epoch 77 | loss: 6.17125 |  0:03:39s\n",
      "epoch 78 | loss: 6.03734 |  0:03:42s\n",
      "epoch 79 | loss: 6.17178 |  0:03:45s\n",
      "epoch 80 | loss: 6.09757 |  0:03:48s\n",
      "epoch 81 | loss: 6.17539 |  0:03:51s\n",
      "epoch 82 | loss: 5.98299 |  0:03:54s\n",
      "epoch 83 | loss: 5.91506 |  0:03:56s\n",
      "epoch 84 | loss: 5.99581 |  0:03:59s\n",
      "epoch 85 | loss: 6.00491 |  0:04:02s\n",
      "epoch 86 | loss: 5.93083 |  0:04:05s\n",
      "epoch 87 | loss: 5.92448 |  0:04:08s\n",
      "epoch 88 | loss: 5.84893 |  0:04:10s\n",
      "epoch 89 | loss: 5.74131 |  0:04:13s\n",
      "epoch 90 | loss: 5.82969 |  0:04:16s\n",
      "epoch 91 | loss: 5.86292 |  0:04:19s\n",
      "epoch 92 | loss: 5.63593 |  0:04:22s\n",
      "epoch 93 | loss: 5.78557 |  0:04:25s\n",
      "epoch 94 | loss: 5.76502 |  0:04:28s\n",
      "epoch 95 | loss: 5.68011 |  0:04:30s\n",
      "epoch 96 | loss: 5.60331 |  0:04:33s\n",
      "epoch 97 | loss: 5.52795 |  0:04:36s\n",
      "epoch 98 | loss: 5.47047 |  0:04:39s\n",
      "epoch 99 | loss: 5.49734 |  0:04:41s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 16\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 148.23604|  0:00:03s\n",
      "epoch 1  | loss: 29.31938|  0:00:06s\n",
      "epoch 2  | loss: 23.19788|  0:00:08s\n",
      "epoch 3  | loss: 19.2165 |  0:00:11s\n",
      "epoch 4  | loss: 16.18241|  0:00:14s\n",
      "epoch 5  | loss: 15.10435|  0:00:17s\n",
      "epoch 6  | loss: 13.94649|  0:00:20s\n",
      "epoch 7  | loss: 12.84248|  0:00:22s\n",
      "epoch 8  | loss: 11.89803|  0:00:25s\n",
      "epoch 9  | loss: 11.20357|  0:00:28s\n",
      "epoch 10 | loss: 11.37096|  0:00:31s\n",
      "epoch 11 | loss: 11.15201|  0:00:33s\n",
      "epoch 12 | loss: 10.47358|  0:00:37s\n",
      "epoch 13 | loss: 10.47713|  0:00:40s\n",
      "epoch 14 | loss: 10.03987|  0:00:42s\n",
      "epoch 15 | loss: 9.99763 |  0:00:45s\n",
      "epoch 16 | loss: 9.6724  |  0:00:48s\n",
      "epoch 17 | loss: 9.47944 |  0:00:51s\n",
      "epoch 18 | loss: 9.49178 |  0:00:54s\n",
      "epoch 19 | loss: 9.2816  |  0:00:57s\n",
      "epoch 20 | loss: 9.08086 |  0:00:59s\n",
      "epoch 21 | loss: 9.10183 |  0:01:02s\n",
      "epoch 22 | loss: 8.98857 |  0:01:05s\n",
      "epoch 23 | loss: 8.87135 |  0:01:08s\n",
      "epoch 24 | loss: 8.77124 |  0:01:11s\n",
      "epoch 25 | loss: 8.69698 |  0:01:14s\n",
      "epoch 26 | loss: 8.49805 |  0:01:16s\n",
      "epoch 27 | loss: 8.47138 |  0:01:19s\n",
      "epoch 28 | loss: 8.50017 |  0:01:22s\n",
      "epoch 29 | loss: 8.32525 |  0:01:25s\n",
      "epoch 30 | loss: 8.15886 |  0:01:27s\n",
      "epoch 31 | loss: 8.11393 |  0:01:30s\n",
      "epoch 32 | loss: 8.18486 |  0:01:33s\n",
      "epoch 33 | loss: 7.88112 |  0:01:36s\n",
      "epoch 34 | loss: 7.79972 |  0:01:39s\n",
      "epoch 35 | loss: 7.89628 |  0:01:42s\n",
      "epoch 36 | loss: 7.65547 |  0:01:45s\n",
      "epoch 37 | loss: 7.56097 |  0:01:47s\n",
      "epoch 38 | loss: 7.58085 |  0:01:50s\n",
      "epoch 39 | loss: 7.36786 |  0:01:53s\n",
      "epoch 40 | loss: 7.36268 |  0:01:56s\n",
      "epoch 41 | loss: 7.23885 |  0:01:58s\n",
      "epoch 42 | loss: 7.37954 |  0:02:01s\n",
      "epoch 43 | loss: 7.23116 |  0:02:04s\n",
      "epoch 44 | loss: 7.21515 |  0:02:07s\n",
      "epoch 45 | loss: 7.01426 |  0:02:10s\n",
      "epoch 46 | loss: 6.85444 |  0:02:13s\n",
      "epoch 47 | loss: 7.01385 |  0:02:16s\n",
      "epoch 48 | loss: 6.85945 |  0:02:19s\n",
      "epoch 49 | loss: 6.91877 |  0:02:21s\n",
      "epoch 50 | loss: 6.91109 |  0:02:24s\n",
      "epoch 51 | loss: 6.97209 |  0:02:27s\n",
      "epoch 52 | loss: 6.69101 |  0:02:30s\n",
      "epoch 53 | loss: 6.65851 |  0:02:32s\n",
      "epoch 54 | loss: 6.62793 |  0:02:35s\n",
      "epoch 55 | loss: 6.62739 |  0:02:38s\n",
      "epoch 56 | loss: 6.5605  |  0:02:41s\n",
      "epoch 57 | loss: 6.49009 |  0:02:44s\n",
      "epoch 58 | loss: 6.46178 |  0:02:47s\n",
      "epoch 59 | loss: 6.40561 |  0:02:50s\n",
      "epoch 60 | loss: 6.2687  |  0:02:52s\n",
      "epoch 61 | loss: 6.2735  |  0:02:55s\n",
      "epoch 62 | loss: 6.25844 |  0:02:58s\n",
      "epoch 63 | loss: 6.33187 |  0:03:01s\n",
      "epoch 64 | loss: 6.32464 |  0:03:03s\n",
      "epoch 65 | loss: 6.11528 |  0:03:06s\n",
      "epoch 66 | loss: 6.12001 |  0:03:09s\n",
      "epoch 67 | loss: 6.1359  |  0:03:12s\n",
      "epoch 68 | loss: 6.04534 |  0:03:15s\n",
      "epoch 69 | loss: 5.93819 |  0:03:18s\n",
      "epoch 70 | loss: 5.91649 |  0:03:21s\n",
      "epoch 71 | loss: 5.90951 |  0:03:23s\n",
      "epoch 72 | loss: 5.88665 |  0:03:26s\n",
      "epoch 73 | loss: 5.9019  |  0:03:29s\n",
      "epoch 74 | loss: 5.87741 |  0:03:31s\n",
      "epoch 75 | loss: 5.93857 |  0:03:34s\n",
      "epoch 76 | loss: 5.77254 |  0:03:37s\n",
      "epoch 77 | loss: 5.75549 |  0:03:40s\n",
      "epoch 78 | loss: 5.7141  |  0:03:43s\n",
      "epoch 79 | loss: 5.69314 |  0:03:46s\n",
      "epoch 80 | loss: 5.7702  |  0:03:49s\n",
      "epoch 81 | loss: 5.58584 |  0:03:52s\n",
      "epoch 82 | loss: 5.61991 |  0:03:54s\n",
      "epoch 83 | loss: 5.51565 |  0:03:57s\n",
      "epoch 84 | loss: 5.50006 |  0:04:00s\n",
      "epoch 85 | loss: 5.593   |  0:04:03s\n",
      "epoch 86 | loss: 5.54302 |  0:04:05s\n",
      "epoch 87 | loss: 5.56818 |  0:04:08s\n",
      "epoch 88 | loss: 5.54804 |  0:04:11s\n",
      "epoch 89 | loss: 5.54063 |  0:04:14s\n",
      "epoch 90 | loss: 5.46011 |  0:04:17s\n",
      "epoch 91 | loss: 5.42954 |  0:04:20s\n",
      "epoch 92 | loss: 5.38021 |  0:04:22s\n",
      "epoch 93 | loss: 5.41042 |  0:04:26s\n",
      "epoch 94 | loss: 5.48433 |  0:04:28s\n",
      "epoch 95 | loss: 5.33735 |  0:04:31s\n",
      "epoch 96 | loss: 5.19541 |  0:04:34s\n",
      "epoch 97 | loss: 5.21026 |  0:04:37s\n",
      "epoch 98 | loss: 5.21382 |  0:04:40s\n",
      "epoch 99 | loss: 5.20726 |  0:04:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 17\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 148.63062|  0:00:03s\n",
      "epoch 1  | loss: 28.95937|  0:00:06s\n",
      "epoch 2  | loss: 23.68539|  0:00:08s\n",
      "epoch 3  | loss: 23.68478|  0:00:11s\n",
      "epoch 4  | loss: 19.04347|  0:00:14s\n",
      "epoch 5  | loss: 17.7189 |  0:00:17s\n",
      "epoch 6  | loss: 18.51848|  0:00:19s\n",
      "epoch 7  | loss: 19.0353 |  0:00:22s\n",
      "epoch 8  | loss: 14.97094|  0:00:25s\n",
      "epoch 9  | loss: 13.22594|  0:00:28s\n",
      "epoch 10 | loss: 12.76025|  0:00:31s\n",
      "epoch 11 | loss: 12.36225|  0:00:34s\n",
      "epoch 12 | loss: 12.03362|  0:00:37s\n",
      "epoch 13 | loss: 11.61599|  0:00:39s\n",
      "epoch 14 | loss: 11.19869|  0:00:42s\n",
      "epoch 15 | loss: 10.91836|  0:00:45s\n",
      "epoch 16 | loss: 10.44852|  0:00:47s\n",
      "epoch 17 | loss: 10.25653|  0:00:50s\n",
      "epoch 18 | loss: 10.20189|  0:00:53s\n",
      "epoch 19 | loss: 9.90026 |  0:00:56s\n",
      "epoch 20 | loss: 9.69548 |  0:00:59s\n",
      "epoch 21 | loss: 9.73738 |  0:01:01s\n",
      "epoch 22 | loss: 9.50868 |  0:01:05s\n",
      "epoch 23 | loss: 9.28002 |  0:01:07s\n",
      "epoch 24 | loss: 10.10354|  0:01:10s\n",
      "epoch 25 | loss: 9.42442 |  0:01:13s\n",
      "epoch 26 | loss: 9.18066 |  0:01:16s\n",
      "epoch 27 | loss: 8.99831 |  0:01:19s\n",
      "epoch 28 | loss: 8.99302 |  0:01:21s\n",
      "epoch 29 | loss: 8.69021 |  0:01:24s\n",
      "epoch 30 | loss: 8.73054 |  0:01:27s\n",
      "epoch 31 | loss: 8.56397 |  0:01:30s\n",
      "epoch 32 | loss: 8.70016 |  0:01:33s\n",
      "epoch 33 | loss: 8.39243 |  0:01:36s\n",
      "epoch 34 | loss: 8.25157 |  0:01:39s\n",
      "epoch 35 | loss: 8.20115 |  0:01:42s\n",
      "epoch 36 | loss: 8.22368 |  0:01:44s\n",
      "epoch 37 | loss: 8.01848 |  0:01:47s\n",
      "epoch 38 | loss: 8.09871 |  0:01:50s\n",
      "epoch 39 | loss: 7.88328 |  0:01:53s\n",
      "epoch 40 | loss: 7.86115 |  0:01:55s\n",
      "epoch 41 | loss: 7.75452 |  0:01:58s\n",
      "epoch 42 | loss: 7.70117 |  0:02:01s\n",
      "epoch 43 | loss: 7.49573 |  0:02:04s\n",
      "epoch 44 | loss: 7.81223 |  0:02:07s\n",
      "epoch 45 | loss: 7.59853 |  0:02:10s\n",
      "epoch 46 | loss: 7.61967 |  0:02:13s\n",
      "epoch 47 | loss: 7.51082 |  0:02:16s\n",
      "epoch 48 | loss: 7.46543 |  0:02:18s\n",
      "epoch 49 | loss: 7.26574 |  0:02:21s\n",
      "epoch 50 | loss: 7.21915 |  0:02:24s\n",
      "epoch 51 | loss: 7.18884 |  0:02:27s\n",
      "epoch 52 | loss: 7.05453 |  0:02:29s\n",
      "epoch 53 | loss: 7.03901 |  0:02:32s\n",
      "epoch 54 | loss: 6.91686 |  0:02:35s\n",
      "epoch 55 | loss: 6.94858 |  0:02:38s\n",
      "epoch 56 | loss: 6.93242 |  0:02:41s\n",
      "epoch 57 | loss: 6.97393 |  0:02:44s\n",
      "epoch 58 | loss: 6.86836 |  0:02:47s\n",
      "epoch 59 | loss: 6.67381 |  0:02:49s\n",
      "epoch 60 | loss: 6.62354 |  0:02:52s\n",
      "epoch 61 | loss: 6.58969 |  0:02:55s\n",
      "epoch 62 | loss: 6.6771  |  0:02:58s\n",
      "epoch 63 | loss: 6.50012 |  0:03:00s\n",
      "epoch 64 | loss: 6.44452 |  0:03:03s\n",
      "epoch 65 | loss: 6.45075 |  0:03:06s\n",
      "epoch 66 | loss: 6.31069 |  0:03:09s\n",
      "epoch 67 | loss: 6.34597 |  0:03:12s\n",
      "epoch 68 | loss: 6.26476 |  0:03:15s\n",
      "epoch 69 | loss: 6.28816 |  0:03:17s\n",
      "epoch 70 | loss: 6.58302 |  0:03:20s\n",
      "epoch 71 | loss: 6.36263 |  0:03:23s\n",
      "epoch 72 | loss: 6.24719 |  0:03:26s\n",
      "epoch 73 | loss: 6.14796 |  0:03:28s\n",
      "epoch 74 | loss: 6.04741 |  0:03:31s\n",
      "epoch 75 | loss: 5.98333 |  0:03:34s\n",
      "epoch 76 | loss: 6.10524 |  0:03:37s\n",
      "epoch 77 | loss: 5.96885 |  0:03:40s\n",
      "epoch 78 | loss: 5.88208 |  0:03:43s\n",
      "epoch 79 | loss: 5.86235 |  0:03:46s\n",
      "epoch 80 | loss: 5.86598 |  0:03:49s\n",
      "epoch 81 | loss: 5.88467 |  0:03:51s\n",
      "epoch 82 | loss: 5.99084 |  0:03:54s\n",
      "epoch 83 | loss: 5.66751 |  0:03:57s\n",
      "epoch 84 | loss: 5.81029 |  0:04:00s\n",
      "epoch 85 | loss: 5.89092 |  0:04:03s\n",
      "epoch 86 | loss: 5.68583 |  0:04:06s\n",
      "epoch 87 | loss: 5.77932 |  0:04:09s\n",
      "epoch 88 | loss: 5.91218 |  0:04:11s\n",
      "epoch 89 | loss: 5.69988 |  0:04:15s\n",
      "epoch 90 | loss: 5.58215 |  0:04:17s\n",
      "epoch 91 | loss: 5.45261 |  0:04:20s\n",
      "epoch 92 | loss: 5.40803 |  0:04:23s\n",
      "epoch 93 | loss: 5.4931  |  0:04:26s\n",
      "epoch 94 | loss: 5.48862 |  0:04:29s\n",
      "epoch 95 | loss: 5.42764 |  0:04:31s\n",
      "epoch 96 | loss: 5.45269 |  0:04:34s\n",
      "epoch 97 | loss: 5.48234 |  0:04:37s\n",
      "epoch 98 | loss: 5.3876  |  0:04:40s\n",
      "epoch 99 | loss: 5.33062 |  0:04:43s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 18\n",
      "(46820,)\n",
      "(46820, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 154.33243|  0:00:02s\n",
      "epoch 1  | loss: 29.6186 |  0:00:05s\n",
      "epoch 2  | loss: 24.68119|  0:00:08s\n",
      "epoch 3  | loss: 21.48843|  0:00:11s\n",
      "epoch 4  | loss: 19.19708|  0:00:14s\n",
      "epoch 5  | loss: 16.72328|  0:00:16s\n",
      "epoch 6  | loss: 15.49621|  0:00:19s\n",
      "epoch 7  | loss: 15.7777 |  0:00:22s\n",
      "epoch 8  | loss: 14.11733|  0:00:25s\n",
      "epoch 9  | loss: 13.34834|  0:00:28s\n",
      "epoch 10 | loss: 12.74193|  0:00:31s\n",
      "epoch 11 | loss: 12.81393|  0:00:33s\n",
      "epoch 12 | loss: 11.98686|  0:00:36s\n",
      "epoch 13 | loss: 12.02584|  0:00:39s\n",
      "epoch 14 | loss: 11.65167|  0:00:42s\n",
      "epoch 15 | loss: 11.41206|  0:00:44s\n",
      "epoch 16 | loss: 17.50116|  0:00:47s\n",
      "epoch 17 | loss: 11.16322|  0:00:50s\n",
      "epoch 18 | loss: 10.66988|  0:00:53s\n",
      "epoch 19 | loss: 10.81375|  0:00:55s\n",
      "epoch 20 | loss: 10.61685|  0:00:58s\n",
      "epoch 21 | loss: 10.40517|  0:01:01s\n",
      "epoch 22 | loss: 10.24871|  0:01:04s\n",
      "epoch 23 | loss: 10.06812|  0:01:07s\n",
      "epoch 24 | loss: 9.99337 |  0:01:10s\n",
      "epoch 25 | loss: 9.90096 |  0:01:13s\n",
      "epoch 26 | loss: 9.64398 |  0:01:15s\n",
      "epoch 27 | loss: 9.56258 |  0:01:18s\n",
      "epoch 28 | loss: 9.78528 |  0:01:21s\n",
      "epoch 29 | loss: 9.36891 |  0:01:24s\n",
      "epoch 30 | loss: 9.32904 |  0:01:27s\n",
      "epoch 31 | loss: 9.47425 |  0:01:30s\n",
      "epoch 32 | loss: 9.26423 |  0:01:33s\n",
      "epoch 33 | loss: 8.92707 |  0:01:36s\n",
      "epoch 34 | loss: 9.18947 |  0:01:38s\n",
      "epoch 35 | loss: 9.12143 |  0:01:41s\n",
      "epoch 36 | loss: 10.92479|  0:01:44s\n",
      "epoch 37 | loss: 8.80868 |  0:01:47s\n",
      "epoch 38 | loss: 8.89713 |  0:01:49s\n",
      "epoch 39 | loss: 9.22671 |  0:01:53s\n",
      "epoch 40 | loss: 8.59987 |  0:01:55s\n",
      "epoch 41 | loss: 8.72405 |  0:01:58s\n",
      "epoch 42 | loss: 8.95503 |  0:02:01s\n",
      "epoch 43 | loss: 8.76184 |  0:02:04s\n",
      "epoch 44 | loss: 8.36209 |  0:02:07s\n",
      "epoch 45 | loss: 8.26239 |  0:02:10s\n",
      "epoch 46 | loss: 10.14025|  0:02:12s\n",
      "epoch 47 | loss: 8.57416 |  0:02:15s\n",
      "epoch 48 | loss: 8.72756 |  0:02:18s\n",
      "epoch 49 | loss: 8.63995 |  0:02:21s\n",
      "epoch 50 | loss: 8.71719 |  0:02:24s\n",
      "epoch 51 | loss: 8.41613 |  0:02:27s\n",
      "epoch 52 | loss: 15.92196|  0:02:29s\n",
      "epoch 53 | loss: 9.92068 |  0:02:32s\n",
      "epoch 54 | loss: 8.81374 |  0:02:35s\n",
      "epoch 55 | loss: 8.59407 |  0:02:38s\n",
      "epoch 56 | loss: 8.35103 |  0:02:41s\n",
      "epoch 57 | loss: 7.99838 |  0:02:44s\n",
      "epoch 58 | loss: 8.00977 |  0:02:46s\n",
      "epoch 59 | loss: 7.93567 |  0:02:49s\n",
      "epoch 60 | loss: 7.88887 |  0:02:52s\n",
      "epoch 61 | loss: 7.7108  |  0:02:55s\n",
      "epoch 62 | loss: 7.55412 |  0:02:58s\n",
      "epoch 63 | loss: 7.64565 |  0:03:00s\n",
      "epoch 64 | loss: 7.5371  |  0:03:03s\n",
      "epoch 65 | loss: 7.40649 |  0:03:06s\n",
      "epoch 66 | loss: 7.25302 |  0:03:09s\n",
      "epoch 67 | loss: 7.12262 |  0:03:12s\n",
      "epoch 68 | loss: 7.24165 |  0:03:15s\n",
      "epoch 69 | loss: 7.13569 |  0:03:17s\n",
      "epoch 70 | loss: 6.96067 |  0:03:20s\n",
      "epoch 71 | loss: 6.86145 |  0:03:23s\n",
      "epoch 72 | loss: 6.80179 |  0:03:26s\n",
      "epoch 73 | loss: 6.76869 |  0:03:29s\n",
      "epoch 74 | loss: 6.79734 |  0:03:31s\n",
      "epoch 75 | loss: 6.66957 |  0:03:34s\n",
      "epoch 76 | loss: 6.57384 |  0:03:37s\n",
      "epoch 77 | loss: 6.81642 |  0:03:40s\n",
      "epoch 78 | loss: 6.53479 |  0:03:43s\n",
      "epoch 79 | loss: 6.45517 |  0:03:46s\n",
      "epoch 80 | loss: 6.41382 |  0:03:48s\n",
      "epoch 81 | loss: 6.55298 |  0:03:51s\n",
      "epoch 82 | loss: 6.31642 |  0:03:54s\n",
      "epoch 83 | loss: 6.45602 |  0:03:57s\n",
      "epoch 84 | loss: 6.20908 |  0:03:59s\n",
      "epoch 85 | loss: 6.36125 |  0:04:02s\n",
      "epoch 86 | loss: 6.25394 |  0:04:05s\n",
      "epoch 87 | loss: 6.24952 |  0:04:08s\n",
      "epoch 88 | loss: 6.14474 |  0:04:11s\n",
      "epoch 89 | loss: 6.0707  |  0:04:14s\n",
      "epoch 90 | loss: 6.21236 |  0:04:17s\n",
      "epoch 91 | loss: 6.07006 |  0:04:19s\n",
      "epoch 92 | loss: 6.31057 |  0:04:22s\n",
      "epoch 93 | loss: 6.01722 |  0:04:25s\n",
      "epoch 94 | loss: 6.12317 |  0:04:28s\n",
      "epoch 95 | loss: 6.16416 |  0:04:31s\n",
      "epoch 96 | loss: 5.92643 |  0:04:33s\n",
      "epoch 97 | loss: 5.80681 |  0:04:36s\n",
      "epoch 98 | loss: 5.93249 |  0:04:39s\n",
      "epoch 99 | loss: 5.85364 |  0:04:42s\n",
      "[++] Saving the model and parameters in corresponding directories\n",
      "[++] Ended the training process for fold 19\n"
     ]
    }
   ],
   "source": [
    "use_df = pd.read_csv(\"../input/perov-scaled-data/scaled_trainable.csv\")\n",
    "tar_col = \"JV_default_PCE_numeric\"\n",
    "model_name = \"tabnet_reg\"\n",
    "fold_dict = joblib.load(\"../input/perov-fold-data/fold_data_export.z\")\n",
    "optim = \"adam\"\n",
    "k_folds = 20\n",
    "trial_fold = random.choice([x for x in range(k_folds)])\n",
    "num_trials = 15\n",
    "\n",
    "trial_data,best_params = train_trial(fold_dict = fold_dict,\n",
    "      fold = trial_fold,\n",
    "      k_folds=k_folds,\n",
    "      model_name=model_name,\n",
    "      sc_df=use_df,\n",
    "      tar_col=tar_col,\n",
    "      optim = optim,\n",
    "      optim_trial = num_trials)\n",
    "for key,value in trial_data.items():\n",
    "  print(f\"{key}: {value['rmse']}\")\n",
    "print(f\"[++] Ended the training process for fold {trial_fold}\")\n",
    "\n",
    "\n",
    "main_folds = [x for x in range(k_folds)]\n",
    "for fold in main_folds:\n",
    "    train(fold_dict = fold_dict,\n",
    "          fold = fold,\n",
    "          k_folds=k_folds,\n",
    "          model_name=model_name,\n",
    "          sc_df=use_df,\n",
    "          tar_col=tar_col,\n",
    "          optim = optim,\n",
    "          best_params = best_params)\n",
    "    print(f\"[++] Ended the training process for fold {fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbf53f",
   "metadata": {
    "papermill": {
     "duration": 0.180603,
     "end_time": "2023-01-01T12:12:38.113727",
     "exception": false,
     "start_time": "2023-01-01T12:12:37.933124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11342.82744,
   "end_time": "2023-01-01T12:12:39.691180",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-01T09:03:36.863740",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
